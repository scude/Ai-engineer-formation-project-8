{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5eeaf89ff30992",
   "metadata": {},
   "source": [
    "# Projet 8"
   ]
  },
  {
   "cell_type": "code",
   "id": "84b61c778fafe1a",
   "metadata": {},
   "source": [
    "# --- Simple dataset scanner (counts only, no filenames, no JSON) ---\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# >>> Change this to your dataset root (WSL path) <<<\n",
    "ROOT = Path(\"../data\")\n",
    "\n",
    "IGNORE_HIDDEN = True       # ignore .git, __pycache__, etc.\n",
    "MAX_DIRS_TO_SHOW = 80      # limit directory lines for readability\n",
    "\n",
    "total_files = 0\n",
    "total_dirs = 0\n",
    "by_ext = Counter()\n",
    "by_dir = {}\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(ROOT):\n",
    "    # optionally hide hidden/internal dirs\n",
    "    if IGNORE_HIDDEN:\n",
    "        dirnames[:] = [d for d in dirnames if not d.startswith(\".\") and not d.startswith(\"__\")]\n",
    "    total_dirs += 1\n",
    "    rel = Path(dirpath).relative_to(ROOT) if Path(dirpath) != ROOT else Path(\".\")\n",
    "    by_dir[str(rel)] = len(filenames)\n",
    "    for fn in filenames:\n",
    "        by_ext[Path(fn).suffix.lower()] += 1\n",
    "    total_files += len(filenames)\n",
    "\n",
    "print(f\"[ROOT] {ROOT}\")\n",
    "print(f\"dirs={total_dirs:,}  files={total_files:,}\\n\")\n",
    "\n",
    "print(\"By extension (top 10):\")\n",
    "for ext, n in by_ext.most_common(10):\n",
    "    print(f\"  {ext or '(no ext)'}: {n:,}\")\n",
    "print()\n",
    "\n",
    "print(f\"Directory counts (first {MAX_DIRS_TO_SHOW}):\")\n",
    "for i, (rel, n) in enumerate(sorted(by_dir.items())):\n",
    "    if i >= MAX_DIRS_TO_SHOW:\n",
    "        print(\"  ... (truncated)\")\n",
    "        break\n",
    "    print(f\"  {rel}: {n}\")\n",
    "\n",
    "# -------- Cityscapes mini-summary (counts only) --------\n",
    "def count_pattern(base: Path, split: str, suffix: str) -> int:\n",
    "    split_dir = base / split\n",
    "    total = 0\n",
    "    if split_dir.exists():\n",
    "        for city_dir in split_dir.iterdir():\n",
    "            if city_dir.is_dir():\n",
    "                total += sum(1 for p in city_dir.iterdir()\n",
    "                             if p.is_file() and p.name.endswith(suffix))\n",
    "    return total\n",
    "\n",
    "print(\"\\n[Cityscapes summary]\")\n",
    "for split in (\"train\", \"val\", \"test\"):\n",
    "    gt_base = ROOT / \"gtFine\"\n",
    "    left_base = ROOT / \"leftImg8bit\"\n",
    "    label = count_pattern(gt_base, split, \"_gtFine_labelIds.png\")\n",
    "    color = count_pattern(gt_base, split, \"_gtFine_color.png\")\n",
    "    inst  = count_pattern(gt_base, split, \"_gtFine_instanceIds.png\")\n",
    "    poly  = count_pattern(gt_base, split, \"_gtFine_polygons.json\")\n",
    "    left  = count_pattern(left_base, split, \"_leftImg8bit.png\")\n",
    "    print(f\"  {split:5s}: leftImg8bit={left:6d}  labelIds={label:6d}  color={color:6d}  instanceIds={inst:6d}  polygons.json={poly:6d}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b8ee8ae56f5c54f",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"../data\")  # adapte si besoin\n",
    "SUF_LEFT = \"_leftImg8bit.png\"\n",
    "SUF_LBL  = \"_gtFine_labelIds.png\"\n",
    "\n",
    "def base_id(name: str) -> str:\n",
    "    return name[:-len(SUF_LEFT)] if name.endswith(SUF_LEFT) else name[:-len(SUF_LBL)]\n",
    "\n",
    "def split_counts(split: str):\n",
    "    left_dir = ROOT / \"leftImg8bit\" / split\n",
    "    lbl_dir  = ROOT / \"gtFine\"      / split\n",
    "    left = sorted(left_dir.rglob(f\"*{SUF_LEFT}\")) if left_dir.exists() else []\n",
    "    lbl  = sorted(lbl_dir.rglob (f\"*{SUF_LBL}\" )) if lbl_dir.exists()  else []\n",
    "    left_ids = {base_id(p.name) for p in left}\n",
    "    lbl_ids  = {base_id(p.name) for p in lbl}\n",
    "    paired = left_ids & lbl_ids\n",
    "    print(f\"{split:<5} | left={len(left):4d}  labels={len(lbl):4d}  paired={len(paired):4d}\")\n",
    "\n",
    "for sp in (\"train\", \"val\", \"test\"):\n",
    "    split_counts(sp)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3b30f7c4d8786b68",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "ROOT = Path(\"../data\")\n",
    "\n",
    "PALETTE = {\n",
    "    7:(128,64,128), 8:(244,35,232), 11:(70,70,70), 12:(102,102,156), 13:(190,153,153),\n",
    "    17:(153,153,153), 19:(250,170,30), 20:(220,220,0), 21:(107,142,35), 22:(152,251,152),\n",
    "    23:(70,130,180), 24:(220,20,60), 25:(255,0,0), 26:(0,0,142), 27:(0,0,70),\n",
    "    28:(0,60,100), 31:(0,80,100), 32:(0,0,230), 33:(119,11,32),\n",
    "}\n",
    "\n",
    "def pairs(split=\"val\"):\n",
    "    lbls = sorted((ROOT/\"gtFine\"/split).rglob(\"*_gtFine_labelIds.png\"))\n",
    "    out = []\n",
    "    for lp in lbls:\n",
    "        stem = lp.name.replace(\"_gtFine_labelIds.png\", \"\")\n",
    "        city = lp.parent.name\n",
    "        left = ROOT/\"leftImg8bit\"/split/city/(stem+\"_leftImg8bit.png\")\n",
    "        if left.exists():\n",
    "            out.append((left, lp))\n",
    "    return out\n",
    "\n",
    "def colorize(ids: np.ndarray) -> Image.Image:\n",
    "    h, w = ids.shape\n",
    "    rgb = np.zeros((h, w, 3), np.uint8)\n",
    "    for k, c in PALETTE.items():\n",
    "        rgb[ids == k] = c\n",
    "    return Image.fromarray(rgb, \"RGB\")\n",
    "\n",
    "def overlay(img: Image.Image, mask_rgb: Image.Image, alpha=0.5) -> Image.Image:\n",
    "    a = np.asarray(img.convert(\"RGB\"), np.float32)\n",
    "    b = np.asarray(mask_rgb, np.float32)\n",
    "    return Image.fromarray(np.clip((1-alpha)*a + alpha*b, 0, 255).astype(np.uint8))\n",
    "\n",
    "samples = pairs(\"val\")\n",
    "assert samples, \"No pairs found — check your paths.\"\n",
    "random.shuffle(samples)\n",
    "k = 3\n",
    "\n",
    "plt.figure(figsize=(15, 5*k))\n",
    "for i, (left_p, lbl_p) in enumerate(samples[:k]):\n",
    "    left = Image.open(left_p).convert(\"RGB\")\n",
    "    ids  = np.array(Image.open(lbl_p))\n",
    "    mask = colorize(ids)\n",
    "    over = overlay(left, mask, alpha=0.45)\n",
    "    for j, (img, title) in enumerate([(left,\"leftImg8bit\"),(mask,\"labelIds (colored)\"),(over,\"overlay\")]):\n",
    "        ax = plt.subplot(k, 3, i*3 + j + 1)\n",
    "        ax.imshow(img); ax.set_title(f\"{left_p.parent.name} — {title}\", fontsize=10); ax.axis(\"off\")\n",
    "plt.tight_layout(); plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a81dea999fabce6b",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f540b1886f92c7b1",
   "metadata": {},
   "source": [
    "\n",
    "### Remapping Cityscapes 32→8 classes\n",
    "\n",
    "Vérifier la balance des classes !!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1b632fc00041fe27",
   "metadata": {},
   "source": [
    "# --- 32→8 mapping (Cityscapes labelIds -> 8-class IDs), ignore = 255 ---\n",
    "import numpy as np\n",
    "\n",
    "CS_LABELID_TO_8 = {\n",
    "    # 0..5 (voids) -> ignore by LUT fill (no need to list)\n",
    "    6: 0,\n",
    "    7: 0,  9: 0, 10: 0,           # road-like: road, parking, rail track\n",
    "    8: 1,                         # sidewalk\n",
    "    11: 2, 12: 2, 13: 2, 14: 2, 15: 2, 16: 2,   # building + barriers\n",
    "    17: 3, 18: 3, 19: 3, 20: 3,                 # traffic objs (pole/ts/tl)\n",
    "    21: 4, 22: 4,                                 # vegetation + terrain\n",
    "    23: 5,                                       # sky\n",
    "    24: 6, 25: 6,                                 # person + rider\n",
    "    26: 7, 27: 7, 28: 7, 29: 7, 30: 7, 31: 7, 32: 7, 33: 7,  # vehicles\n",
    "}\n",
    "\n",
    "def build_labelid_to8_lut(ignore_value: int = 255) -> np.ndarray:\n",
    "    \"\"\"Create a 256-entry LUT mapping Cityscapes labelIds -> {0..7} or 255(ignore).\"\"\"\n",
    "    lut = np.full(256, ignore_value, dtype=np.uint8)\n",
    "    for k, v in CS_LABELID_TO_8.items():\n",
    "        lut[k] = v\n",
    "    return lut\n",
    "\n",
    "LUT_32TO8 = build_labelid_to8_lut(ignore_value=255)\n",
    "\n",
    "def remap_labelids_to8(arr_uint16: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Vectorized remap of HxW labelIds (uint16/uint8) to 8-class IDs with 255 ignore.\"\"\"\n",
    "    arr = arr_uint16.astype(np.uint16)\n",
    "    arr = np.minimum(arr, 255).astype(np.uint8)\n",
    "    return LUT_32TO8[arr]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4090ae75134d47fe",
   "metadata": {},
   "source": [
    "PALETTE_8 = {\n",
    "    0:(128,64,128),   # road\n",
    "    1:(244,35,232),   # sidewalk\n",
    "    2:(70,70,70),     # building+barrier\n",
    "    3:(220,220,0),    # traffic objs\n",
    "    4:(107,142,35),   # vegetation/terrain\n",
    "    5:(70,130,180),   # sky\n",
    "    6:(220,20,60),    # person+rider\n",
    "    7:(0,0,142),      # vehicle\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "623c52d0928c016c",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def colorize_8(label8: np.ndarray, palette: dict) -> Image.Image:\n",
    "    h, w = label8.shape\n",
    "    rgb = np.zeros((h, w, 3), np.uint8)\n",
    "    for k, c in palette.items():\n",
    "        rgb[label8 == k] = c\n",
    "    return Image.fromarray(rgb, \"RGB\")\n",
    "\n",
    "sample_lbl = next(Path(\"../data/gtFine/val/frankfurt\").glob(\"*_gtFine_labelIds.png\"))\n",
    "arr = np.array(Image.open(sample_lbl))\n",
    "arr8 = remap_labelids_to8(arr)\n",
    "plt.figure(figsize=(8,4)); plt.imshow(colorize_8(arr8, PALETTE_8)); plt.axis(\"off\"); plt.title(\"8-class mask\"); plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "71f824d108f9b17f",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# ↓↓↓ Quieter TensorFlow logs (set BEFORE importing tf)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"     # 0=all, 1=INFO off, 2=INFO+WARNING off, 3=all off\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"  # avoid grabbing all GPU memory\n",
    "# Optional: disable oneDNN (removes the \"oneDNN custom ops are on\" line, and tiny numeric diffs)\n",
    "# os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from absl import logging as absl_logging\n",
    "absl_logging.set_verbosity(absl_logging.ERROR)  # reduce absl spam\n",
    "\n",
    "# (Optional) confirm GPU + set memory growth (extra safety)\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for g in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(g, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"TF:\", tf.__version__, \"| GPUs:\", gpus)\n",
    "\n",
    "# ==========================\n",
    "# Cityscapes 32→8 remapping\n",
    "# ==========================\n",
    "import numpy as np\n",
    "\n",
    "# 8 classes for embedded use (ignore=255):\n",
    "# 0=road (7,9,10) | 1=sidewalk(8) | 2=building+barriers(11–16) | 3=traffic objs(17–20)\n",
    "# 4=vegetation+terrain(21,22) | 5=sky(23) | 6=person+rider(24,25) | 7=vehicle(26–33)\n",
    "CS_LABELID_TO_8 = {\n",
    "    6:0,\n",
    "    7:0, 9:0, 10:0,\n",
    "    8:1,\n",
    "    11:2, 12:2, 13:2, 14:2, 15:2, 16:2,\n",
    "    17:3, 18:3, 19:3, 20:3,\n",
    "    21:4, 22:4,\n",
    "    23:5,\n",
    "    24:6, 25:6,\n",
    "    26:7, 27:7, 28:7, 29:7, 30:7, 31:7, 32:7, 33:7,\n",
    "}\n",
    "\n",
    "def build_labelid_to8_lut(ignore_value: int = 255) -> np.ndarray:\n",
    "    lut = np.full(256, ignore_value, dtype=np.uint8)\n",
    "    for k, v in CS_LABELID_TO_8.items():\n",
    "        lut[k] = v\n",
    "    return lut\n",
    "\n",
    "LUT_32TO8 = build_labelid_to8_lut(ignore_value=255)\n",
    "LUT_TF = tf.convert_to_tensor(LUT_32TO8, dtype=tf.uint8)  # shape [256]\n",
    "\n",
    "# ===================\n",
    "# Dataset (tf.data)\n",
    "# ===================\n",
    "from pathlib import Path\n",
    "ROOT = Path(\"../data\")               # <<< change if needed (WSL path)\n",
    "INPUT_SIZE = (512, 1024)             # (H, W)\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "SUF_LEFT = \"_leftImg8bit.png\"\n",
    "SUF_LBL  = \"_gtFine_labelIds.png\"\n",
    "\n",
    "def list_pairs(split: str):\n",
    "    \"\"\"Return two aligned lists: left paths and label paths for a given split.\"\"\"\n",
    "    lefts, labels = [], []\n",
    "    lbl_root = ROOT / \"gtFine\" / split\n",
    "    for lbl in sorted(lbl_root.rglob(f\"*{SUF_LBL}\")):\n",
    "        city = lbl.parent.name\n",
    "        stem = lbl.name.replace(SUF_LBL, \"\")\n",
    "        left = ROOT / \"leftImg8bit\" / split / city / f\"{stem}{SUF_LEFT}\"\n",
    "        if left.exists():\n",
    "            lefts.append(str(left))\n",
    "            labels.append(str(lbl))\n",
    "    if not lefts:\n",
    "        raise FileNotFoundError(f\"No pairs found for split='{split}'. Check your paths under {ROOT}.\")\n",
    "    return lefts, labels\n",
    "\n",
    "def decode_and_preprocess(left_path, lbl_path, training: bool):\n",
    "    # 1) Read bytes\n",
    "    left_bytes = tf.io.read_file(left_path)\n",
    "    lbl_bytes  = tf.io.read_file(lbl_path)\n",
    "\n",
    "    # 2) Decode\n",
    "    img = tf.io.decode_png(left_bytes, channels=3)     # uint8 [H,W,3]\n",
    "    lab = tf.io.decode_png(lbl_bytes,  channels=1)     # uint8/16 [H,W,1]\n",
    "\n",
    "    # 3) To workable dtypes\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)   # [0,1]\n",
    "    lab = tf.cast(lab, tf.int32)                          # index dtype for LUT\n",
    "\n",
    "    # 4) Remap 32→8 via LUT\n",
    "    lab_clipped = tf.minimum(lab, 255)\n",
    "    lab8 = tf.gather(LUT_TF, lab_clipped)                 # uint8 [H,W,1]\n",
    "    lab8 = tf.squeeze(lab8, axis=-1)                      # uint8 [H,W]\n",
    "\n",
    "    # 5) Simple augment (sync flip)\n",
    "    if training:\n",
    "        do_flip = tf.random.uniform(()) > 0.5\n",
    "        img  = tf.cond(do_flip, lambda: tf.image.flip_left_right(img), lambda: img)\n",
    "        lab8 = tf.cond(do_flip, lambda: tf.image.flip_left_right(lab8[..., None])[:, :, 0], lambda: lab8)\n",
    "\n",
    "    # 6) Resize (labels in nearest, keep uint8)\n",
    "    img  = tf.image.resize(img,  INPUT_SIZE, method=\"bilinear\")\n",
    "    lab8 = tf.cast(tf.image.resize(lab8[..., None], INPUT_SIZE, method=\"nearest\")[:, :, 0], tf.uint8)\n",
    "\n",
    "    # 7) Ignore handling → sample_weight (float32); labels safe (uint8→int32)\n",
    "    ignore_val = tf.constant(255, dtype=tf.uint8)\n",
    "    ignore = tf.equal(lab8, ignore_val)  # bool [H,W]\n",
    "\n",
    "    weights = tf.where(ignore,\n",
    "                       tf.zeros_like(lab8, dtype=tf.float32),\n",
    "                       tf.ones_like(lab8,  dtype=tf.float32))              # float32 [H,W]\n",
    "\n",
    "    lab8_safe = tf.where(ignore,\n",
    "                         tf.zeros_like(lab8),   # uint8 0 (will be masked by weights anyway)\n",
    "                         lab8)\n",
    "    labels = tf.cast(lab8_safe, tf.int32)                                    # int32 [H,W]\n",
    "\n",
    "    return img, labels, weights\n",
    "\n",
    "def make_dataset(split: str, batch_size: int = BATCH_SIZE, training: bool = True) -> tf.data.Dataset:\n",
    "    lefts, labels = list_pairs(split)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((lefts, labels))\n",
    "    if training:\n",
    "        ds = ds.shuffle(buffer_size=min(len(lefts), 2000), reshuffle_each_iteration=True)\n",
    "    ds = ds.map(lambda l, y: decode_and_preprocess(l, y, training),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size, drop_remainder=training)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# ==============\n",
    "# Smoke test\n",
    "# ==============\n",
    "train_ds = make_dataset(\"train\", batch_size=2, training=True)\n",
    "xb, yb, wb = next(iter(train_ds))\n",
    "print(\"x:\", xb.shape, xb.dtype, \"| y:\", yb.shape, yb.dtype, \"| w:\", wb.shape, wb.dtype)\n",
    "\n",
    "# Example compile/fit (model must output logits with 8 channels)\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# opt  = tf.keras.optimizers.Adam(1e-3)\n",
    "# model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
    "# model.fit(train_ds,\n",
    "#           validation_data=make_dataset(\"val\", batch_size=2, training=False),\n",
    "#           epochs=1)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fa8eafa784ea3287",
   "metadata": {},
   "source": [
    "# ==== Class balance for Cityscapes 8 classes (with ignore=255) ====\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# ---- Config (adapt if needed) ----\n",
    "ROOT = Path(\"../data\")          # dataset root (WSL path)\n",
    "SPLIT = \"train\"                 # \"train\" | \"val\" | \"test\"\n",
    "SUF_LBL = \"_gtFine_labelIds.png\"\n",
    "\n",
    "# 8-class names (your mapping)\n",
    "CLASS8_NAMES = [\n",
    "    \"road\", \"sidewalk\", \"building+barriers\", \"traffic-objs\",\n",
    "    \"vegetation+terrain\", \"sky\", \"person+rider\", \"vehicle\"\n",
    "]\n",
    "\n",
    "# If LUT_32TO8 not in scope, (re)build it quickly:\n",
    "try:\n",
    "    LUT_32TO8\n",
    "except NameError:\n",
    "    CS_LABELID_TO_8 = {\n",
    "        6:0, 7:0, 9:0, 10:0, 8:1, 11:2,12:2,13:2,14:2,15:2,16:2,\n",
    "        17:3,18:3,19:3,20:3, 21:4,22:4, 23:5, 24:6,25:6,\n",
    "        26:7,27:7,28:7,29:7,30:7,31:7,32:7,33:7,\n",
    "    }\n",
    "    LUT_32TO8 = np.full(256, 255, dtype=np.uint8)\n",
    "    for k, v in CS_LABELID_TO_8.items():\n",
    "        LUT_32TO8[k] = v\n",
    "\n",
    "def remap_to8_np(arr_label_ids: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"arr_label_ids: HxW uint16/uint8 -> HxW uint8 in {0..7,255}\"\"\"\n",
    "    arr = arr_label_ids.astype(np.uint16)\n",
    "    arr = np.minimum(arr, 255).astype(np.uint8)\n",
    "    return LUT_32TO8[arr]\n",
    "\n",
    "def class_balance(split: str = SPLIT):\n",
    "    lbl_paths = sorted((ROOT/\"gtFine\"/split).rglob(f\"*{SUF_LBL}\"))\n",
    "    assert lbl_paths, f\"No labels found under {ROOT}/gtFine/{split}\"\n",
    "    counts = np.zeros(8, dtype=np.int64)\n",
    "    ignore = 0\n",
    "    for i, p in enumerate(lbl_paths, 1):\n",
    "        lab = np.array(Image.open(p))         # (H,W) uint16/uint8\n",
    "        lab8 = remap_to8_np(lab)              # (H,W) uint8\n",
    "        m_ignore = (lab8 == 255)\n",
    "        ignore += int(m_ignore.sum())\n",
    "        # bincount only on valid pixels\n",
    "        c = np.bincount(lab8[~m_ignore].ravel(), minlength=8)\n",
    "        counts += c[:8]\n",
    "        if i % 500 == 0 or i == len(lbl_paths):\n",
    "            print(f\"[{split}] processed {i}/{len(lbl_paths)} images...\", end=\"\\r\")\n",
    "    print()\n",
    "    total_valid = int(counts.sum())\n",
    "    total_pixels = total_valid + ignore\n",
    "    freqs = counts / max(total_valid, 1)\n",
    "    return counts, ignore, total_valid, total_pixels, freqs\n",
    "\n",
    "counts, ignore, total_valid, total_pixels, freqs = class_balance(\"train\")\n",
    "\n",
    "print(\"\\n=== Class balance (train) ===\")\n",
    "for k, (name, n, f) in enumerate(zip(CLASS8_NAMES, counts, freqs)):\n",
    "    print(f\"{k}: {name:<20s}  pixels={n:,}   freq={f:.4%}\")\n",
    "print(f\"\\nignore pixels (==255): {ignore:,}\")\n",
    "print(f\"total valid pixels:     {total_valid:,}\")\n",
    "print(f\"total pixels (incl. ignore): {total_pixels:,}\")\n",
    "\n",
    "# ---- Optional: derive class weights ----\n",
    "# Inverse-frequency, normalized to mean=1 (good starting point)\n",
    "weights_inv = (1.0 / np.maximum(freqs, 1e-12))\n",
    "weights_inv = weights_inv / weights_inv.mean()\n",
    "print(\"\\nSuggested class weights (inverse-freq, mean≈1):\")\n",
    "for k, (name, w) in enumerate(zip(CLASS8_NAMES, weights_inv)):\n",
    "    print(f\"{k}: {name:<20s}  w={w:.3f}\")\n",
    "\n",
    "# Median-frequency balancing (alternative)\n",
    "median_f = np.median(freqs[freqs > 0])\n",
    "weights_med = median_f / np.maximum(freqs, 1e-12)\n",
    "weights_med = weights_med / weights_med.mean()\n",
    "print(\"\\nSuggested class weights (median-freq, mean≈1):\")\n",
    "for k, (name, w) in enumerate(zip(CLASS8_NAMES, weights_med)):\n",
    "    print(f\"{k}: {name:<20s}  w={w:.3f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8433da3712483ac",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# expects: counts (np.array shape [8]), freqs (shape [8]), ignore (int),\n",
    "#          total_valid (int), total_pixels (int), CLASS8_NAMES (list of 8 str)\n",
    "\n",
    "# ---- 1) Bar chart des 8 classes (trié décroissant) ----\n",
    "order = np.argsort(freqs)[::-1]\n",
    "names_sorted = [CLASS8_NAMES[i] for i in order]\n",
    "freqs_sorted = freqs[order]\n",
    "counts_sorted = counts[order]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.bar(range(len(names_sorted)), freqs_sorted)  # no explicit colors\n",
    "plt.xticks(range(len(names_sorted)), names_sorted, rotation=20, ha=\"right\")\n",
    "plt.ylabel(\"Frequency (share of valid pixels)\")\n",
    "plt.title(\"Cityscapes (train) — Class balance (8 classes)\")\n",
    "\n",
    "# annotations: % + millions de pixels\n",
    "for i, (b, f, c) in enumerate(zip(bars, freqs_sorted, counts_sorted)):\n",
    "    plt.text(b.get_x() + b.get_width()/2,\n",
    "             b.get_height() + 0.002,\n",
    "             f\"{f*100:.1f}%\\n{c/1e6:.1f}M\",\n",
    "             ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.ylim(0, max(freqs_sorted)*1.15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 2) Valid vs Ignore (pour info) ----\n",
    "valid_share = total_valid / total_pixels\n",
    "ignore_share = 1.0 - valid_share\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "bars2 = plt.bar([0,1], [valid_share, ignore_share])\n",
    "plt.xticks([0,1], [\"valid\", \"ignore (==255)\"])\n",
    "plt.ylabel(\"Share of total pixels\")\n",
    "plt.title(\"Valid vs Ignore pixels (train)\")\n",
    "\n",
    "for x, v in zip([0,1], [valid_share, ignore_share]):\n",
    "    plt.text(x, v + 0.005, f\"{v*100:.1f}%\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.ylim(0, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eb8a233ceccaddd6",
   "metadata": {},
   "source": [
    "if False:\n",
    "    # -*- coding: utf-8 -*-\n",
    "    \"\"\"\n",
    "    Cityscapes -> 8 classes (PALETTE_8), ignore=255\n",
    "    DeepLabV3+ (ResNet50, ImageNet), TensorFlow/Keras\n",
    "    + MLflow logging (params/metrics/artifacts)\n",
    "    + Save best model (.keras) & log to MLflow\n",
    "    \"\"\"\n",
    "\n",
    "    import os, glob, re, shutil, datetime\n",
    "    from typing import List, Tuple, Optional\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "\n",
    "    # ---------------------- Config ----------------------\n",
    "    DATA_ROOT = \"../data\"\n",
    "    LEFT_DIR  = os.path.join(DATA_ROOT, \"leftImg8bit\")\n",
    "    GT_DIR    = os.path.join(DATA_ROOT, \"gtFine\")\n",
    "\n",
    "    IMG_SUFFIX = \"_leftImg8bit.png\"\n",
    "    LBL_SUFFIX = \"_gtFine_labelIds.png\"   # we remap labelIds (0..33) ourselves\n",
    "\n",
    "    SEED = 1337\n",
    "    NUM_CLASSES = 8                       # matches PALETTE_8 (0..7)\n",
    "    IGNORE_INDEX = 255\n",
    "    HEIGHT, WIDTH = 512, 1024\n",
    "    BATCH_SIZE = 2\n",
    "    LR = 3e-4\n",
    "    EPOCHS = 60\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    # Ensure deterministic behavior (as much as possible)\n",
    "    tf.keras.utils.set_random_seed(SEED)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "    # Optional: force float32 policy (stable numerics)\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    mixed_precision.set_global_policy(\"float32\")\n",
    "\n",
    "    print(f\"TF: {tf.__version__} | GPUs: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "    # Optional palette (viz only)\n",
    "    PALETTE_8 = {\n",
    "        0:(128, 64,128),  # road\n",
    "        1:(244, 35,232),  # sidewalk\n",
    "        2:( 70, 70, 70),  # building+barrier\n",
    "        3:(220,220,  0),  # traffic objs\n",
    "        4:(107,142, 35),  # vegetation/terrain\n",
    "        5:( 70,130,180),  # sky\n",
    "        6:(220, 20, 60),  # person+rider\n",
    "        7:(  0,  0,142),  # vehicle\n",
    "    }\n",
    "\n",
    "    # ---------------------- Pair listing ----------------------\n",
    "    def left_to_label_path(left_path: str) -> str:\n",
    "        parts = left_path.split(os.sep)\n",
    "        parts[parts.index(\"leftImg8bit\")] = \"gtFine\"\n",
    "        parts[-1] = parts[-1].replace(IMG_SUFFIX, LBL_SUFFIX)\n",
    "        return os.path.join(*parts)\n",
    "\n",
    "    def gather_pairs(split: str) -> Tuple[List[str], List[str], List[str]]:\n",
    "        img_glob = os.path.join(LEFT_DIR, split, \"*\", f\"*{IMG_SUFFIX}\")\n",
    "        left_files = sorted(glob.glob(img_glob))\n",
    "        left_ok, lbl_ok, missing = [], [], []\n",
    "        for lp in left_files:\n",
    "            lb = left_to_label_path(lp)\n",
    "            if os.path.exists(lb):\n",
    "                left_ok.append(lp)\n",
    "                lbl_ok.append(lb)\n",
    "            else:\n",
    "                missing.append(lb)\n",
    "        return left_ok, lbl_ok, missing\n",
    "\n",
    "    def report_pairs(split: str, xs: List[str], ys: List[str], miss: List[str]):\n",
    "        print(f\"[info] {split}: {len(xs)} paires valides | {len(miss)} manquantes\")\n",
    "        if xs:\n",
    "            print(f\"exemple mapping: {xs[0]} -> {ys[0]}\")\n",
    "\n",
    "    # ---------------------- Label remap (Cityscapes 0..33 -> 8 classes + 255) ----------------------\n",
    "    # 0 road: 7,6,9,10\n",
    "    # 1 sidewalk: 8\n",
    "    # 2 building+barrier: 11..16\n",
    "    # 3 traffic objects: 17..20\n",
    "    # 4 vegetation/terrain: 21,22\n",
    "    # 5 sky: 23\n",
    "    # 6 human: 24,25\n",
    "    # 7 vehicle: 26..33\n",
    "    # ignore: 0..5\n",
    "    REMAP_TABLE = [IGNORE_INDEX] * 34\n",
    "    for k in [6, 7, 9, 10]: REMAP_TABLE[k] = 0\n",
    "    REMAP_TABLE[8] = 1\n",
    "    for k in [11, 12, 13, 14, 15, 16]: REMAP_TABLE[k] = 2\n",
    "    for k in [17, 18, 19, 20]: REMAP_TABLE[k] = 3\n",
    "    for k in [21, 22]: REMAP_TABLE[k] = 4\n",
    "    REMAP_TABLE[23] = 5\n",
    "    for k in [24, 25]: REMAP_TABLE[k] = 6\n",
    "    for k in [26, 27, 28, 29, 30, 31, 32, 33]: REMAP_TABLE[k] = 7\n",
    "\n",
    "    REMAP_LUT = tf.constant(REMAP_TABLE, dtype=tf.int32)\n",
    "\n",
    "    @tf.function\n",
    "    def remap_labels(y: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Map Cityscapes labelIds (0..33) to {0..7, 255} according to PALETTE_8.\"\"\"\n",
    "        y = tf.clip_by_value(y, 0, 33)\n",
    "        return tf.gather(REMAP_LUT, y)\n",
    "\n",
    "    # ---------------------- I/O & dataset ----------------------\n",
    "    def decode_img(path: tf.Tensor) -> tf.Tensor:\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_png(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)   # [0,1]\n",
    "        return img\n",
    "\n",
    "    def decode_lbl(path: tf.Tensor) -> tf.Tensor:\n",
    "        raw = tf.io.read_file(path)\n",
    "        y = tf.image.decode_png(raw, channels=1)\n",
    "        y = tf.squeeze(y, axis=-1)        # [H,W] uint8\n",
    "        y = tf.cast(y, tf.int32)\n",
    "        y = remap_labels(y)               # -> {0..7, 255}\n",
    "        return y\n",
    "\n",
    "    def resize_pair(x: tf.Tensor, y: tf.Tensor):\n",
    "        x = tf.image.resize(x, (HEIGHT, WIDTH), method=\"bilinear\")\n",
    "        y = tf.cast(tf.image.resize(tf.expand_dims(y, -1), (HEIGHT, WIDTH),\n",
    "                                    method=\"nearest\"), tf.int32)\n",
    "        y = tf.squeeze(y, axis=-1)\n",
    "        return x, y\n",
    "\n",
    "    def augment(x: tf.Tensor, y: tf.Tensor):\n",
    "        do_flip = tf.less(tf.random.uniform([]), 0.5)\n",
    "        x = tf.cond(do_flip, lambda: tf.image.flip_left_right(x), lambda: x)\n",
    "        y = tf.cond(do_flip, lambda: tf.image.flip_left_right(tf.expand_dims(y, -1)),\n",
    "                    lambda: tf.expand_dims(y, -1))\n",
    "        y = tf.squeeze(y, -1)\n",
    "        return x, y\n",
    "\n",
    "    def make_weights(y: tf.Tensor) -> tf.Tensor:\n",
    "        # 0 on ignored pixels, 1 otherwise\n",
    "        return tf.where(tf.equal(y, IGNORE_INDEX), 0.0, 1.0)\n",
    "\n",
    "    def sanitize_labels_for_loss(y: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Replace ignore index (255) by a valid index (0) before sending to loss.\n",
    "        These pixels have sample_weight=0, so this replacement doesn't affect optimization.\"\"\"\n",
    "        return tf.where(tf.equal(y, IGNORE_INDEX), tf.zeros_like(y), y)\n",
    "\n",
    "    def parse_example(img_path: tf.Tensor, lbl_path: tf.Tensor, training: bool):\n",
    "        x = decode_img(img_path)\n",
    "        y = decode_lbl(lbl_path)             # {0..7, 255}\n",
    "        x, y = resize_pair(x, y)\n",
    "        if training:\n",
    "            x, y = augment(x, y)\n",
    "        w = make_weights(y)                  # 0 where y==255, else 1\n",
    "        y = sanitize_labels_for_loss(y)      # 255 -> 0 to avoid NaN in sparse CE\n",
    "        return x, y, w\n",
    "\n",
    "    def build_dataset(x_paths: List[str], y_paths: List[str], training: bool) -> tf.data.Dataset:\n",
    "        ds = tf.data.Dataset.from_tensor_slices((x_paths, y_paths))\n",
    "        if training:\n",
    "            ds = ds.shuffle(buffer_size=len(x_paths), seed=SEED, reshuffle_each_iteration=True)\n",
    "        ds = ds.map(lambda xp, yp: parse_example(xp, yp, training), num_parallel_calls=AUTOTUNE)\n",
    "        ds = ds.batch(BATCH_SIZE, drop_remainder=False)\n",
    "        ds = ds.prefetch(AUTOTUNE)\n",
    "        return ds\n",
    "\n",
    "    # ---------------------- Model: ASPP & DeepLabV3+ ----------------------\n",
    "    def ASPP(x, out_channels=256, rates=(6, 12, 18), name=\"aspp\"):\n",
    "        h, w = keras.backend.int_shape(x)[1:3]\n",
    "\n",
    "        img_pool = layers.GlobalAveragePooling2D(keepdims=True, name=f\"{name}_imgpool\")(x)\n",
    "        img_pool = layers.Conv2D(out_channels, 1, padding=\"same\", use_bias=False, name=f\"{name}_img_1x1\")(img_pool)\n",
    "        img_pool = layers.BatchNormalization(name=f\"{name}_img_1x1_bn\")(img_pool)\n",
    "        img_pool = layers.ReLU(name=f\"{name}_img_relu\")(img_pool)\n",
    "        img_pool = layers.Resizing(h, w, interpolation=\"bilinear\", name=f\"{name}_ups\")(img_pool)\n",
    "\n",
    "        conv_1x1 = layers.Conv2D(out_channels, 1, padding=\"same\", use_bias=False, name=f\"{name}_1x1\")(x)\n",
    "        conv_1x1 = layers.BatchNormalization(name=f\"{name}_1x1_bn\")(conv_1x1)\n",
    "        conv_1x1 = layers.ReLU(name=f\"{name}_1x1_relu\")(conv_1x1)\n",
    "\n",
    "        atrous = []\n",
    "        for r in rates:\n",
    "            a = layers.Conv2D(out_channels, 3, padding=\"same\", dilation_rate=r, use_bias=False, name=f\"{name}_r{r}\")(x)\n",
    "            a = layers.BatchNormalization(name=f\"{name}_r{r}_bn\")(a)\n",
    "            a = layers.ReLU(name=f\"{name}_r{r}_relu\")(a)\n",
    "            atrous.append(a)\n",
    "\n",
    "        x = layers.Concatenate(name=f\"{name}_concat\")([conv_1x1, *atrous, img_pool])\n",
    "        x = layers.Conv2D(out_channels, 1, padding=\"same\", use_bias=False, name=f\"{name}_proj\")(x)\n",
    "        x = layers.BatchNormalization(name=f\"{name}_proj_bn\")(x)\n",
    "        x = layers.ReLU(name=f\"{name}_proj_relu\")(x)\n",
    "        x = layers.Dropout(0.1, name=f\"{name}_drop\")(x)\n",
    "        return x\n",
    "\n",
    "    def DeeplabV3Plus_ResNet50(input_shape=(HEIGHT, WIDTH, 3), num_classes=NUM_CLASSES, output_stride=16):\n",
    "        assert output_stride in (8, 16)\n",
    "        base = keras.applications.ResNet50(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "        low  = base.get_layer(\"conv2_block3_out\").output    # (H/4,  W/4,  256)\n",
    "        high = base.get_layer(\"conv4_block6_out\").output    # (H/16, W/16, 1024)\n",
    "\n",
    "        x = ASPP(high, out_channels=256, rates=(6, 12, 18), name=\"aspp\")\n",
    "\n",
    "        low_h, low_w = keras.backend.int_shape(low)[1:3]\n",
    "        x = layers.Resizing(low_h, low_w, interpolation=\"bilinear\", name=\"ups_aspp\")(x)\n",
    "\n",
    "        low_proj = layers.Conv2D(48, 1, padding=\"same\", use_bias=False, name=\"low_proj\")(low)\n",
    "        low_proj = layers.BatchNormalization(name=\"low_proj_bn\")(low_proj)\n",
    "        low_proj = layers.ReLU(name=\"low_proj_relu\")(low_proj)\n",
    "\n",
    "        x = layers.Concatenate(name=\"dec_concat\")([x, low_proj])\n",
    "        x = layers.SeparableConv2D(256, 3, padding=\"same\", use_bias=False, name=\"dec_sep1\")(x)\n",
    "        x = layers.BatchNormalization(name=\"dec_sep1_bn\")(x)\n",
    "        x = layers.ReLU(name=\"dec_relu1\")(x)\n",
    "        x = layers.SeparableConv2D(256, 3, padding=\"same\", use_bias=False, name=\"dec_sep2\")(x)\n",
    "        x = layers.BatchNormalization(name=\"dec_sep2_bn\")(x)\n",
    "        x = layers.ReLU(name=\"dec_relu2\")(x)\n",
    "\n",
    "        in_h, in_w = input_shape[0], input_shape[1]\n",
    "        x = layers.Resizing(in_h, in_w, interpolation=\"bilinear\", name=\"ups_logits\")(x)\n",
    "        logits = layers.Conv2D(num_classes, 1, padding=\"same\", name=\"logits\")(x)  # from_logits=True\n",
    "        return keras.Model(inputs=base.input, outputs=logits, name=\"deeplabv3plus_resnet50\")\n",
    "\n",
    "    # ---------------------- Metrics (ignore support) ----------------------\n",
    "    class MaskedMeanIoU(tf.keras.metrics.MeanIoU):\n",
    "        def __init__(self, num_classes: int, ignore_index: int = IGNORE_INDEX, name: str = \"masked_mIoU\", **kwargs):\n",
    "            super().__init__(num_classes=num_classes, name=name, **kwargs)\n",
    "            self._ignore_index = tf.cast(ignore_index, tf.int32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.int32)\n",
    "            y_pred = tf.cast(tf.argmax(y_pred, axis=-1), tf.int32)\n",
    "            mask = tf.cast(tf.not_equal(y_true, self._ignore_index), tf.float32)\n",
    "            if sample_weight is None:\n",
    "                sample_weight = mask\n",
    "            else:\n",
    "                sample_weight = tf.cast(sample_weight, tf.float32) * mask\n",
    "            # Clip labels to valid range where mask==1 to avoid OOB\n",
    "            y_true = tf.where(mask > 0, tf.clip_by_value(y_true, 0, NUM_CLASSES - 1), 0)\n",
    "            y_pred = tf.where(mask > 0, tf.clip_by_value(y_pred, 0, NUM_CLASSES - 1), 0)\n",
    "            return super().update_state(y_true, y_pred, sample_weight=sample_weight)\n",
    "\n",
    "    # ---------------------- MLflow utils ----------------------\n",
    "    def get_mlflow_client():\n",
    "        import mlflow\n",
    "        # Use local file-based tracking inside artifacts/mlruns\n",
    "        os.makedirs(\"artifacts/mlruns\", exist_ok=True)\n",
    "        tracking_uri = \"file://\" + os.path.abspath(\"artifacts/mlruns\")\n",
    "        mlflow.set_tracking_uri(tracking_uri)\n",
    "        exp_name = \"cityscapes-deeplabv3plus-8cls\"\n",
    "        mlflow.set_experiment(exp_name)\n",
    "        return mlflow\n",
    "\n",
    "    class MLflowLogger(keras.callbacks.Callback):\n",
    "        \"\"\"Minimal MLflow logger for Keras training.\"\"\"\n",
    "        def __init__(self, run_params: dict):\n",
    "            super().__init__()\n",
    "            self.run_params = run_params\n",
    "            self.mlflow = get_mlflow_client()\n",
    "            self._run = None\n",
    "\n",
    "        def on_train_begin(self, logs=None):\n",
    "            self._run = self.mlflow.start_run(run_name=f\"run-{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "            # log params once\n",
    "            self.mlflow.log_params(self.run_params)\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if logs:\n",
    "                # log metrics with step=epoch\n",
    "                self.mlflow.log_metrics(\n",
    "                    {\n",
    "                        \"loss\": float(logs.get(\"loss\", 0.0)),\n",
    "                        \"pix_acc\": float(logs.get(\"pix_acc\", 0.0)),\n",
    "                        \"masked_mIoU\": float(logs.get(\"masked_mIoU\", 0.0)),\n",
    "                        \"val_loss\": float(logs.get(\"val_loss\", 0.0)),\n",
    "                        \"val_pix_acc\": float(logs.get(\"val_pix_acc\", 0.0)),\n",
    "                        \"val_masked_mIoU\": float(logs.get(\"val_masked_mIoU\", 0.0)),\n",
    "                    },\n",
    "                    step=epoch + 1,\n",
    "                )\n",
    "\n",
    "        def on_train_end(self, logs=None):\n",
    "            # also log training CSV (if exists)\n",
    "            if os.path.exists(\"artifacts/train_log.csv\"):\n",
    "                self.mlflow.log_artifact(\"artifacts/train_log.csv\", artifact_path=\"logs\")\n",
    "            # Leave run open here; we'll end it in main() after logging the best model.\n",
    "            # (some users prefer ending here; we control it in main for clarity)\n",
    "\n",
    "    # ---------------------- Helpers: best checkpoint handling ----------------------\n",
    "    CKPT_PATTERN = re.compile(r\"weights\\.(\\d+)-([0-9]*\\.[0-9]+)\\.keras$\")  # epoch, val_mIoU\n",
    "\n",
    "    def find_best_checkpoint(ckpt_dir: str) -> Optional[str]:\n",
    "        if not os.path.isdir(ckpt_dir):\n",
    "            return None\n",
    "        best_path, best_score = None, -1.0\n",
    "        for fname in os.listdir(ckpt_dir):\n",
    "            m = CKPT_PATTERN.match(fname)\n",
    "            if m:\n",
    "                score = float(m.group(2))  # val_masked_mIoU\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_path = os.path.join(ckpt_dir, fname)\n",
    "        return best_path\n",
    "\n",
    "    # ---------------------- Train ----------------------\n",
    "    def main():\n",
    "        # pairs\n",
    "        train_x, train_y, miss_tr = gather_pairs(\"train\")\n",
    "        val_x,   val_y,   miss_va = gather_pairs(\"val\")\n",
    "        report_pairs(\"train\", train_x, train_y, miss_tr)\n",
    "        report_pairs(\"val\",   val_x,   val_y,   miss_va)\n",
    "\n",
    "        # datasets\n",
    "        train_ds = build_dataset(train_x, train_y, training=True)\n",
    "        val_ds   = build_dataset(val_x,   val_y,   training=False)\n",
    "\n",
    "        # probe batch\n",
    "        xb, yb, wb = next(iter(train_ds))\n",
    "        print(f\"x: {xb.shape} {xb.dtype} | y: {yb.shape} {yb.dtype} | w: {wb.shape} {wb.dtype}\")\n",
    "        tf.debugging.assert_less_equal(tf.reduce_max(yb), NUM_CLASSES - 1)\n",
    "        tf.debugging.assert_greater_equal(tf.reduce_min(yb), 0)\n",
    "\n",
    "        # model\n",
    "        model = DeeplabV3Plus_ResNet50(input_shape=(HEIGHT, WIDTH, 3), num_classes=NUM_CLASSES, output_stride=16)\n",
    "        # model.summary()  # keep disabled to avoid console spam\n",
    "\n",
    "        # compile\n",
    "        loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        metrics = [\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"pix_acc\"),\n",
    "            MaskedMeanIoU(num_classes=NUM_CLASSES, name=\"masked_mIoU\"),\n",
    "        ]\n",
    "        opt = keras.optimizers.Adam(LR)\n",
    "        model.compile(optimizer=opt, loss=loss, metrics=metrics)\n",
    "\n",
    "        # callbacks\n",
    "        ckpt_dir = \"artifacts/checkpoints\"; os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "        # MLflow logger (create early to own the run)\n",
    "        run_params = dict(\n",
    "            seed=SEED, height=HEIGHT, width=WIDTH, batch_size=BATCH_SIZE, lr=LR, epochs=EPOCHS,\n",
    "            num_classes=NUM_CLASSES, ignore_index=IGNORE_INDEX, backbone=\"ResNet50\",\n",
    "            dataset=\"Cityscapes-8cls\", augment=\"flip-only\", optimizer=\"Adam\"\n",
    "        )\n",
    "        mlflow_logger = MLflowLogger(run_params)\n",
    "\n",
    "        callbacks = [\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath=os.path.join(ckpt_dir, \"weights.{epoch:03d}-{val_masked_mIoU:.4f}.keras\"),\n",
    "                monitor=\"val_masked_mIoU\", mode=\"max\", save_best_only=True, save_weights_only=False),\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_masked_mIoU\", mode=\"max\", patience=10, restore_best_weights=True),\n",
    "            keras.callbacks.TerminateOnNaN(),\n",
    "            keras.callbacks.CSVLogger(\"artifacts/train_log.csv\"),\n",
    "            mlflow_logger,\n",
    "        ]\n",
    "\n",
    "        # train\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        # paths & saving\n",
    "        os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "        # 1) Save final model (last epoch state)\n",
    "        final_path = \"artifacts/deeplabv3plus_resnet50_cityscapes.keras\"\n",
    "        model.save(final_path)\n",
    "        print(f\"✅ Saved final -> {final_path}\")\n",
    "\n",
    "        # 2) Find and export the BEST checkpoint (highest val_masked_mIoU)\n",
    "        best_ckpt = find_best_checkpoint(ckpt_dir)\n",
    "        if best_ckpt is None:\n",
    "            print(\"⚠️ No checkpoint found in\", ckpt_dir)\n",
    "            best_export = None\n",
    "        else:\n",
    "            best_export = \"artifacts/deeplabv3plus_resnet50_cityscapes_best.keras\"\n",
    "            # Option A: copy the .keras best directly\n",
    "            shutil.copy2(best_ckpt, best_export)\n",
    "            print(f\"🏅 Best checkpoint -> {os.path.basename(best_ckpt)}\")\n",
    "            print(f\"✅ Exported best -> {best_export}\")\n",
    "\n",
    "        # ---------------- MLflow logging of artifacts & best model ----------------\n",
    "        mlflow = get_mlflow_client()  # reuse same client\n",
    "\n",
    "        # log key artifacts\n",
    "        mlflow.log_artifact(\"artifacts/train_log.csv\", artifact_path=\"logs\")\n",
    "        if os.path.exists(final_path):\n",
    "            mlflow.log_artifact(final_path, artifact_path=\"models\")\n",
    "        if best_export and os.path.exists(best_export):\n",
    "            # also log the best model file as an artifact\n",
    "            mlflow.log_artifact(best_export, artifact_path=\"models\")\n",
    "\n",
    "            # Optionally log as a MLflow \"model\" (full flavor) for best:\n",
    "            try:\n",
    "                import mlflow.keras\n",
    "                best_model = keras.models.load_model(best_export, compile=False)\n",
    "                mlflow.keras.log_model(best_model, artifact_path=\"best_model\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Could not log MLflow Keras model flavor: {e}\")\n",
    "\n",
    "        # Log final best metrics (from history) for convenience\n",
    "        try:\n",
    "            best_val_miou = max(history.history.get(\"val_masked_mIoU\", []))\n",
    "            mlflow.log_metric(\"best_val_masked_mIoU\", float(best_val_miou))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # End MLflow run cleanly\n",
    "        from mlflow import active_run, end_run\n",
    "        if active_run() is not None:\n",
    "            end_run()\n",
    "\n",
    "        print(\"🎯 MLflow run closed. Check artifacts/mlruns for the run.\")\n",
    "\n",
    "    # In notebook, call main():\n",
    "    os.makedirs(\"artifacts/mlruns\", exist_ok=True)\n",
    "    main()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8cadf2c2e5d50564",
   "metadata": {},
   "source": [
    "# In notebook (Python)\n",
    "from scripts.config import DataConfig, TrainConfig, AugmentConfig\n",
    "from scripts.train import train\n",
    "\n",
    "data_cfg = DataConfig(\n",
    "    data_root=\"../data\",\n",
    "    height=512, width=1024,\n",
    "    batch_size=2,\n",
    "    max_train_samples=100,\n",
    "    max_val_samples=100,\n",
    ")\n",
    "\n",
    "# Essai 1 : DeepLab ResNet50, augmentation légère\n",
    "aug_cfg = AugmentConfig(\n",
    "    enabled=False, hflip=True, vflip=False,\n",
    "    random_rotate_deg=3.0,\n",
    "    random_scale_min=0.85, random_scale_max=1.20,\n",
    "    random_crop=True,\n",
    "    brightness_delta=0.10, contrast_delta=0.10, saturation_delta=0.05, hue_delta=0.02,\n",
    "    gaussian_noise_std=0.00\n",
    ")\n",
    "train_cfg = TrainConfig(lr=3e-4, epochs=60, optimizer=\"adam\", exp_name=\"cityscapes-seg-8cls\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f72ed5e2-c322-4e78-a8bd-beb6267febf1",
   "metadata": {},
   "source": [
    "## Contrôle visuel de la data augmentation\n",
    "\n",
    "La cellule suivante pioche quelques paires image/masque, applique le pipeline Albumentations configuré (\\`aug_cfg\\`) et affiche les versions redimensionnées vs augmentées pour vérifier que les masques restent alignés.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "665d89bc-a7a7-4757-9e6f-892cb23509b4",
   "metadata": {},
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from scripts.config import AugmentConfig\n",
    "from scripts.augment import build_augment_fn\n",
    "from scripts.remap import build_cityscapes_8cls_lut, remap_labels\n",
    "\n",
    "lut = build_cityscapes_8cls_lut(data_cfg.ignore_index)\n",
    "no_aug_fn = build_augment_fn(AugmentConfig(enabled=False), data_cfg.height, data_cfg.width, data_cfg.ignore_index)\n",
    "augmented_fn = build_augment_fn(aug_cfg, data_cfg.height, data_cfg.width, data_cfg.ignore_index)\n",
    "\n",
    "def remap_to_training_ids(mask_np):\n",
    "    mask_tf = tf.convert_to_tensor(mask_np, dtype=tf.int32)\n",
    "    return remap_labels(mask_tf, lut).numpy()\n",
    "\n",
    "def colorize_mask(mask_np, palette=PALETTE_8, ignore_value=data_cfg.ignore_index):\n",
    "    rgb = np.zeros((mask_np.shape[0], mask_np.shape[1], 3), dtype=np.uint8)\n",
    "    for cls_id, color in palette.items():\n",
    "        rgb[mask_np == cls_id] = color\n",
    "    if ignore_value is not None:\n",
    "        rgb[mask_np == ignore_value] = (0, 0, 0)\n",
    "    return rgb\n",
    "\n",
    "def overlay_mask(image_uint8, mask_uint8, alpha=0.45):\n",
    "    colored = colorize_mask(mask_uint8)\n",
    "    return np.clip((1.0 - alpha) * image_uint8 + alpha * colored, 0, 255).astype(np.uint8)\n",
    "\n",
    "samples = pairs(\"train\")\n",
    "assert samples, \"Aucun couple image/masque trouvé — vérifie le dossier data.\"\n",
    "\n",
    "random.shuffle(samples)\n",
    "num_rows = min(3, len(samples))\n",
    "fig, axes = plt.subplots(num_rows, 6, figsize=(22, 5 * num_rows))\n",
    "if num_rows == 1:\n",
    "    axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "for row, (left_path, lbl_path) in enumerate(samples[:num_rows]):\n",
    "    raw_img = np.array(Image.open(left_path).convert(\"RGB\"), dtype=np.float32) / 255.0\n",
    "    raw_mask = np.array(Image.open(lbl_path), dtype=np.int32)\n",
    "\n",
    "    mask8 = remap_to_training_ids(raw_mask)\n",
    "\n",
    "    img_tf = tf.convert_to_tensor(raw_img, dtype=tf.float32)\n",
    "    mask_tf = tf.convert_to_tensor(mask8, dtype=tf.int32)\n",
    "\n",
    "    base_img, base_mask = no_aug_fn(img_tf, mask_tf)\n",
    "    aug_img, aug_mask = augmented_fn(img_tf, mask_tf)\n",
    "\n",
    "    base_img_u8 = np.clip(base_img.numpy() * 255.0, 0, 255).astype(np.uint8)\n",
    "    aug_img_u8 = np.clip(aug_img.numpy() * 255.0, 0, 255).astype(np.uint8)\n",
    "    base_mask_u8 = base_mask.numpy().astype(np.uint8)\n",
    "    aug_mask_u8 = aug_mask.numpy().astype(np.uint8)\n",
    "\n",
    "    base_mask_rgb = colorize_mask(base_mask_u8)\n",
    "    aug_mask_rgb = colorize_mask(aug_mask_u8)\n",
    "\n",
    "    axes[row, 0].imshow(base_img_u8)\n",
    "    axes[row, 0].set_title(\"Image (resize)\")\n",
    "    axes[row, 1].imshow(base_mask_rgb)\n",
    "    axes[row, 1].set_title(\"Masque (resize)\")\n",
    "    axes[row, 2].imshow(overlay_mask(base_img_u8, base_mask_u8))\n",
    "    axes[row, 2].set_title(\"Overlay resize\")\n",
    "    axes[row, 3].imshow(aug_img_u8)\n",
    "    axes[row, 3].set_title(\"Image augmentée\")\n",
    "    axes[row, 4].imshow(aug_mask_rgb)\n",
    "    axes[row, 4].set_title(\"Masque augmenté\")\n",
    "    axes[row, 5].imshow(overlay_mask(aug_img_u8, aug_mask_u8))\n",
    "    axes[row, 5].set_title(\"Overlay augmentée\")\n",
    "\n",
    "    for ax in axes[row]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a5c4cad5",
   "metadata": {},
   "source": [
    "from notebook.scripts.data import build_dataset\n",
    "\n",
    "val_ds = build_dataset(\n",
    "    data_cfg,\n",
    "    AugmentConfig(enabled=False),\n",
    "    split=\"val\",\n",
    "    training=False,\n",
    ")\n",
    "\n",
    "images, masks, _ = next(iter(val_ds))\n",
    "images_np = images.numpy()\n",
    "masks_np = masks.numpy()\n",
    "\n",
    "num_samples = min(3, images_np.shape[0])\n",
    "plt.figure(figsize=(15, 5 * num_samples))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    image = images_np[i]\n",
    "    mask = masks_np[i]\n",
    "\n",
    "    if image.dtype != np.uint8:\n",
    "        image_u8 = np.clip(image * 255.0, 0, 255).astype(np.uint8)\n",
    "    else:\n",
    "        image_u8 = image\n",
    "\n",
    "    mask_u8 = mask.astype(np.uint8)\n",
    "    mask_rgb = colorize_mask(mask_u8)\n",
    "    overlay_rgb = overlay_mask(image_u8, mask_u8)\n",
    "\n",
    "    overlay_on_black = overlay_mask(np.zeros_like(image_u8), mask_u8, alpha=1.0)\n",
    "    assert np.array_equal(overlay_on_black, mask_rgb), \"Overlay misaligned with mask (check dataset pipeline).\"\n",
    "\n",
    "    for j, (img, title) in enumerate([\n",
    "        (image_u8, \"Image (val)\"),\n",
    "        (mask_rgb, \"Masque colorisé\"),\n",
    "        (overlay_rgb, \"Overlay\"),\n",
    "    ]):\n",
    "        ax = plt.subplot(num_samples, 3, i * 3 + j + 1)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Échantillon {i + 1} — {title}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8270ff24-cf3d-4817-be01-31c3adc9f0ac",
   "metadata": {},
   "source": [
    "# Entraînement\n",
    "train(\"deeplab_resnet50\", data_cfg, train_cfg, aug_cfg)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
