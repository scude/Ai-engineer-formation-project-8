{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5eeaf89ff30992",
   "metadata": {},
   "source": [
    "# Projet 8"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_mode = True",
   "id": "5d0adc820682eef2"
  },
  {
   "cell_type": "code",
   "id": "84b61c778fafe1a",
   "metadata": {},
   "source": [
    "# --- Simple dataset scanner (counts only, no filenames, no JSON) ---\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# >>> Change this to your dataset root (WSL path) <<<\n",
    "ROOT = Path(\"../data\")\n",
    "\n",
    "IGNORE_HIDDEN = True       # ignore .git, __pycache__, etc.\n",
    "MAX_DIRS_TO_SHOW = 80      # limit directory lines for readability\n",
    "\n",
    "total_files = 0\n",
    "total_dirs = 0\n",
    "by_ext = Counter()\n",
    "by_dir = {}\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(ROOT):\n",
    "    # optionally hide hidden/internal dirs\n",
    "    if IGNORE_HIDDEN:\n",
    "        dirnames[:] = [d for d in dirnames if not d.startswith(\".\") and not d.startswith(\"__\")]\n",
    "    total_dirs += 1\n",
    "    rel = Path(dirpath).relative_to(ROOT) if Path(dirpath) != ROOT else Path(\".\")\n",
    "    by_dir[str(rel)] = len(filenames)\n",
    "    for fn in filenames:\n",
    "        by_ext[Path(fn).suffix.lower()] += 1\n",
    "    total_files += len(filenames)\n",
    "\n",
    "print(f\"[ROOT] {ROOT}\")\n",
    "print(f\"dirs={total_dirs:,}  files={total_files:,}\\n\")\n",
    "\n",
    "print(\"By extension (top 10):\")\n",
    "for ext, n in by_ext.most_common(10):\n",
    "    print(f\"  {ext or '(no ext)'}: {n:,}\")\n",
    "print()\n",
    "\n",
    "print(f\"Directory counts (first {MAX_DIRS_TO_SHOW}):\")\n",
    "for i, (rel, n) in enumerate(sorted(by_dir.items())):\n",
    "    if i >= MAX_DIRS_TO_SHOW:\n",
    "        print(\"  ... (truncated)\")\n",
    "        break\n",
    "    print(f\"  {rel}: {n}\")\n",
    "\n",
    "# -------- Cityscapes mini-summary (counts only) --------\n",
    "def count_pattern(base: Path, split: str, suffix: str) -> int:\n",
    "    split_dir = base / split\n",
    "    total = 0\n",
    "    if split_dir.exists():\n",
    "        for city_dir in split_dir.iterdir():\n",
    "            if city_dir.is_dir():\n",
    "                total += sum(1 for p in city_dir.iterdir()\n",
    "                             if p.is_file() and p.name.endswith(suffix))\n",
    "    return total\n",
    "\n",
    "print(\"\\n[Cityscapes summary]\")\n",
    "for split in (\"train\", \"val\", \"test\"):\n",
    "    gt_base = ROOT / \"gtFine\"\n",
    "    left_base = ROOT / \"leftImg8bit\"\n",
    "    label = count_pattern(gt_base, split, \"_gtFine_labelIds.png\")\n",
    "    color = count_pattern(gt_base, split, \"_gtFine_color.png\")\n",
    "    inst  = count_pattern(gt_base, split, \"_gtFine_instanceIds.png\")\n",
    "    poly  = count_pattern(gt_base, split, \"_gtFine_polygons.json\")\n",
    "    left  = count_pattern(left_base, split, \"_leftImg8bit.png\")\n",
    "    print(f\"  {split:5s}: leftImg8bit={left:6d}  labelIds={label:6d}  color={color:6d}  instanceIds={inst:6d}  polygons.json={poly:6d}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b8ee8ae56f5c54f",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"../data\")  # adapte si besoin\n",
    "SUF_LEFT = \"_leftImg8bit.png\"\n",
    "SUF_LBL  = \"_gtFine_labelIds.png\"\n",
    "\n",
    "def base_id(name: str) -> str:\n",
    "    return name[:-len(SUF_LEFT)] if name.endswith(SUF_LEFT) else name[:-len(SUF_LBL)]\n",
    "\n",
    "def split_counts(split: str):\n",
    "    left_dir = ROOT / \"leftImg8bit\" / split\n",
    "    lbl_dir  = ROOT / \"gtFine\"      / split\n",
    "    left = sorted(left_dir.rglob(f\"*{SUF_LEFT}\")) if left_dir.exists() else []\n",
    "    lbl  = sorted(lbl_dir.rglob (f\"*{SUF_LBL}\" )) if lbl_dir.exists()  else []\n",
    "    left_ids = {base_id(p.name) for p in left}\n",
    "    lbl_ids  = {base_id(p.name) for p in lbl}\n",
    "    paired = left_ids & lbl_ids\n",
    "    print(f\"{split:<5} | left={len(left):4d}  labels={len(lbl):4d}  paired={len(paired):4d}\")\n",
    "\n",
    "for sp in (\"train\", \"val\", \"test\"):\n",
    "    split_counts(sp)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3b30f7c4d8786b68",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "ROOT = Path(\"../data\")\n",
    "\n",
    "PALETTE = {\n",
    "    7:(128,64,128), 8:(244,35,232), 11:(70,70,70), 12:(102,102,156), 13:(190,153,153),\n",
    "    17:(153,153,153), 19:(250,170,30), 20:(220,220,0), 21:(107,142,35), 22:(152,251,152),\n",
    "    23:(70,130,180), 24:(220,20,60), 25:(255,0,0), 26:(0,0,142), 27:(0,0,70),\n",
    "    28:(0,60,100), 31:(0,80,100), 32:(0,0,230), 33:(119,11,32),\n",
    "}\n",
    "\n",
    "def pairs(split=\"val\"):\n",
    "    lbls = sorted((ROOT/\"gtFine\"/split).rglob(\"*_gtFine_labelIds.png\"))\n",
    "    out = []\n",
    "    for lp in lbls:\n",
    "        stem = lp.name.replace(\"_gtFine_labelIds.png\", \"\")\n",
    "        city = lp.parent.name\n",
    "        left = ROOT/\"leftImg8bit\"/split/city/(stem+\"_leftImg8bit.png\")\n",
    "        if left.exists():\n",
    "            out.append((left, lp))\n",
    "    return out\n",
    "\n",
    "def colorize(ids: np.ndarray) -> Image.Image:\n",
    "    h, w = ids.shape\n",
    "    rgb = np.zeros((h, w, 3), np.uint8)\n",
    "    for k, c in PALETTE.items():\n",
    "        rgb[ids == k] = c\n",
    "    return Image.fromarray(rgb, \"RGB\")\n",
    "\n",
    "def overlay(img: Image.Image, mask_rgb: Image.Image, alpha=0.5) -> Image.Image:\n",
    "    a = np.asarray(img.convert(\"RGB\"), np.float32)\n",
    "    b = np.asarray(mask_rgb, np.float32)\n",
    "    return Image.fromarray(np.clip((1-alpha)*a + alpha*b, 0, 255).astype(np.uint8))\n",
    "\n",
    "samples = pairs(\"val\")\n",
    "assert samples, \"No pairs found ‚Äî check your paths.\"\n",
    "random.shuffle(samples)\n",
    "k = 3\n",
    "\n",
    "plt.figure(figsize=(15, 5*k))\n",
    "for i, (left_p, lbl_p) in enumerate(samples[:k]):\n",
    "    left = Image.open(left_p).convert(\"RGB\")\n",
    "    ids  = np.array(Image.open(lbl_p))\n",
    "    mask = colorize(ids)\n",
    "    over = overlay(left, mask, alpha=0.45)\n",
    "    for j, (img, title) in enumerate([(left,\"leftImg8bit\"),(mask,\"labelIds (colored)\"),(over,\"overlay\")]):\n",
    "        ax = plt.subplot(k, 3, i*3 + j + 1)\n",
    "        ax.imshow(img); ax.set_title(f\"{left_p.parent.name} ‚Äî {title}\", fontsize=10); ax.axis(\"off\")\n",
    "plt.tight_layout(); plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a81dea999fabce6b",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f540b1886f92c7b1",
   "metadata": {},
   "source": [
    "\n",
    "### Remapping Cityscapes 32‚Üí8 classes\n",
    "\n",
    "V√©rifier la balance des classes !!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1b632fc00041fe27",
   "metadata": {},
   "source": [
    "# --- 32‚Üí8 mapping (Cityscapes labelIds -> 8-class IDs), ignore = 255 ---\n",
    "import numpy as np\n",
    "\n",
    "CS_LABELID_TO_8 = {\n",
    "    # 0..5 (voids) -> ignore by LUT fill (no need to list)\n",
    "    6: 0,\n",
    "    7: 0,  9: 0, 10: 0,           # road-like: road, parking, rail track\n",
    "    8: 1,                         # sidewalk\n",
    "    11: 2, 12: 2, 13: 2, 14: 2, 15: 2, 16: 2,   # building + barriers\n",
    "    17: 3, 18: 3, 19: 3, 20: 3,                 # traffic objs (pole/ts/tl)\n",
    "    21: 4, 22: 4,                                 # vegetation + terrain\n",
    "    23: 5,                                       # sky\n",
    "    24: 6, 25: 6,                                 # person + rider\n",
    "    26: 7, 27: 7, 28: 7, 29: 7, 30: 7, 31: 7, 32: 7, 33: 7,  # vehicles\n",
    "}\n",
    "\n",
    "def build_labelid_to8_lut(ignore_value: int = 255) -> np.ndarray:\n",
    "    \"\"\"Create a 256-entry LUT mapping Cityscapes labelIds -> {0..7} or 255(ignore).\"\"\"\n",
    "    lut = np.full(256, ignore_value, dtype=np.uint8)\n",
    "    for k, v in CS_LABELID_TO_8.items():\n",
    "        lut[k] = v\n",
    "    return lut\n",
    "\n",
    "LUT_32TO8 = build_labelid_to8_lut(ignore_value=255)\n",
    "\n",
    "def remap_labelids_to8(arr_uint16: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Vectorized remap of HxW labelIds (uint16/uint8) to 8-class IDs with 255 ignore.\"\"\"\n",
    "    arr = arr_uint16.astype(np.uint16)\n",
    "    arr = np.minimum(arr, 255).astype(np.uint8)\n",
    "    return LUT_32TO8[arr]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4090ae75134d47fe",
   "metadata": {},
   "source": [
    "PALETTE_8 = {\n",
    "    0:(128,64,128),   # road\n",
    "    1:(244,35,232),   # sidewalk\n",
    "    2:(70,70,70),     # building+barrier\n",
    "    3:(220,220,0),    # traffic objs\n",
    "    4:(107,142,35),   # vegetation/terrain\n",
    "    5:(70,130,180),   # sky\n",
    "    6:(220,20,60),    # person+rider\n",
    "    7:(0,0,142),      # vehicle\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "623c52d0928c016c",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def colorize_8(label8: np.ndarray, palette: dict) -> Image.Image:\n",
    "    h, w = label8.shape\n",
    "    rgb = np.zeros((h, w, 3), np.uint8)\n",
    "    for k, c in palette.items():\n",
    "        rgb[label8 == k] = c\n",
    "    return Image.fromarray(rgb, \"RGB\")\n",
    "\n",
    "sample_lbl = next(Path(\"../data/gtFine/val/frankfurt\").glob(\"*_gtFine_labelIds.png\"))\n",
    "arr = np.array(Image.open(sample_lbl))\n",
    "arr8 = remap_labelids_to8(arr)\n",
    "plt.figure(figsize=(8,4)); plt.imshow(colorize_8(arr8, PALETTE_8)); plt.axis(\"off\"); plt.title(\"8-class mask\"); plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "71f824d108f9b17f",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# ‚Üì‚Üì‚Üì Quieter TensorFlow logs (set BEFORE importing tf)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"     # 0=all, 1=INFO off, 2=INFO+WARNING off, 3=all off\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"  # avoid grabbing all GPU memory\n",
    "# Optional: disable oneDNN (removes the \"oneDNN custom ops are on\" line, and tiny numeric diffs)\n",
    "# os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from absl import logging as absl_logging\n",
    "absl_logging.set_verbosity(absl_logging.ERROR)  # reduce absl spam\n",
    "\n",
    "# (Optional) confirm GPU + set memory growth (extra safety)\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for g in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(g, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"TF:\", tf.__version__, \"| GPUs:\", gpus)\n",
    "\n",
    "# ==========================\n",
    "# Cityscapes 32‚Üí8 remapping\n",
    "# ==========================\n",
    "import numpy as np\n",
    "\n",
    "# 8 classes for embedded use (ignore=255):\n",
    "# 0=road (7,9,10) | 1=sidewalk(8) | 2=building+barriers(11‚Äì16) | 3=traffic objs(17‚Äì20)\n",
    "# 4=vegetation+terrain(21,22) | 5=sky(23) | 6=person+rider(24,25) | 7=vehicle(26‚Äì33)\n",
    "CS_LABELID_TO_8 = {\n",
    "    6:0,\n",
    "    7:0, 9:0, 10:0,\n",
    "    8:1,\n",
    "    11:2, 12:2, 13:2, 14:2, 15:2, 16:2,\n",
    "    17:3, 18:3, 19:3, 20:3,\n",
    "    21:4, 22:4,\n",
    "    23:5,\n",
    "    24:6, 25:6,\n",
    "    26:7, 27:7, 28:7, 29:7, 30:7, 31:7, 32:7, 33:7,\n",
    "}\n",
    "\n",
    "def build_labelid_to8_lut(ignore_value: int = 255) -> np.ndarray:\n",
    "    lut = np.full(256, ignore_value, dtype=np.uint8)\n",
    "    for k, v in CS_LABELID_TO_8.items():\n",
    "        lut[k] = v\n",
    "    return lut\n",
    "\n",
    "LUT_32TO8 = build_labelid_to8_lut(ignore_value=255)\n",
    "LUT_TF = tf.convert_to_tensor(LUT_32TO8, dtype=tf.uint8)  # shape [256]\n",
    "\n",
    "# ===================\n",
    "# Dataset (tf.data)\n",
    "# ===================\n",
    "from pathlib import Path\n",
    "ROOT = Path(\"../data\")               # <<< change if needed (WSL path)\n",
    "INPUT_SIZE = (512, 1024)             # (H, W)\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "SUF_LEFT = \"_leftImg8bit.png\"\n",
    "SUF_LBL  = \"_gtFine_labelIds.png\"\n",
    "\n",
    "def list_pairs(split: str):\n",
    "    \"\"\"Return two aligned lists: left paths and label paths for a given split.\"\"\"\n",
    "    lefts, labels = [], []\n",
    "    lbl_root = ROOT / \"gtFine\" / split\n",
    "    for lbl in sorted(lbl_root.rglob(f\"*{SUF_LBL}\")):\n",
    "        city = lbl.parent.name\n",
    "        stem = lbl.name.replace(SUF_LBL, \"\")\n",
    "        left = ROOT / \"leftImg8bit\" / split / city / f\"{stem}{SUF_LEFT}\"\n",
    "        if left.exists():\n",
    "            lefts.append(str(left))\n",
    "            labels.append(str(lbl))\n",
    "    if not lefts:\n",
    "        raise FileNotFoundError(f\"No pairs found for split='{split}'. Check your paths under {ROOT}.\")\n",
    "    return lefts, labels\n",
    "\n",
    "def decode_and_preprocess(left_path, lbl_path, training: bool):\n",
    "    # 1) Read bytes\n",
    "    left_bytes = tf.io.read_file(left_path)\n",
    "    lbl_bytes  = tf.io.read_file(lbl_path)\n",
    "\n",
    "    # 2) Decode\n",
    "    img = tf.io.decode_png(left_bytes, channels=3)     # uint8 [H,W,3]\n",
    "    lab = tf.io.decode_png(lbl_bytes,  channels=1)     # uint8/16 [H,W,1]\n",
    "\n",
    "    # 3) To workable dtypes\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)   # [0,1]\n",
    "    lab = tf.cast(lab, tf.int32)                          # index dtype for LUT\n",
    "\n",
    "    # 4) Remap 32‚Üí8 via LUT\n",
    "    lab_clipped = tf.minimum(lab, 255)\n",
    "    lab8 = tf.gather(LUT_TF, lab_clipped)                 # uint8 [H,W,1]\n",
    "    lab8 = tf.squeeze(lab8, axis=-1)                      # uint8 [H,W]\n",
    "\n",
    "    # 5) Simple augment (sync flip)\n",
    "    if training:\n",
    "        do_flip = tf.random.uniform(()) > 0.5\n",
    "        img  = tf.cond(do_flip, lambda: tf.image.flip_left_right(img), lambda: img)\n",
    "        lab8 = tf.cond(do_flip, lambda: tf.image.flip_left_right(lab8[..., None])[:, :, 0], lambda: lab8)\n",
    "\n",
    "    # 6) Resize (labels in nearest, keep uint8)\n",
    "    img  = tf.image.resize(img,  INPUT_SIZE, method=\"bilinear\")\n",
    "    lab8 = tf.cast(tf.image.resize(lab8[..., None], INPUT_SIZE, method=\"nearest\")[:, :, 0], tf.uint8)\n",
    "\n",
    "    # 7) Ignore handling ‚Üí sample_weight (float32); labels safe (uint8‚Üíint32)\n",
    "    ignore_val = tf.constant(255, dtype=tf.uint8)\n",
    "    ignore = tf.equal(lab8, ignore_val)  # bool [H,W]\n",
    "\n",
    "    weights = tf.where(ignore,\n",
    "                       tf.zeros_like(lab8, dtype=tf.float32),\n",
    "                       tf.ones_like(lab8,  dtype=tf.float32))              # float32 [H,W]\n",
    "\n",
    "    lab8_safe = tf.where(ignore,\n",
    "                         tf.zeros_like(lab8),   # uint8 0 (will be masked by weights anyway)\n",
    "                         lab8)\n",
    "    labels = tf.cast(lab8_safe, tf.int32)                                    # int32 [H,W]\n",
    "\n",
    "    return img, labels, weights\n",
    "\n",
    "def make_dataset(split: str, batch_size: int = BATCH_SIZE, training: bool = True) -> tf.data.Dataset:\n",
    "    lefts, labels = list_pairs(split)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((lefts, labels))\n",
    "    if training:\n",
    "        ds = ds.shuffle(buffer_size=min(len(lefts), 2000), reshuffle_each_iteration=True)\n",
    "    ds = ds.map(lambda l, y: decode_and_preprocess(l, y, training),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size, drop_remainder=training)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# ==============\n",
    "# Smoke test\n",
    "# ==============\n",
    "train_ds = make_dataset(\"train\", batch_size=2, training=True)\n",
    "xb, yb, wb = next(iter(train_ds))\n",
    "print(\"x:\", xb.shape, xb.dtype, \"| y:\", yb.shape, yb.dtype, \"| w:\", wb.shape, wb.dtype)\n",
    "\n",
    "# Example compile/fit (model must output logits with 8 channels)\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# opt  = tf.keras.optimizers.Adam(1e-3)\n",
    "# model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
    "# model.fit(train_ds,\n",
    "#           validation_data=make_dataset(\"val\", batch_size=2, training=False),\n",
    "#           epochs=1)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fa8eafa784ea3287",
   "metadata": {},
   "source": [
    "# ==== Class balance for Cityscapes 8 classes (with ignore=255) ====\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# ---- Config (adapt if needed) ----\n",
    "ROOT = Path(\"../data\")          # dataset root (WSL path)\n",
    "SPLIT = \"train\"                 # \"train\" | \"val\" | \"test\"\n",
    "SUF_LBL = \"_gtFine_labelIds.png\"\n",
    "\n",
    "# 8-class names (your mapping)\n",
    "CLASS8_NAMES = [\n",
    "    \"road\", \"sidewalk\", \"building+barriers\", \"traffic-objs\",\n",
    "    \"vegetation+terrain\", \"sky\", \"person+rider\", \"vehicle\"\n",
    "]\n",
    "\n",
    "# If LUT_32TO8 not in scope, (re)build it quickly:\n",
    "try:\n",
    "    LUT_32TO8\n",
    "except NameError:\n",
    "    CS_LABELID_TO_8 = {\n",
    "        6:0, 7:0, 9:0, 10:0, 8:1, 11:2,12:2,13:2,14:2,15:2,16:2,\n",
    "        17:3,18:3,19:3,20:3, 21:4,22:4, 23:5, 24:6,25:6,\n",
    "        26:7,27:7,28:7,29:7,30:7,31:7,32:7,33:7,\n",
    "    }\n",
    "    LUT_32TO8 = np.full(256, 255, dtype=np.uint8)\n",
    "    for k, v in CS_LABELID_TO_8.items():\n",
    "        LUT_32TO8[k] = v\n",
    "\n",
    "def remap_to8_np(arr_label_ids: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"arr_label_ids: HxW uint16/uint8 -> HxW uint8 in {0..7,255}\"\"\"\n",
    "    arr = arr_label_ids.astype(np.uint16)\n",
    "    arr = np.minimum(arr, 255).astype(np.uint8)\n",
    "    return LUT_32TO8[arr]\n",
    "\n",
    "def class_balance(split: str = SPLIT):\n",
    "    lbl_paths = sorted((ROOT/\"gtFine\"/split).rglob(f\"*{SUF_LBL}\"))\n",
    "    assert lbl_paths, f\"No labels found under {ROOT}/gtFine/{split}\"\n",
    "    counts = np.zeros(8, dtype=np.int64)\n",
    "    ignore = 0\n",
    "    for i, p in enumerate(lbl_paths, 1):\n",
    "        lab = np.array(Image.open(p))         # (H,W) uint16/uint8\n",
    "        lab8 = remap_to8_np(lab)              # (H,W) uint8\n",
    "        m_ignore = (lab8 == 255)\n",
    "        ignore += int(m_ignore.sum())\n",
    "        # bincount only on valid pixels\n",
    "        c = np.bincount(lab8[~m_ignore].ravel(), minlength=8)\n",
    "        counts += c[:8]\n",
    "        if i % 500 == 0 or i == len(lbl_paths):\n",
    "            print(f\"[{split}] processed {i}/{len(lbl_paths)} images...\", end=\"\\r\")\n",
    "    print()\n",
    "    total_valid = int(counts.sum())\n",
    "    total_pixels = total_valid + ignore\n",
    "    freqs = counts / max(total_valid, 1)\n",
    "    return counts, ignore, total_valid, total_pixels, freqs\n",
    "\n",
    "counts, ignore, total_valid, total_pixels, freqs = class_balance(\"train\")\n",
    "\n",
    "print(\"\\n=== Class balance (train) ===\")\n",
    "for k, (name, n, f) in enumerate(zip(CLASS8_NAMES, counts, freqs)):\n",
    "    print(f\"{k}: {name:<20s}  pixels={n:,}   freq={f:.4%}\")\n",
    "print(f\"\\nignore pixels (==255): {ignore:,}\")\n",
    "print(f\"total valid pixels:     {total_valid:,}\")\n",
    "print(f\"total pixels (incl. ignore): {total_pixels:,}\")\n",
    "\n",
    "# ---- Optional: derive class weights ----\n",
    "# Inverse-frequency, normalized to mean=1 (good starting point)\n",
    "weights_inv = (1.0 / np.maximum(freqs, 1e-12))\n",
    "weights_inv = weights_inv / weights_inv.mean()\n",
    "print(\"\\nSuggested class weights (inverse-freq, mean‚âà1):\")\n",
    "for k, (name, w) in enumerate(zip(CLASS8_NAMES, weights_inv)):\n",
    "    print(f\"{k}: {name:<20s}  w={w:.3f}\")\n",
    "\n",
    "# Median-frequency balancing (alternative)\n",
    "median_f = np.median(freqs[freqs > 0])\n",
    "weights_med = median_f / np.maximum(freqs, 1e-12)\n",
    "weights_med = weights_med / weights_med.mean()\n",
    "print(\"\\nSuggested class weights (median-freq, mean‚âà1):\")\n",
    "for k, (name, w) in enumerate(zip(CLASS8_NAMES, weights_med)):\n",
    "    print(f\"{k}: {name:<20s}  w={w:.3f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8433da3712483ac",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# expects: counts (np.array shape [8]), freqs (shape [8]), ignore (int),\n",
    "#          total_valid (int), total_pixels (int), CLASS8_NAMES (list of 8 str)\n",
    "\n",
    "# ---- 1) Bar chart des 8 classes (tri√© d√©croissant) ----\n",
    "order = np.argsort(freqs)[::-1]\n",
    "names_sorted = [CLASS8_NAMES[i] for i in order]\n",
    "freqs_sorted = freqs[order]\n",
    "counts_sorted = counts[order]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.bar(range(len(names_sorted)), freqs_sorted)  # no explicit colors\n",
    "plt.xticks(range(len(names_sorted)), names_sorted, rotation=20, ha=\"right\")\n",
    "plt.ylabel(\"Frequency (share of valid pixels)\")\n",
    "plt.title(\"Cityscapes (train) ‚Äî Class balance (8 classes)\")\n",
    "\n",
    "# annotations: % + millions de pixels\n",
    "for i, (b, f, c) in enumerate(zip(bars, freqs_sorted, counts_sorted)):\n",
    "    plt.text(b.get_x() + b.get_width()/2,\n",
    "             b.get_height() + 0.002,\n",
    "             f\"{f*100:.1f}%\\n{c/1e6:.1f}M\",\n",
    "             ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.ylim(0, max(freqs_sorted)*1.15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 2) Valid vs Ignore (pour info) ----\n",
    "valid_share = total_valid / total_pixels\n",
    "ignore_share = 1.0 - valid_share\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "bars2 = plt.bar([0,1], [valid_share, ignore_share])\n",
    "plt.xticks([0,1], [\"valid\", \"ignore (==255)\"])\n",
    "plt.ylabel(\"Share of total pixels\")\n",
    "plt.title(\"Valid vs Ignore pixels (train)\")\n",
    "\n",
    "for x, v in zip([0,1], [valid_share, ignore_share]):\n",
    "    plt.text(x, v + 0.005, f\"{v*100:.1f}%\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.ylim(0, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# In notebook (Python)\n",
    "from scripts.config import DataConfig, TrainConfig, AugmentConfig\n",
    "from scripts.train import train\n",
    "\n",
    "data_cfg = DataConfig(\n",
    "    data_root=\"../data\",\n",
    "    height=512, width=1024,\n",
    "    batch_size=2,\n",
    "    max_train_samples=100,\n",
    "    max_val_samples=100,\n",
    ")\n",
    "\n",
    "# Essai 1 : DeepLab ResNet50, augmentation l√©g√®re\n",
    "aug_cfg = AugmentConfig(\n",
    "    enabled=False, hflip=True, vflip=False,\n",
    "    random_rotate_deg=3.0,\n",
    "    random_scale_min=0.85, random_scale_max=1.20,\n",
    "    random_crop=True,\n",
    "    brightness_delta=0.10, contrast_delta=0.10, saturation_delta=0.05, hue_delta=0.02,\n",
    "    gaussian_noise_std=0.00\n",
    ")\n",
    "train_cfg = TrainConfig(lr=3e-4, epochs=60, optimizer=\"adam\", exp_name=\"cityscapes-seg-8cls\")\n"
   ],
   "id": "a76257cdcbb6ea60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Contr√¥le visuel de la data augmentation\n",
    "\n",
    "La cellule suivante pioche quelques paires image/masque, applique le pipeline Albumentations configur√© (\\`aug_cfg\\`) et affiche les versions redimensionn√©es vs augment√©es pour v√©rifier que les masques restent align√©s.\n"
   ],
   "id": "b3b9819cc887027b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not train_mode :\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "\n",
    "    from scripts.config import AugmentConfig\n",
    "    from scripts.augment import build_augment_fn\n",
    "    from scripts.remap import build_cityscapes_8cls_lut, remap_labels\n",
    "\n",
    "    lut = build_cityscapes_8cls_lut(data_cfg.ignore_index)\n",
    "    no_aug_fn = build_augment_fn(AugmentConfig(enabled=False), data_cfg.height, data_cfg.width, data_cfg.ignore_index)\n",
    "    augmented_fn = build_augment_fn(aug_cfg, data_cfg.height, data_cfg.width, data_cfg.ignore_index)\n",
    "\n",
    "    def remap_to_training_ids(mask_np):\n",
    "        mask_tf = tf.convert_to_tensor(mask_np, dtype=tf.int32)\n",
    "        return remap_labels(mask_tf, lut).numpy()\n",
    "\n",
    "    def colorize_mask(mask_np, palette=PALETTE_8, ignore_value=data_cfg.ignore_index):\n",
    "        rgb = np.zeros((mask_np.shape[0], mask_np.shape[1], 3), dtype=np.uint8)\n",
    "        for cls_id, color in palette.items():\n",
    "            rgb[mask_np == cls_id] = color\n",
    "        if ignore_value is not None:\n",
    "            rgb[mask_np == ignore_value] = (0, 0, 0)\n",
    "        return rgb\n",
    "\n",
    "    def overlay_mask(image_uint8, mask_uint8, alpha=0.45):\n",
    "        colored = colorize_mask(mask_uint8)\n",
    "        return np.clip((1.0 - alpha) * image_uint8 + alpha * colored, 0, 255).astype(np.uint8)\n",
    "\n",
    "    samples = pairs(\"train\")\n",
    "    assert samples, \"Aucun couple image/masque trouv√© ‚Äî v√©rifie le dossier data.\"\n",
    "\n",
    "    random.shuffle(samples)\n",
    "    num_rows = min(3, len(samples))\n",
    "    fig, axes = plt.subplots(num_rows, 6, figsize=(22, 5 * num_rows))\n",
    "    if num_rows == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "    for row, (left_path, lbl_path) in enumerate(samples[:num_rows]):\n",
    "        raw_img = np.array(Image.open(left_path).convert(\"RGB\"), dtype=np.float32) / 255.0\n",
    "        raw_mask = np.array(Image.open(lbl_path), dtype=np.int32)\n",
    "\n",
    "        mask8 = remap_to_training_ids(raw_mask)\n",
    "\n",
    "        img_tf = tf.convert_to_tensor(raw_img, dtype=tf.float32)\n",
    "        mask_tf = tf.convert_to_tensor(mask8, dtype=tf.int32)\n",
    "\n",
    "        base_img, base_mask = no_aug_fn(img_tf, mask_tf)\n",
    "        aug_img, aug_mask = augmented_fn(img_tf, mask_tf)\n",
    "\n",
    "        base_img_u8 = np.clip(base_img.numpy() * 255.0, 0, 255).astype(np.uint8)\n",
    "        aug_img_u8 = np.clip(aug_img.numpy() * 255.0, 0, 255).astype(np.uint8)\n",
    "        base_mask_u8 = base_mask.numpy().astype(np.uint8)\n",
    "        aug_mask_u8 = aug_mask.numpy().astype(np.uint8)\n",
    "\n",
    "        base_mask_rgb = colorize_mask(base_mask_u8)\n",
    "        aug_mask_rgb = colorize_mask(aug_mask_u8)\n",
    "\n",
    "        axes[row, 0].imshow(base_img_u8)\n",
    "        axes[row, 0].set_title(\"Image (resize)\")\n",
    "        axes[row, 1].imshow(base_mask_rgb)\n",
    "        axes[row, 1].set_title(\"Masque (resize)\")\n",
    "        axes[row, 2].imshow(overlay_mask(base_img_u8, base_mask_u8))\n",
    "        axes[row, 2].set_title(\"Overlay resize\")\n",
    "        axes[row, 3].imshow(aug_img_u8)\n",
    "        axes[row, 3].set_title(\"Image augment√©e\")\n",
    "        axes[row, 4].imshow(aug_mask_rgb)\n",
    "        axes[row, 4].set_title(\"Masque augment√©\")\n",
    "        axes[row, 5].imshow(overlay_mask(aug_img_u8, aug_mask_u8))\n",
    "        axes[row, 5].set_title(\"Overlay augment√©e\")\n",
    "\n",
    "        for ax in axes[row]:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "fae32854cfcd9ef9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "    from notebook.scripts.data import build_dataset\n",
    "\n",
    "    val_ds = build_dataset(\n",
    "        data_cfg,\n",
    "        AugmentConfig(enabled=False),\n",
    "        split=\"val\",\n",
    "        training=False,\n",
    "    )\n",
    "\n",
    "    images, masks, _ = next(iter(val_ds))\n",
    "    images_np = images.numpy()\n",
    "    masks_np = masks.numpy()\n",
    "\n",
    "    num_samples = min(3, images_np.shape[0])\n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image = images_np[i]\n",
    "        mask = masks_np[i]\n",
    "\n",
    "        if image.dtype != np.uint8:\n",
    "            image_u8 = np.clip(image * 255.0, 0, 255).astype(np.uint8)\n",
    "        else:\n",
    "            image_u8 = image\n",
    "\n",
    "        mask_u8 = mask.astype(np.uint8)\n",
    "        mask_rgb = colorize_mask(mask_u8)\n",
    "        overlay_rgb = overlay_mask(image_u8, mask_u8)\n",
    "\n",
    "        overlay_on_black = overlay_mask(np.zeros_like(image_u8), mask_u8, alpha=1.0)\n",
    "        assert np.array_equal(overlay_on_black, mask_rgb), \"Overlay misaligned with mask (check dataset pipeline).\"\n",
    "\n",
    "        for j, (img, title) in enumerate([\n",
    "            (image_u8, \"Image (val)\"),\n",
    "            (mask_rgb, \"Masque coloris√©\"),\n",
    "            (overlay_rgb, \"Overlay\"),\n",
    "        ]):\n",
    "            ax = plt.subplot(num_samples, 3, i * 3 + j + 1)\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(f\"√âchantillon {i + 1} ‚Äî {title}\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "44c2b5ff61d6ffcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### UNet Mini\n",
    "Mod√®le U-Net compact utilis√© comme r√©f√©rence rapide pour valider le pipeline d'entra√Ænement.\n",
    "Sa l√©g√®ret√© le rend adapt√© aux tests it√©ratifs sur Cityscapes r√©duit.\n"
   ],
   "id": "b54c83afdf449ad0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    from dataclasses import replace\n",
    "    unet_mini_cfg = replace(\n",
    "        train_cfg,\n",
    "        output_dir=\"artifacts/unet_mini\",\n",
    "    )\n",
    "    train(\"unet_mini\", data_cfg, unet_mini_cfg, aug_cfg)\n"
   ],
   "id": "f08d845ee14a58e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### UNet VGG16\n",
    "Architecture U-Net √† encodeur VGG16 offrant une capacit√© accrue pour capturer des d√©tails fins.\n",
    "Elle reste compatible avec notre pipeline et sert de baseline plus profonde.\n",
    "\n",
    "/!\\ Le d√©codeur √† √©t√© r√©duit sur plusieurs couches par soucis de consommation de vram\n"
   ],
   "id": "625be39c1c74f8c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    unet_vgg16_cfg = replace(\n",
    "        train_cfg,\n",
    "        output_dir=\"artifacts/unet_vgg16\",\n",
    "    )\n",
    "    train(\"unet_vgg16\", data_cfg, unet_vgg16_cfg, aug_cfg)\n"
   ],
   "id": "968a91d468deb367",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### MobileDet Seg\n",
    "Variante segmentation de MobileDet pens√©e pour l'inf√©rence embarqu√©e tout en conservant une pr√©cision correcte.\n",
    "Ce mod√®le illustre un compromis agressif entre vitesse et qualit√©.\n"
   ],
   "id": "97eac04f2c1a0511"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    mobiledet_seg_cfg = replace(\n",
    "        train_cfg,\n",
    "        output_dir=\"artifacts/mobiledet_seg\",\n",
    "    )\n",
    "    train(\"mobiledet_seg\", data_cfg, mobiledet_seg_cfg, aug_cfg)\n"
   ],
   "id": "17cc0800dcd2447b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### YOLOv9 Seg\n",
    "D√©clinaison segmentation de YOLOv9 visant une extraction simultan√©e des instances et des masques.\n",
    "Nous l'√©valuons pour mesurer le gain potentiel des architectures one-stage.\n"
   ],
   "id": "dda9ed5cbc57b167"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    yolov9_seg_cfg = replace(\n",
    "        train_cfg,\n",
    "        output_dir=\"artifacts/yolov9_seg\",\n",
    "    )\n",
    "    train(\"yolov9_seg\", data_cfg, yolov9_seg_cfg, aug_cfg)\n"
   ],
   "id": "2bfc844613848b5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## deeplab resnet50",
   "id": "e73d2a8040472e9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # Entra√Ænement\n",
    "    train(\"deeplab_resnet50\", data_cfg, train_cfg, aug_cfg)"
   ],
   "id": "c919827d0f66cddd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üß© Vue d‚Äôensemble des r√©sultats\n",
    "\n",
    "| Mod√®le                     |   Dur√©e  | `masked_mIoU` (train) | `val_masked_mIoU` | `pix_acc` | `val_pix_acc` | `dice_coef` | `val_dice_coef` |\n",
    "| :------------------------- | :------: | :-------------------: | :---------------: | :-------: | :-----------: | :---------: | :-------------: |\n",
    "| **DeepLabV3+ (ResNet50)**  | 13.4 min |       **0.947**       |     **0.639**     | **0.989** |   **0.872**   |  **0.965**  |    **0.716**    |\n",
    "| **YOLOv9_seg (simplifi√©)** | 10.5 min |         0.689         |       0.400       |   0.913   |     0.714     |    0.753    |      0.494      |\n",
    "| **MobileDet_seg**          | 16.3 min |         0.938         |       0.502       |   0.987   |     0.779     |    0.953    |      0.600      |\n",
    "| **U-Net VGG16**            | 29.7 min |         0.903         |       0.542       |   0.977   |     0.805     |    0.923    |      0.633      |\n",
    "| **U-Net mini**             |  6.1 min |         0.563         |       0.319       |   0.851   |     0.634     |    0.650    |      0.407      |\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Interpr√©tation m√©trique par m√©trique\n",
    "\n",
    "### üü¶ `masked_mIoU` (train)\n",
    "\n",
    "* Mesure principale de segmentation (intersection sur union moyenne).\n",
    "* Tous sauf U-Net mini > 0.9 en entra√Ænement ‚Üí bon apprentissage.\n",
    "* U-Net mini (0.56) : trop l√©ger, manque de capacit√©.\n",
    "\n",
    "### üüß `val_masked_mIoU`\n",
    "\n",
    "* √âvalue la **g√©n√©ralisation**.\n",
    "* DeepLab (0.639) est **nettement sup√©rieur** aux autres.\n",
    "* U-Net VGG16 (0.54) et MobileDet (0.50) suivent derri√®re.\n",
    "* YOLOv9 seg (0.40) et U-Net mini (0.32) d√©crochent clairement.\n",
    "\n",
    "### üü© `val_pix_acc`\n",
    "\n",
    "* Corr√©lation assez bonne avec `val_mIoU`.\n",
    "* DeepLab atteint 0.87 ‚Üí tr√®s bonne segmentation globale.\n",
    "* U-Net VGG16 ‚âà 0.80 ‚Üí correct.\n",
    "* Les autres chutent < 0.78.\n",
    "\n",
    "### üü™ `val_dice_coef`\n",
    "\n",
    "* Tr√®s proche du mIoU mais plus sensible aux petits objets.\n",
    "* DeepLab ‚âà 0.72 ‚Üí coh√©rent avec sa bonne mIoU.\n",
    "* U-Net VGG16 ‚âà 0.63 et MobileDet ‚âà 0.60 ‚Üí acceptables.\n",
    "* YOLOv9 ‚âà 0.49, U-Net mini ‚âà 0.40 ‚Üí faibles.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è Analyse comparative\n",
    "\n",
    "| Crit√®re                               | Meilleur mod√®le                            |\n",
    "| :------------------------------------ | :----------------------------------------- |\n",
    "| **Pr√©cision globale (mIoU/Dice)**     | üü¢ **DeepLabV3+ ResNet50**                 |\n",
    "| **G√©n√©ralisation / stabilit√© val**    | üü¢ **DeepLabV3+ ResNet50**                 |\n",
    "| **Compromis vitesse/qualit√©**         | üü¢ **MobileDet_seg** (plus l√©ger, correct) |\n",
    "| **Performance brute (haute qualit√©)** | üü¢ **U-Net VGG16** si VRAM suffisante      |\n",
    "| **L√©geret√© / prototypage rapide**     | üü¢ **U-Net mini**, mais pr√©cision faible   |\n",
    "\n",
    "---\n",
    "\n",
    "## Interpr√©tation d√©taill√©e\n",
    "\n",
    "### ü•á **DeepLabV3+ (ResNet50)**\n",
    "\n",
    "* **Meilleur √©quilibre** entre pr√©cision et stabilit√©.\n",
    "* mIoU = 0.64 (val) et Dice = 0.72 (val) : excellents scores sur 8 classes.\n",
    "* Surapprentissage mod√©r√© (train-val gap raisonnable).\n",
    "* Tr√®s bonne capacit√© √† capter les contours fins et la hi√©rarchie spatiale.\n",
    "  ‚úÖ **‚Üí Mod√®le √† garder comme r√©f√©rence.**\n",
    "\n",
    "### ü•à **U-Net VGG16**\n",
    "\n",
    "* Tr√®s bon entra√Ænement, mais √©cart train-val > 0.35 : l√©ger overfit.\n",
    "* Lourdeur m√©moire (VGG16) mais r√©sultats solides.\n",
    "  ‚úÖ Alternative si tu veux plus de stabilit√© visuelle (textures fines).\n",
    "\n",
    "### ü•â **MobileDet_seg**\n",
    "\n",
    "* Performances correctes pour un mod√®le ‚Äúmobile-like‚Äù.\n",
    "* Bonne efficacit√© (seulement 16 min d‚Äôentra√Ænement, r√©sultats d√©cents).\n",
    "  üü° Bon compromis si tu cibles l‚Äôinf√©rence embarqu√©e.\n",
    "\n",
    "### ‚öôÔ∏è **YOLOv9_seg**\n",
    "\n",
    "* Correct mais sous-optimal : architecture pas parfaitement adapt√©e √† la segmentation dense.\n",
    "* Val mIoU = 0.40, Dice = 0.49 : pas suffisant pour une segmentation de qualit√©.\n",
    "  üî¥ √Ä √©viter pour cette t√¢che sp√©cifique.\n",
    "\n",
    "### ‚ö™ **U-Net mini**\n",
    "\n",
    "* Tr√®s rapide mais sous-entra√Æn√© / sous-dimensionn√©.\n",
    "* Mauvais scores val (mIoU = 0.32, Dice = 0.40).\n",
    "  üî¥ Bon pour tests rapides, pas pour production.\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Conclusion\n",
    "\n",
    "| Rang | Mod√®le                    | Pourquoi                                                  |\n",
    "| :--: | :------------------------ | :-------------------------------------------------------- |\n",
    "|  ü•á  | **DeepLabV3+ (ResNet50)** | Meilleur √©quilibre pr√©cision / g√©n√©ralisation / stabilit√© |\n",
    "|  ü•à  | **U-Net VGG16**           | Tr√®s bon mais plus lourd, tendance √† overfitter           |\n",
    "|  ü•â  | **MobileDet_seg**         | L√©g√®ret√© et vitesse, mais pr√©cision un cran en dessous    |\n",
    "|   4  | **YOLOv9_seg**            | Pas adapt√© √† la segmentation dense                        |\n",
    "|   5  | **U-Net mini**            | Trop limit√©, r√©sultats faibles                            |\n",
    "\n",
    "---\n",
    "\n",
    "### üîß En r√©sum√©\n",
    "\n",
    "> **DeepLabV3+ ResNet50** est le **meilleur mod√®le global** :\n",
    ">\n",
    "> * meilleures m√©triques de validation,\n",
    "> * bon Dice et mIoU,\n",
    "> * rapport vitesse/qualit√© tr√®s favorable,\n",
    "> * faible overfit compar√© √† VGG16.\n"
   ],
   "id": "154c65411f171045"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Entra√Ænement final DeepLabV3+ (ResNet50) sur l'int√©gralit√© du dataset\n\nNous relan√ßons DeepLabV3+ avec **toutes** les images `train/val` de Cityscapes (plus de limite `max_*_samples`).\nLes artefacts (best/final) seront export√©s dans `artifacts/deeplab_resnet50_full` et suivis dans **MLflow** (`artifacts/mlruns`)."
   ],
   "id": "f224bfb380ca5bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration finale : dataset complet + sortie d√©di√©e\nfinal_data_cfg = replace(\n    data_cfg,\n    max_train_samples=None,\n    max_val_samples=None,\n)\nfinal_train_cfg = replace(\n    train_cfg,\n    output_dir=\"artifacts/deeplab_resnet50_full\",\n    exp_name=\"cityscapes-seg-8cls-full\",\n)\nfinal_aug_cfg = replace(\n    aug_cfg,\n    enabled=True,\n)\n\n# Lancement de l'entra√Ænement complet (sauvegarde locale + tracking MLflow)\ntrain(\"deeplab_resnet50\", final_data_cfg, final_train_cfg, final_aug_cfg)"
   ],
   "id": "1828657bb72691a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copie explicite du meilleur mod√®le pour l'API\nfrom pathlib import Path\nimport shutil\n\nbest_model = Path(\"artifacts/deeplab_resnet50_full/deeplab_resnet50_best.keras\")\nfinal_model = Path(\"artifacts/deeplab_resnet50_full/deeplab_resnet50_final.keras\")\napi_export = Path(\"artifacts/api/deeplabv3plus_resnet50_full.keras\")\napi_export.parent.mkdir(parents=True, exist_ok=True)\n\nif best_model.exists():\n    shutil.copy2(best_model, api_export)\n    print(f\"‚úÖ Mod√®le API (best) : {api_export}\")\nelif final_model.exists():\n    shutil.copy2(final_model, api_export)\n    print(f\"‚ö†Ô∏è Best absent, export du mod√®le final : {api_export}\")\nelse:\n    raise FileNotFoundError(\"Aucun mod√®le entra√Æn√© trouv√©. Lance d'abord la cellule d'entra√Ænement.\")"
   ],
   "id": "73c6a586d43eb52b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
