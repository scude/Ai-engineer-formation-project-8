{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5eeaf89ff30992",
   "metadata": {},
   "source": [
    "# Projet 8 · Segmentation sémantique Cityscapes\n",
    "\n",
    "Ce carnet sert de journal de bord : j'y raconte toutes les étapes qui mènent de l'audit des données jusqu'à l'entraînement final des modèles. L'idée est de garder une trace claire de mes choix et de faciliter la relecture pour toute personne qui reprendra le projet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac61e4ff",
   "metadata": {},
   "source": [
    "## 1. Choisir le mode d'exécution\n",
    "\n",
    "Avant de lancer des calculs lourds, je décide si je suis en mode *explicatif* (`train_mode = True`) ou en mode *entraînement complet`. En mode explicatif, je parcours simplement le pipeline pour documenter chaque étape sans déclencher les boucles de training."
   ]
  },
  {
   "cell_type": "code",
   "id": "6327d30735ab087e",
   "metadata": {},
   "source": [
    "train_mode = True\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices(\"GPU\"))\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "973340e2",
   "metadata": {},
   "source": [
    "Je garde `train_mode` sur `True` pour rester en mode explication : je parcours le pipeline sans lancer de longs entraînements. C'est idéal pour commenter chaque étape calmement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4776a7",
   "metadata": {},
   "source": [
    "## 2. Faire l'inventaire du dossier Cityscapes\n",
    "\n",
    "Je dresse un état des lieux automatique (nombre de fichiers, extensions, sous-dossiers) pour m'assurer que les données ont bien été téléchargées et rangées."
   ]
  },
  {
   "cell_type": "code",
   "id": "84b61c778fafe1a",
   "metadata": {},
   "source": [
    "# --- Simple dataset scanner (counts only, no filenames, no JSON) ---\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# >>> Change this to your dataset root (WSL path) <<<\n",
    "ROOT = Path(\"../data\")\n",
    "\n",
    "IGNORE_HIDDEN = True       # ignore .git, __pycache__, etc.\n",
    "MAX_DIRS_TO_SHOW = 80      # limit directory lines for readability\n",
    "\n",
    "total_files = 0\n",
    "total_dirs = 0\n",
    "by_ext = Counter()\n",
    "by_dir = {}\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(ROOT):\n",
    "    # optionally hide hidden/internal dirs\n",
    "    if IGNORE_HIDDEN:\n",
    "        dirnames[:] = [d for d in dirnames if not d.startswith(\".\") and not d.startswith(\"__\")]\n",
    "    total_dirs += 1\n",
    "    rel = Path(dirpath).relative_to(ROOT) if Path(dirpath) != ROOT else Path(\".\")\n",
    "    by_dir[str(rel)] = len(filenames)\n",
    "    for fn in filenames:\n",
    "        by_ext[Path(fn).suffix.lower()] += 1\n",
    "    total_files += len(filenames)\n",
    "\n",
    "print(f\"[ROOT] {ROOT}\")\n",
    "print(f\"dirs={total_dirs:,}  files={total_files:,}\\n\")\n",
    "\n",
    "print(\"By extension (top 10):\")\n",
    "for ext, n in by_ext.most_common(10):\n",
    "    print(f\"  {ext or '(no ext)'}: {n:,}\")\n",
    "print()\n",
    "\n",
    "print(f\"Directory counts (first {MAX_DIRS_TO_SHOW}):\")\n",
    "for i, (rel, n) in enumerate(sorted(by_dir.items())):\n",
    "    if i >= MAX_DIRS_TO_SHOW:\n",
    "        print(\"  ... (truncated)\")\n",
    "        break\n",
    "    print(f\"  {rel}: {n}\")\n",
    "\n",
    "# -------- Cityscapes mini-summary (counts only) --------\n",
    "def count_pattern(base: Path, split: str, suffix: str) -> int:\n",
    "    split_dir = base / split\n",
    "    total = 0\n",
    "    if split_dir.exists():\n",
    "        for city_dir in split_dir.iterdir():\n",
    "            if city_dir.is_dir():\n",
    "                total += sum(1 for p in city_dir.iterdir()\n",
    "                             if p.is_file() and p.name.endswith(suffix))\n",
    "    return total\n",
    "\n",
    "print(\"\\n[Cityscapes summary]\")\n",
    "for split in (\"train\", \"val\", \"test\"):\n",
    "    gt_base = ROOT / \"gtFine\"\n",
    "    left_base = ROOT / \"leftImg8bit\"\n",
    "    label = count_pattern(gt_base, split, \"_gtFine_labelIds.png\")\n",
    "    color = count_pattern(gt_base, split, \"_gtFine_color.png\")\n",
    "    inst  = count_pattern(gt_base, split, \"_gtFine_instanceIds.png\")\n",
    "    poly  = count_pattern(gt_base, split, \"_gtFine_polygons.json\")\n",
    "    left  = count_pattern(left_base, split, \"_leftImg8bit.png\")\n",
    "    print(f\"  {split:5s}: leftImg8bit={left:6d}  labelIds={label:6d}  color={color:6d}  instanceIds={inst:6d}  polygons.json={poly:6d}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9ec2192e",
   "metadata": {},
   "source": [
    "Je fais tourner un petit script qui compte les fichiers par dossier et par extension. Si une image ou un masque manque, je le vois tout de suite dans le récapitulatif."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70308dc3",
   "metadata": {},
   "source": [
    "## 3. Vérifier l'appairage images / masques\n",
    "\n",
    "Chaque image RGB doit avoir un masque d'annotations qui porte le même identifiant. Je compare les ensembles pour repérer d'éventuels manques avant de poursuivre."
   ]
  },
  {
   "cell_type": "code",
   "id": "6b8ee8ae56f5c54f",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"../data\")  # adapte si besoin\n",
    "SUF_LEFT = \"_leftImg8bit.png\"\n",
    "SUF_LBL  = \"_gtFine_labelIds.png\"\n",
    "\n",
    "def base_id(name: str) -> str:\n",
    "    return name[:-len(SUF_LEFT)] if name.endswith(SUF_LEFT) else name[:-len(SUF_LBL)]\n",
    "\n",
    "def split_counts(split: str):\n",
    "    left_dir = ROOT / \"leftImg8bit\" / split\n",
    "    lbl_dir  = ROOT / \"gtFine\"      / split\n",
    "    left = sorted(left_dir.rglob(f\"*{SUF_LEFT}\")) if left_dir.exists() else []\n",
    "    lbl  = sorted(lbl_dir.rglob (f\"*{SUF_LBL}\" )) if lbl_dir.exists()  else []\n",
    "    left_ids = {base_id(p.name) for p in left}\n",
    "    lbl_ids  = {base_id(p.name) for p in lbl}\n",
    "    paired = left_ids & lbl_ids\n",
    "    print(f\"{split:<5} | left={len(left):4d}  labels={len(lbl):4d}  paired={len(paired):4d}\")\n",
    "\n",
    "for sp in (\"train\", \"val\", \"test\"):\n",
    "    split_counts(sp)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "71ee298b",
   "metadata": {},
   "source": [
    "Je construis deux ensembles de noms (images et masques) et je vérifie qu'ils coïncident parfaitement. En cas d'écart, je saurais qu'il faut régénérer ou télécharger les données manquantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31fa17c",
   "metadata": {},
   "source": [
    "## 4. Visualiser rapidement quelques exemples\n",
    "\n",
    "Un contrôle visuel reste indispensable : je charge une poignée d'images et superpose leur masque couleur pour confirmer que les annotations collent bien à la réalité."
   ]
  },
  {
   "cell_type": "code",
   "id": "3b30f7c4d8786b68",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "ROOT = Path(\"../data\")\n",
    "\n",
    "PALETTE = {\n",
    "    7:(128,64,128), 8:(244,35,232), 11:(70,70,70), 12:(102,102,156), 13:(190,153,153),\n",
    "    17:(153,153,153), 19:(250,170,30), 20:(220,220,0), 21:(107,142,35), 22:(152,251,152),\n",
    "    23:(70,130,180), 24:(220,20,60), 25:(255,0,0), 26:(0,0,142), 27:(0,0,70),\n",
    "    28:(0,60,100), 31:(0,80,100), 32:(0,0,230), 33:(119,11,32),\n",
    "}\n",
    "\n",
    "def pairs(split=\"val\"):\n",
    "    lbls = sorted((ROOT/\"gtFine\"/split).rglob(\"*_gtFine_labelIds.png\"))\n",
    "    out = []\n",
    "    for lp in lbls:\n",
    "        stem = lp.name.replace(\"_gtFine_labelIds.png\", \"\")\n",
    "        city = lp.parent.name\n",
    "        left = ROOT/\"leftImg8bit\"/split/city/(stem+\"_leftImg8bit.png\")\n",
    "        if left.exists():\n",
    "            out.append((left, lp))\n",
    "    return out\n",
    "\n",
    "def colorize(ids: np.ndarray) -> Image.Image:\n",
    "    h, w = ids.shape\n",
    "    rgb = np.zeros((h, w, 3), np.uint8)\n",
    "    for k, c in PALETTE.items():\n",
    "        rgb[ids == k] = c\n",
    "    return Image.fromarray(rgb, \"RGB\")\n",
    "\n",
    "def overlay(img: Image.Image, mask_rgb: Image.Image, alpha=0.5) -> Image.Image:\n",
    "    a = np.asarray(img.convert(\"RGB\"), np.float32)\n",
    "    b = np.asarray(mask_rgb, np.float32)\n",
    "    return Image.fromarray(np.clip((1-alpha)*a + alpha*b, 0, 255).astype(np.uint8))\n",
    "\n",
    "samples = pairs(\"val\")\n",
    "assert samples, \"No pairs found — check your paths.\"\n",
    "random.shuffle(samples)\n",
    "k = 3\n",
    "\n",
    "plt.figure(figsize=(15, 5*k))\n",
    "for i, (left_p, lbl_p) in enumerate(samples[:k]):\n",
    "    left = Image.open(left_p).convert(\"RGB\")\n",
    "    ids  = np.array(Image.open(lbl_p))\n",
    "    mask = colorize(ids)\n",
    "    over = overlay(left, mask, alpha=0.45)\n",
    "    for j, (img, title) in enumerate([(left,\"leftImg8bit\"),(mask,\"labelIds (colored)\"),(over,\"overlay\")]):\n",
    "        ax = plt.subplot(k, 3, i*3 + j + 1)\n",
    "        ax.imshow(img); ax.set_title(f\"{left_p.parent.name} — {title}\", fontsize=10); ax.axis(\"off\")\n",
    "plt.tight_layout(); plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9abf36f1",
   "metadata": {},
   "source": [
    "Je pioche quelques couples image/masque et j'affiche la superposition couleur. C'est un contrôle visuel simple mais efficace pour confirmer que le prétraitement fonctionne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1280e711",
   "metadata": {},
   "source": [
    "## 5. Ramener 32 classes Cityscapes vers 8 classes pratiques\n",
    "\n",
    "La version originale de Cityscapes comporte beaucoup de catégories fines. Je compacte ces 32 classes en 8 grandes familles pour accélérer l'entraînement tout en conservant le signal principal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f540b1886f92c7b1",
   "metadata": {},
   "source": [
    "\n",
    "### Remapping Cityscapes 32→8 classes"
   ]
  },
  {
   "cell_type": "code",
   "id": "1b632fc00041fe27",
   "metadata": {},
   "source": [
    "# --- 32→8 mapping (Cityscapes labelIds -> 8-class IDs), ignore = 255 ---\n",
    "import numpy as np\n",
    "\n",
    "CS_LABELID_TO_8 = {\n",
    "    # 0..5 (voids) -> ignore by LUT fill (no need to list)\n",
    "    6: 0,\n",
    "    7: 0,  9: 0, 10: 0,           # road-like: road, parking, rail track\n",
    "    8: 1,                         # sidewalk\n",
    "    11: 2, 12: 2, 13: 2, 14: 2, 15: 2, 16: 2,   # building + barriers\n",
    "    17: 3, 18: 3, 19: 3, 20: 3,                 # traffic objs (pole/ts/tl)\n",
    "    21: 4, 22: 4,                                 # vegetation + terrain\n",
    "    23: 5,                                       # sky\n",
    "    24: 6, 25: 6,                                 # person + rider\n",
    "    26: 7, 27: 7, 28: 7, 29: 7, 30: 7, 31: 7, 32: 7, 33: 7,  # vehicles\n",
    "}\n",
    "\n",
    "def build_labelid_to8_lut(ignore_value: int = 255) -> np.ndarray:\n",
    "    \"\"\"Create a 256-entry LUT mapping Cityscapes labelIds -> {0..7} or 255(ignore).\"\"\"\n",
    "    lut = np.full(256, ignore_value, dtype=np.uint8)\n",
    "    for k, v in CS_LABELID_TO_8.items():\n",
    "        lut[k] = v\n",
    "    return lut\n",
    "\n",
    "LUT_32TO8 = build_labelid_to8_lut(ignore_value=255)\n",
    "\n",
    "def remap_labelids_to8(arr_uint16: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Vectorized remap of HxW labelIds (uint16/uint8) to 8-class IDs with 255 ignore.\"\"\"\n",
    "    arr = arr_uint16.astype(np.uint16)\n",
    "    arr = np.minimum(arr, 255).astype(np.uint8)\n",
    "    return LUT_32TO8[arr]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2724760c",
   "metadata": {},
   "source": [
    "Je définis un dictionnaire qui convertit les 32 `labelIds` officiels de Cityscapes en seulement 8 catégories (plus la valeur 255 pour ignorer). Cette réduction facilite l'entraînement de modèles légers sans perdre l'essentiel."
   ]
  },
  {
   "cell_type": "code",
   "id": "4090ae75134d47fe",
   "metadata": {},
   "source": [
    "PALETTE_8 = {\n",
    "    0:(128,64,128),   # road\n",
    "    1:(244,35,232),   # sidewalk\n",
    "    2:(70,70,70),     # building+barrier\n",
    "    3:(220,220,0),    # traffic objs\n",
    "    4:(107,142,35),   # vegetation/terrain\n",
    "    5:(70,130,180),   # sky\n",
    "    6:(220,20,60),    # person+rider\n",
    "    7:(0,0,142),      # vehicle\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a912fec5",
   "metadata": {},
   "source": [
    "Pour garder des visualisations lisibles, j'associe chaque identifiant compressé à une couleur RGB. Je pourrai ainsi comparer facilement les masques réels et ceux prédits."
   ]
  },
  {
   "cell_type": "code",
   "id": "623c52d0928c016c",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def colorize_8(label8: np.ndarray, palette: dict) -> Image.Image:\n",
    "    h, w = label8.shape\n",
    "    rgb = np.zeros((h, w, 3), np.uint8)\n",
    "    for k, c in palette.items():\n",
    "        rgb[label8 == k] = c\n",
    "    return Image.fromarray(rgb, \"RGB\")\n",
    "\n",
    "sample_lbl = next(Path(\"../data/gtFine/val/frankfurt\").glob(\"*_gtFine_labelIds.png\"))\n",
    "arr = np.array(Image.open(sample_lbl))\n",
    "arr8 = remap_labelids_to8(arr)\n",
    "plt.figure(figsize=(8,4)); plt.imshow(colorize_8(arr8, PALETTE_8)); plt.axis(\"off\"); plt.title(\"8-class mask\"); plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "50683e6e",
   "metadata": {},
   "source": [
    "Je rassemble deux utilitaires : `colorize_8` pour transformer un masque d'indices en image colorisée et `overlay_mask` pour coller ce masque sur l'image d'origine. Je les réutiliserai tout au long du carnet pour illustrer les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54219814",
   "metadata": {},
   "source": [
    "Je coupe les logs bavards de TensorFlow, j'autorise la mémoire GPU à grandir au fil des besoins et je convertis la table de remapping en tenseur. Je regroupe aussi les fonctions qui listent les fichiers, chargent les images/masques et appliquent les augmentations pour fabriquer des `tf.data.Dataset` efficaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80fe41e",
   "metadata": {},
   "source": [
    "## 7. Mesurer l'équilibre des classes\n",
    "\n",
    "Avant d'entraîner un modèle, je veux connaître le poids de chaque classe : cela influence le choix des métriques et des pondérations de perte."
   ]
  },
  {
   "cell_type": "code",
   "id": "fa8eafa784ea3287",
   "metadata": {},
   "source": [
    "# ==== Class balance for Cityscapes 8 classes (with ignore=255) ====\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# ---- Config (adapt if needed) ----\n",
    "ROOT = Path(\"../data\")          # dataset root (WSL path)\n",
    "SPLIT = \"train\"                 # \"train\" | \"val\" | \"test\"\n",
    "SUF_LBL = \"_gtFine_labelIds.png\"\n",
    "\n",
    "# 8-class names (your mapping)\n",
    "CLASS8_NAMES = [\n",
    "    \"road\", \"sidewalk\", \"building+barriers\", \"traffic-objs\",\n",
    "    \"vegetation+terrain\", \"sky\", \"person+rider\", \"vehicle\"\n",
    "]\n",
    "\n",
    "# If LUT_32TO8 not in scope, (re)build it quickly:\n",
    "try:\n",
    "    LUT_32TO8\n",
    "except NameError:\n",
    "    CS_LABELID_TO_8 = {\n",
    "        6:0, 7:0, 9:0, 10:0, 8:1, 11:2,12:2,13:2,14:2,15:2,16:2,\n",
    "        17:3,18:3,19:3,20:3, 21:4,22:4, 23:5, 24:6,25:6,\n",
    "        26:7,27:7,28:7,29:7,30:7,31:7,32:7,33:7,\n",
    "    }\n",
    "    LUT_32TO8 = np.full(256, 255, dtype=np.uint8)\n",
    "    for k, v in CS_LABELID_TO_8.items():\n",
    "        LUT_32TO8[k] = v\n",
    "\n",
    "def remap_to8_np(arr_label_ids: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"arr_label_ids: HxW uint16/uint8 -> HxW uint8 in {0..7,255}\"\"\"\n",
    "    arr = arr_label_ids.astype(np.uint16)\n",
    "    arr = np.minimum(arr, 255).astype(np.uint8)\n",
    "    return LUT_32TO8[arr]\n",
    "\n",
    "def class_balance(split: str = SPLIT):\n",
    "    lbl_paths = sorted((ROOT/\"gtFine\"/split).rglob(f\"*{SUF_LBL}\"))\n",
    "    assert lbl_paths, f\"No labels found under {ROOT}/gtFine/{split}\"\n",
    "    counts = np.zeros(8, dtype=np.int64)\n",
    "    ignore = 0\n",
    "    for i, p in enumerate(lbl_paths, 1):\n",
    "        lab = np.array(Image.open(p))         # (H,W) uint16/uint8\n",
    "        lab8 = remap_to8_np(lab)              # (H,W) uint8\n",
    "        m_ignore = (lab8 == 255)\n",
    "        ignore += int(m_ignore.sum())\n",
    "        # bincount only on valid pixels\n",
    "        c = np.bincount(lab8[~m_ignore].ravel(), minlength=8)\n",
    "        counts += c[:8]\n",
    "        if i % 500 == 0 or i == len(lbl_paths):\n",
    "            print(f\"[{split}] processed {i}/{len(lbl_paths)} images...\", end=\"\\r\")\n",
    "    print()\n",
    "    total_valid = int(counts.sum())\n",
    "    total_pixels = total_valid + ignore\n",
    "    freqs = counts / max(total_valid, 1)\n",
    "    return counts, ignore, total_valid, total_pixels, freqs\n",
    "\n",
    "counts, ignore, total_valid, total_pixels, freqs = class_balance(\"train\")\n",
    "\n",
    "print(\"\\n=== Class balance (train) ===\")\n",
    "for k, (name, n, f) in enumerate(zip(CLASS8_NAMES, counts, freqs)):\n",
    "    print(f\"{k}: {name:<20s}  pixels={n:,}   freq={f:.4%}\")\n",
    "print(f\"\\nignore pixels (==255): {ignore:,}\")\n",
    "print(f\"total valid pixels:     {total_valid:,}\")\n",
    "print(f\"total pixels (incl. ignore): {total_pixels:,}\")\n",
    "\n",
    "# ---- Optional: derive class weights ----\n",
    "# Inverse-frequency, normalized to mean=1 (good starting point)\n",
    "weights_inv = (1.0 / np.maximum(freqs, 1e-12))\n",
    "weights_inv = weights_inv / weights_inv.mean()\n",
    "print(\"\\nSuggested class weights (inverse-freq, mean≈1):\")\n",
    "for k, (name, w) in enumerate(zip(CLASS8_NAMES, weights_inv)):\n",
    "    print(f\"{k}: {name:<20s}  w={w:.3f}\")\n",
    "\n",
    "# Median-frequency balancing (alternative)\n",
    "median_f = np.median(freqs[freqs > 0])\n",
    "weights_med = median_f / np.maximum(freqs, 1e-12)\n",
    "weights_med = weights_med / weights_med.mean()\n",
    "print(\"\\nSuggested class weights (median-freq, mean≈1):\")\n",
    "for k, (name, w) in enumerate(zip(CLASS8_NAMES, weights_med)):\n",
    "    print(f\"{k}: {name:<20s}  w={w:.3f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "217d1bd6",
   "metadata": {},
   "source": [
    "Je calcule la distribution de pixels par classe sur le split choisi. Ce comptage (après conversion 32→8) m'aide à diagnostiquer les déséquilibres et à décider s'il faut appliquer des pondérations de perte ou des stratégies de sur-échantillonnage."
   ]
  },
  {
   "cell_type": "code",
   "id": "e8433da3712483ac",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# expects: counts (np.array shape [8]), freqs (shape [8]), ignore (int),\n",
    "#          total_valid (int), total_pixels (int), CLASS8_NAMES (list of 8 str)\n",
    "\n",
    "# ---- 1) Bar chart des 8 classes (trié décroissant) ----\n",
    "order = np.argsort(freqs)[::-1]\n",
    "names_sorted = [CLASS8_NAMES[i] for i in order]\n",
    "freqs_sorted = freqs[order]\n",
    "counts_sorted = counts[order]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.bar(range(len(names_sorted)), freqs_sorted)  # no explicit colors\n",
    "plt.xticks(range(len(names_sorted)), names_sorted, rotation=20, ha=\"right\")\n",
    "plt.ylabel(\"Frequency (share of valid pixels)\")\n",
    "plt.title(\"Cityscapes (train) — Class balance (8 classes)\")\n",
    "\n",
    "# annotations: % + millions de pixels\n",
    "for i, (b, f, c) in enumerate(zip(bars, freqs_sorted, counts_sorted)):\n",
    "    plt.text(b.get_x() + b.get_width()/2,\n",
    "             b.get_height() + 0.002,\n",
    "             f\"{f*100:.1f}%\\n{c/1e6:.1f}M\",\n",
    "             ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.ylim(0, max(freqs_sorted)*1.15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 2) Valid vs Ignore (pour info) ----\n",
    "valid_share = total_valid / total_pixels\n",
    "ignore_share = 1.0 - valid_share\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "bars2 = plt.bar([0,1], [valid_share, ignore_share])\n",
    "plt.xticks([0,1], [\"valid\", \"ignore (==255)\"])\n",
    "plt.ylabel(\"Share of total pixels\")\n",
    "plt.title(\"Valid vs Ignore pixels (train)\")\n",
    "\n",
    "for x, v in zip([0,1], [valid_share, ignore_share]):\n",
    "    plt.text(x, v + 0.005, f\"{v*100:.1f}%\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.ylim(0, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "461c6915",
   "metadata": {},
   "source": [
    "À partir des comptages précédents, je trace un histogramme normalisé, j'affiche les fréquences et quelques statistiques globales. C'est une étape clé pour documenter la difficulté du jeu de données et motiver d'éventuelles compensations durant l'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e99ba",
   "metadata": {},
   "source": [
    "## 8. Instancier les configurations\n",
    "\n",
    "Je matérialise les dataclasses `DataConfig`, `TrainConfig` et `AugmentConfig`. Elles regroupent toutes les options (chemins, tailles d'images, scheduler, callbacks) pour éviter les copier-coller et garder des expériences traçables."
   ]
  },
  {
   "cell_type": "code",
   "id": "d9d9ad6ec23362d",
   "metadata": {},
   "source": [
    "# In notebook (Python)\n",
    "from scripts.config import DataConfig, TrainConfig, AugmentConfig\n",
    "from scripts.train import train\n",
    "\n",
    "# Configuration rapide pour les expériences contrôlées\n",
    "data_cfg = DataConfig(\n",
    "    data_root=\"../data\",\n",
    "    height=512,\n",
    "    width=1024,\n",
    "    batch_size=2,\n",
    "    deterministic_input=False,\n",
    "    cache_val=False,\n",
    "    max_train_samples=100,\n",
    "    max_val_samples=100,\n",
    ")\n",
    "\n",
    "# Recette DeepLab optimisée pour converger plus vite (<5h)\n",
    "aug_cfg = AugmentConfig(\n",
    "    enabled=True,\n",
    ")\n",
    "\n",
    "train_cfg = TrainConfig(\n",
    "    lr=5e-4,\n",
    "    epochs=80,\n",
    "    optimizer=\"adamw\",\n",
    "    weight_decay=1e-4,\n",
    "    lr_schedule=\"cosine_warmup\",\n",
    "    warmup_epochs=5.0,\n",
    "    min_lr_ratio=0.05,\n",
    "    precision_policy=\"mixed_float16\",\n",
    "    exp_name=\"cityscapes-seg-8cls\",\n",
    ")\n",
    "\n",
    "deeplab_kwargs = {\"aspp_dropout\": 0.5}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b3b9819cc887027b",
   "metadata": {},
   "source": [
    "## 10. Contrôler visuellement la data augmentation\n",
    "\n",
    "En mode analyse, j'affiche plusieurs couples image/masque avant/après transformation pour vérifier que les augmentations Albumentations respectent bien les contours."
   ]
  },
  {
   "cell_type": "code",
   "id": "fae32854cfcd9ef9",
   "metadata": {},
   "source": [
    "if not train_mode :\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "\n",
    "    from scripts.config import AugmentConfig\n",
    "    from scripts.augment import build_augment_fn\n",
    "    from scripts.remap import build_cityscapes_8cls_lut, remap_labels\n",
    "\n",
    "    lut = build_cityscapes_8cls_lut(data_cfg.ignore_index)\n",
    "    no_aug_fn = build_augment_fn(AugmentConfig(enabled=False), data_cfg.height, data_cfg.width, data_cfg.ignore_index)\n",
    "    augmented_fn = build_augment_fn(aug_cfg, data_cfg.height, data_cfg.width, data_cfg.ignore_index)\n",
    "\n",
    "    def remap_to_training_ids(mask_np):\n",
    "        mask_tf = tf.convert_to_tensor(mask_np, dtype=tf.int32)\n",
    "        return remap_labels(mask_tf, lut).numpy()\n",
    "\n",
    "    def colorize_mask(mask_np, palette=PALETTE_8, ignore_value=data_cfg.ignore_index):\n",
    "        rgb = np.zeros((mask_np.shape[0], mask_np.shape[1], 3), dtype=np.uint8)\n",
    "        for cls_id, color in palette.items():\n",
    "            rgb[mask_np == cls_id] = color\n",
    "        if ignore_value is not None:\n",
    "            rgb[mask_np == ignore_value] = (0, 0, 0)\n",
    "        return rgb\n",
    "\n",
    "    def overlay_mask(image_uint8, mask_uint8, alpha=0.45):\n",
    "        colored = colorize_mask(mask_uint8)\n",
    "        return np.clip((1.0 - alpha) * image_uint8 + alpha * colored, 0, 255).astype(np.uint8)\n",
    "\n",
    "    samples = pairs(\"train\")\n",
    "    assert samples, \"Aucun couple image/masque trouvé — vérifie le dossier data.\"\n",
    "\n",
    "    random.shuffle(samples)\n",
    "    num_rows = min(3, len(samples))\n",
    "    fig, axes = plt.subplots(num_rows, 6, figsize=(22, 5 * num_rows))\n",
    "    if num_rows == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "    for row, (left_path, lbl_path) in enumerate(samples[:num_rows]):\n",
    "        raw_img = np.array(Image.open(left_path).convert(\"RGB\"), dtype=np.float32) / 255.0\n",
    "        raw_mask = np.array(Image.open(lbl_path), dtype=np.int32)\n",
    "\n",
    "        mask8 = remap_to_training_ids(raw_mask)\n",
    "\n",
    "        img_tf = tf.convert_to_tensor(raw_img, dtype=tf.float32)\n",
    "        mask_tf = tf.convert_to_tensor(mask8, dtype=tf.int32)\n",
    "\n",
    "        base_img, base_mask = no_aug_fn(img_tf, mask_tf)\n",
    "        aug_img, aug_mask = augmented_fn(img_tf, mask_tf)\n",
    "\n",
    "        base_img_u8 = np.clip(base_img.numpy() * 255.0, 0, 255).astype(np.uint8)\n",
    "        aug_img_u8 = np.clip(aug_img.numpy() * 255.0, 0, 255).astype(np.uint8)\n",
    "        base_mask_u8 = base_mask.numpy().astype(np.uint8)\n",
    "        aug_mask_u8 = aug_mask.numpy().astype(np.uint8)\n",
    "\n",
    "        base_mask_rgb = colorize_mask(base_mask_u8)\n",
    "        aug_mask_rgb = colorize_mask(aug_mask_u8)\n",
    "\n",
    "        axes[row, 0].imshow(base_img_u8)\n",
    "        axes[row, 0].set_title(\"Image (resize)\")\n",
    "        axes[row, 1].imshow(base_mask_rgb)\n",
    "        axes[row, 1].set_title(\"Masque (resize)\")\n",
    "        axes[row, 2].imshow(overlay_mask(base_img_u8, base_mask_u8))\n",
    "        axes[row, 2].set_title(\"Overlay resize\")\n",
    "        axes[row, 3].imshow(aug_img_u8)\n",
    "        axes[row, 3].set_title(\"Image augmentée\")\n",
    "        axes[row, 4].imshow(aug_mask_rgb)\n",
    "        axes[row, 4].set_title(\"Masque augmenté\")\n",
    "        axes[row, 5].imshow(overlay_mask(aug_img_u8, aug_mask_u8))\n",
    "        axes[row, 5].set_title(\"Overlay augmentée\")\n",
    "\n",
    "        for ax in axes[row]:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "96f84853",
   "metadata": {},
   "source": [
    "Quand je suis en mode analyse, j'affiche côte à côte l'image originale et l'image augmentée. Cela me permet de valider à l'œil nu que les transformations (rotations, flips, jitter de couleur) respectent les contours des objets."
   ]
  },
  {
   "cell_type": "code",
   "id": "44c2b5ff61d6ffcf",
   "metadata": {},
   "source": [
    "if not train_mode :\n",
    "    from notebook.scripts.data import build_dataset\n",
    "\n",
    "    val_ds = build_dataset(\n",
    "        data_cfg,\n",
    "        AugmentConfig(enabled=False),\n",
    "        split=\"val\",\n",
    "        training=False,\n",
    "    )\n",
    "\n",
    "    images, masks, _ = next(iter(val_ds))\n",
    "    images_np = images.numpy()\n",
    "    masks_np = masks.numpy()\n",
    "\n",
    "    num_samples = min(3, images_np.shape[0])\n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image = images_np[i]\n",
    "        mask = masks_np[i]\n",
    "\n",
    "        if image.dtype != np.uint8:\n",
    "            image_u8 = np.clip(image * 255.0, 0, 255).astype(np.uint8)\n",
    "        else:\n",
    "            image_u8 = image\n",
    "\n",
    "        mask_u8 = mask.astype(np.uint8)\n",
    "        mask_rgb = colorize_mask(mask_u8)\n",
    "        overlay_rgb = overlay_mask(image_u8, mask_u8)\n",
    "\n",
    "        overlay_on_black = overlay_mask(np.zeros_like(image_u8), mask_u8, alpha=1.0)\n",
    "        assert np.array_equal(overlay_on_black, mask_rgb), \"Overlay misaligned with mask (check dataset pipeline).\"\n",
    "\n",
    "        for j, (img, title) in enumerate([\n",
    "            (image_u8, \"Image (val)\"),\n",
    "            (mask_rgb, \"Masque colorisé\"),\n",
    "            (overlay_rgb, \"Overlay\"),\n",
    "        ]):\n",
    "            ax = plt.subplot(num_samples, 3, i * 3 + j + 1)\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(f\"Échantillon {i + 1} — {title}\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7b1c755b",
   "metadata": {},
   "source": [
    "Toujours en mode exploration, je prépare un petit dataset de validation sans augmentation. Il me sert de jeu d'inspection pour vérifier plus tard la cohérence des prédictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18988ddd",
   "metadata": {},
   "source": [
    "## 11. Préparer les entraînements des différents modèles\n",
    "\n",
    "Je réduis les logs TensorFlow, j'initialise la croissance mémoire GPU puis je duplique la configuration de base pour chaque architecture testée."
   ]
  },
  {
   "cell_type": "code",
   "id": "e50c7bd50e46b8ac",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# ↓↓↓ Quieter TensorFlow logs (set BEFORE importing tf)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"     # 0=all, 1=INFO off, 2=INFO+WARNING off, 3=all off\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"  # avoid grabbing all GPU memory\n",
    "# Optional: disable oneDNN (removes the \"oneDNN custom ops are on\" line, and tiny numeric diffs)\n",
    "# os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "from absl import logging as absl_logging\n",
    "absl_logging.set_verbosity(absl_logging.ERROR)  # reduce absl spam\n",
    "\n",
    "# (Optional) confirm GPU + set memory growth (extra safety)\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for g in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(g, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"TF:\", tf.__version__, \"| GPUs:\", gpus)\n",
    "\n",
    "# ==========================\n",
    "# Cityscapes 32→8 remapping\n",
    "# ==========================\n",
    "import numpy as np\n",
    "\n",
    "# 8 classes for embedded use (ignore=255):\n",
    "# 0=road (7,9,10) | 1=sidewalk(8) | 2=building+barriers(11–16) | 3=traffic objs(17–20)\n",
    "# 4=vegetation+terrain(21,22) | 5=sky(23) | 6=person+rider(24,25) | 7=vehicle(26–33)\n",
    "CS_LABELID_TO_8 = {\n",
    "    6:0,\n",
    "    7:0, 9:0, 10:0,\n",
    "    8:1,\n",
    "    11:2, 12:2, 13:2, 14:2, 15:2, 16:2,\n",
    "    17:3, 18:3, 19:3, 20:3,\n",
    "    21:4, 22:4,\n",
    "    23:5,\n",
    "    24:6, 25:6,\n",
    "    26:7, 27:7, 28:7, 29:7, 30:7, 31:7, 32:7, 33:7,\n",
    "}\n",
    "\n",
    "def build_labelid_to8_lut(ignore_value: int = 255) -> np.ndarray:\n",
    "    lut = np.full(256, ignore_value, dtype=np.uint8)\n",
    "    for k, v in CS_LABELID_TO_8.items():\n",
    "        lut[k] = v\n",
    "    return lut\n",
    "\n",
    "LUT_32TO8 = build_labelid_to8_lut(ignore_value=255)\n",
    "LUT_TF = tf.convert_to_tensor(LUT_32TO8, dtype=tf.uint8)  # shape [256]\n",
    "\n",
    "# ===================\n",
    "# Dataset (tf.data)\n",
    "# ===================\n",
    "from pathlib import Path\n",
    "ROOT = Path(\"../data\")               # <<< change if needed (WSL path)\n",
    "INPUT_SIZE = (512, 1024)             # (H, W)\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "SUF_LEFT = \"_leftImg8bit.png\"\n",
    "SUF_LBL  = \"_gtFine_labelIds.png\"\n",
    "\n",
    "def list_pairs(split: str):\n",
    "    \"\"\"Return two aligned lists: left paths and label paths for a given split.\"\"\"\n",
    "    lefts, labels = [], []\n",
    "    lbl_root = ROOT / \"gtFine\" / split\n",
    "    for lbl in sorted(lbl_root.rglob(f\"*{SUF_LBL}\")):\n",
    "        city = lbl.parent.name\n",
    "        stem = lbl.name.replace(SUF_LBL, \"\")\n",
    "        left = ROOT / \"leftImg8bit\" / split / city / f\"{stem}{SUF_LEFT}\"\n",
    "        if left.exists():\n",
    "            lefts.append(str(left))\n",
    "            labels.append(str(lbl))\n",
    "    if not lefts:\n",
    "        raise FileNotFoundError(f\"No pairs found for split='{split}'. Check your paths under {ROOT}.\")\n",
    "    return lefts, labels\n",
    "\n",
    "def decode_and_preprocess(left_path, lbl_path, training: bool):\n",
    "    # 1) Read bytes\n",
    "    left_bytes = tf.io.read_file(left_path)\n",
    "    lbl_bytes  = tf.io.read_file(lbl_path)\n",
    "\n",
    "    # 2) Decode\n",
    "    img = tf.io.decode_png(left_bytes, channels=3)     # uint8 [H,W,3]\n",
    "    lab = tf.io.decode_png(lbl_bytes,  channels=1)     # uint8/16 [H,W,1]\n",
    "\n",
    "    # 3) To workable dtypes\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)   # [0,1]\n",
    "    lab = tf.cast(lab, tf.int32)                          # index dtype for LUT\n",
    "\n",
    "    # 4) Remap 32→8 via LUT\n",
    "    lab_clipped = tf.minimum(lab, 255)\n",
    "    lab8 = tf.gather(LUT_TF, lab_clipped)                 # uint8 [H,W,1]\n",
    "    lab8 = tf.squeeze(lab8, axis=-1)                      # uint8 [H,W]\n",
    "\n",
    "    # 5) Simple augment (sync flip)\n",
    "    if training:\n",
    "        do_flip = tf.random.uniform(()) > 0.5\n",
    "        img  = tf.cond(do_flip, lambda: tf.image.flip_left_right(img), lambda: img)\n",
    "        lab8 = tf.cond(do_flip, lambda: tf.image.flip_left_right(lab8[..., None])[:, :, 0], lambda: lab8)\n",
    "\n",
    "    # 6) Resize (labels in nearest, keep uint8)\n",
    "    img  = tf.image.resize(img,  INPUT_SIZE, method=\"bilinear\")\n",
    "    lab8 = tf.cast(tf.image.resize(lab8[..., None], INPUT_SIZE, method=\"nearest\")[:, :, 0], tf.uint8)\n",
    "\n",
    "    # 7) Ignore handling → sample_weight (float32); labels safe (uint8→int32)\n",
    "    ignore_val = tf.constant(255, dtype=tf.uint8)\n",
    "    ignore = tf.equal(lab8, ignore_val)  # bool [H,W]\n",
    "\n",
    "    weights = tf.where(ignore,\n",
    "                       tf.zeros_like(lab8, dtype=tf.float32),\n",
    "                       tf.ones_like(lab8,  dtype=tf.float32))              # float32 [H,W]\n",
    "\n",
    "    lab8_safe = tf.where(ignore,\n",
    "                         tf.zeros_like(lab8),   # uint8 0 (will be masked by weights anyway)\n",
    "                         lab8)\n",
    "    labels = tf.cast(lab8_safe, tf.int32)                                    # int32 [H,W]\n",
    "\n",
    "    return img, labels, weights\n",
    "\n",
    "def make_dataset(split: str, batch_size: int = BATCH_SIZE, training: bool = True) -> tf.data.Dataset:\n",
    "    lefts, labels = list_pairs(split)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((lefts, labels))\n",
    "    if training:\n",
    "        ds = ds.shuffle(buffer_size=min(len(lefts), 2000), reshuffle_each_iteration=True)\n",
    "    ds = ds.map(lambda l, y: decode_and_preprocess(l, y, training),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size, drop_remainder=training)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# ==============\n",
    "# Smoke test\n",
    "# ==============\n",
    "train_ds = make_dataset(\"train\", batch_size=2, training=True)\n",
    "xb, yb, wb = next(iter(train_ds))\n",
    "print(\"x:\", xb.shape, xb.dtype, \"| y:\", yb.shape, yb.dtype, \"| w:\", wb.shape, wb.dtype)\n",
    "\n",
    "# Example compile/fit (model must output logits with 8 channels)\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# opt  = tf.keras.optimizers.Adam(1e-3)\n",
    "# model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
    "# model.fit(train_ds,\n",
    "#           validation_data=make_dataset(\"val\", batch_size=2, training=False),\n",
    "#           epochs=1)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b54c83afdf449ad0",
   "metadata": {},
   "source": [
    "### UNet Mini — Baseline rapide"
   ]
  },
  {
   "cell_type": "code",
   "id": "f08d845ee14a58e2",
   "metadata": {},
   "source": [
    "if not train_mode :\n",
    "    from dataclasses import replace\n",
    "    unet_mini_cfg = replace(\n",
    "        train_cfg,\n",
    "        output_dir=\"artifacts/unet_mini\",\n",
    "    )\n",
    "    train(\"unet_mini\", data_cfg, unet_mini_cfg, aug_cfg)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "da988384",
   "metadata": {},
   "source": [
    "Je duplique la configuration de base pour lancer un entraînement U-Net Mini. C'est mon modèle le plus léger : parfait pour valider rapidement que tout le pipeline fonctionne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625be39c1c74f8c2",
   "metadata": {},
   "source": [
    "### UNet VGG16 — Version plus profonde"
   ]
  },
  {
   "cell_type": "code",
   "id": "968a91d468deb367",
   "metadata": {},
   "source": [
    " if not train_mode :\n",
    "    unet_vgg16_cfg = replace(\n",
    "        train_cfg,\n",
    "        output_dir=\"artifacts/unet_vgg16\",\n",
    "    )\n",
    "    train(\"unet_vgg16\", data_cfg, unet_vgg16_cfg, aug_cfg)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c7c1b78b",
   "metadata": {},
   "source": [
    "Même principe pour U-Net VGG16, plus profond : je change uniquement le nom de sortie pour isoler ses artefacts et comparer ses performances à part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eac04f2c1a0511",
   "metadata": {},
   "source": [
    "### MobileDet Seg — Variante compacte"
   ]
  },
  {
   "cell_type": "code",
   "id": "17cc0800dcd2447b",
   "metadata": {},
   "source": [
    " if not train_mode :\n",
    "    mobiledet_seg_cfg = replace(\n",
    "        train_cfg,\n",
    "        output_dir=\"artifacts/mobiledet_seg\",\n",
    "    )\n",
    "    train(\"mobiledet_seg\", data_cfg, mobiledet_seg_cfg, aug_cfg)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a8b21ded",
   "metadata": {},
   "source": [
    "Pour MobileDet-Seg, je garde la même fonction `train` mais je cible un dossier dédié. L'objectif est d'évaluer une architecture optimisée pour les appareils embarqués."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda9ed5cbc57b167",
   "metadata": {},
   "source": [
    "### YOLOv9 Seg — Approche one-stage"
   ]
  },
  {
   "cell_type": "code",
   "id": "2bfc844613848b5f",
   "metadata": {},
   "source": [
    " if not train_mode :\n",
    "    yolov9_seg_cfg = replace(\n",
    "        train_cfg,\n",
    "        output_dir=\"artifacts/yolov9_seg\",\n",
    "    )\n",
    "    train(\"yolov9_seg\", data_cfg, yolov9_seg_cfg, aug_cfg)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0cbf5690",
   "metadata": {},
   "source": [
    "YOLOv9-Seg suit exactement la même recette : je paramètre un répertoire de sortie séparé afin de garder les résultats bien rangés et comparables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73d2a8040472e9b",
   "metadata": {},
   "source": [
    "### DeepLabV3+ ResNet50 — Modèle de référence"
   ]
  },
  {
   "cell_type": "code",
   "id": "c919827d0f66cddd",
   "metadata": {},
   "source": [
    "if not train_mode :\n",
    "    train(\"deeplab_resnet50\", data_cfg, train_cfg, aug_cfg, model_kwargs=deeplab_kwargs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "33fca599",
   "metadata": {},
   "source": [
    "DeepLabV3+ (ResNet50) joue ici le rôle de modèle de référence : je le lance seulement en mode entraînement complet afin de mesurer le meilleur niveau atteignable sur ce pipeline réduit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154c65411f171045",
   "metadata": {},
   "source": [
    "## 12. Faire le point sur les résultats intermédiaires\n",
    "\n",
    "Je regroupe ici toutes les métriques obtenues lors des entraînements exploratoires. Cela permet de comparer d'un coup d'œil le temps d'entraînement et la qualité de segmentation de chaque architecture.\n",
    "\n",
    "| Modèle                     |   Durée  | `masked_mIoU` (train) | `val_masked_mIoU` | `pix_acc` | `val_pix_acc` | `dice_coef` | `val_dice_coef` |\n",
    "| :------------------------- | :------: | :-------------------: | :---------------: | :-------: | :-----------: | :---------: | :-------------: |\n",
    "| **DeepLabV3+ (ResNet50)**  | 13.4 min |       **0.947**       |     **0.639**     | **0.989** |   **0.872**   |  **0.965**  |    **0.716**    |\n",
    "| **YOLOv9_seg (simplifié)** | 10.5 min |         0.689         |       0.400       |   0.913   |     0.714     |    0.753    |      0.494      |\n",
    "| **MobileDet_seg**          | 16.3 min |         0.938         |       0.502       |   0.987   |     0.779     |    0.953    |      0.600      |\n",
    "| **U-Net VGG16**            | 29.7 min |         0.903         |       0.542       |   0.977   |     0.805     |    0.923    |      0.633      |\n",
    "| **U-Net mini**             |  6.1 min |         0.563         |       0.319       |   0.851   |     0.634     |    0.650    |      0.407      |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Interprétation métrique par métrique\n",
    "\n",
    "### 🟦 `masked_mIoU` (train)\n",
    "\n",
    "* Mesure principale de segmentation (intersection sur union moyenne).\n",
    "* Tous sauf U-Net mini > 0.9 en entraînement → bon apprentissage.\n",
    "* U-Net mini (0.56) : trop léger, manque de capacité.\n",
    "\n",
    "### 🟧 `val_masked_mIoU`\n",
    "\n",
    "* Évalue la **généralisation**.\n",
    "* DeepLab (0.639) est **nettement supérieur** aux autres.\n",
    "* U-Net VGG16 (0.54) et MobileDet (0.50) suivent derrière.\n",
    "* YOLOv9 seg (0.40) et U-Net mini (0.32) décrochent clairement.\n",
    "\n",
    "### 🟩 `val_pix_acc`\n",
    "\n",
    "* Corrélation assez bonne avec `val_mIoU`.\n",
    "* DeepLab atteint 0.87 → très bonne segmentation globale.\n",
    "* U-Net VGG16 ≈ 0.80 → correct.\n",
    "* Les autres chutent < 0.78.\n",
    "\n",
    "### 🟪 `val_dice_coef`\n",
    "\n",
    "* Très proche du mIoU mais plus sensible aux petits objets.\n",
    "* DeepLab ≈ 0.72 → cohérent avec sa bonne mIoU.\n",
    "* U-Net VGG16 ≈ 0.63 et MobileDet ≈ 0.60 → acceptables.\n",
    "* YOLOv9 ≈ 0.49, U-Net mini ≈ 0.40 → faibles.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚖️ Analyse comparative\n",
    "\n",
    "| Critère                               | Meilleur modèle                            |\n",
    "| :------------------------------------ | :----------------------------------------- |\n",
    "| **Précision globale (mIoU/Dice)**     | 🟢 **DeepLabV3+ ResNet50**                 |\n",
    "| **Généralisation / stabilité val**    | 🟢 **DeepLabV3+ ResNet50**                 |\n",
    "| **Compromis vitesse/qualité**         | 🟢 **MobileDet_seg** (plus léger, correct) |\n",
    "| **Performance brute (haute qualité)** | 🟢 **U-Net VGG16** si VRAM suffisante      |\n",
    "| **Légereté / prototypage rapide**     | 🟢 **U-Net mini**, mais précision faible   |\n",
    "\n",
    "---\n",
    "\n",
    "## Interprétation détaillée\n",
    "\n",
    "### 🥇 **DeepLabV3+ (ResNet50)**\n",
    "\n",
    "* **Meilleur équilibre** entre précision et stabilité.\n",
    "* mIoU = 0.64 (val) et Dice = 0.72 (val) : excellents scores sur 8 classes.\n",
    "* Surapprentissage modéré (train-val gap raisonnable).\n",
    "* Très bonne capacité à capter les contours fins et la hiérarchie spatiale.\n",
    "  ✅ **→ Modèle à garder comme référence.**\n",
    "\n",
    "### 🥈 **U-Net VGG16**\n",
    "\n",
    "* Très bon entraînement, mais écart train-val > 0.35 : léger overfit.\n",
    "* Lourdeur mémoire (VGG16) mais résultats solides.\n",
    "  ✅ Alternative si tu veux plus de stabilité visuelle (textures fines).\n",
    "\n",
    "### 🥉 **MobileDet_seg**\n",
    "\n",
    "* Performances correctes pour un modèle “mobile-like”.\n",
    "* Bonne efficacité (seulement 16 min d’entraînement, résultats décents).\n",
    "  🟡 Bon compromis si tu cibles l’inférence embarquée.\n",
    "\n",
    "### ⚙️ **YOLOv9_seg**\n",
    "\n",
    "* Correct mais sous-optimal : architecture pas parfaitement adaptée à la segmentation dense.\n",
    "* Val mIoU = 0.40, Dice = 0.49 : pas suffisant pour une segmentation de qualité.\n",
    "  🔴 À éviter pour cette tâche spécifique.\n",
    "\n",
    "### ⚪ **U-Net mini**\n",
    "\n",
    "* Très rapide mais sous-entraîné / sous-dimensionné.\n",
    "* Mauvais scores val (mIoU = 0.32, Dice = 0.40).\n",
    "  🔴 Bon pour tests rapides, pas pour production.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧾 Conclusion\n",
    "\n",
    "| Rang | Modèle                    | Pourquoi                                                  |\n",
    "| :--: | :------------------------ | :-------------------------------------------------------- |\n",
    "|  🥇  | **DeepLabV3+ (ResNet50)** | Meilleur équilibre précision / généralisation / stabilité |\n",
    "|  🥈  | **U-Net VGG16**           | Très bon mais plus lourd, tendance à overfitter           |\n",
    "|  🥉  | **MobileDet_seg**         | Légèreté et vitesse, mais précision un cran en dessous    |\n",
    "|   4  | **YOLOv9_seg**            | Pas adapté à la segmentation dense                        |\n",
    "|   5  | **U-Net mini**            | Trop limité, résultats faibles                            |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 En résumé\n",
    "\n",
    "> **DeepLabV3+ ResNet50** est le **meilleur modèle global** :\n",
    ">\n",
    "> * meilleures métriques de validation,\n",
    "> * bon Dice et mIoU,\n",
    "> * rapport vitesse/qualité très favorable,\n",
    "> * faible overfit comparé à VGG16.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f224bfb380ca5bf",
   "metadata": {},
   "source": [
    "## 13. Chercher les hyperparamètres avec Optuna\n",
    "\n",
    "Cette cellule encapsule une recherche Optuna (désactivée par défaut) pour ajuster automatiquement les hyperparamètres de DeepLabV3+. Je la garde comme référence lorsque j'ai le temps de lancer une exploration complète."
   ]
  },
  {
   "cell_type": "code",
   "id": "f1b2aaea",
   "metadata": {},
   "source": [
    "if False:\n",
    "    # === Optuna hyperparameter search for DeepLabV3+ (ResNet50) ===\n",
    "    import json\n",
    "    import gc\n",
    "    from dataclasses import replace\n",
    "\n",
    "    from optuna.exceptions import TrialPruned\n",
    "    try:\n",
    "        import optuna\n",
    "    except ModuleNotFoundError:  # pragma: no cover - handled at runtime\n",
    "        import subprocess, sys\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'optuna', '--quiet'])\n",
    "        import optuna\n",
    "\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    physical_gpus = tf.config.list_physical_devices('GPU')\n",
    "    for _gpu in physical_gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(_gpu, True)\n",
    "        except (RuntimeError, ValueError):\n",
    "            pass\n",
    "\n",
    "\n",
    "    search_data_cfg = replace(\n",
    "        data_cfg,\n",
    "        batch_size=2,\n",
    "        max_train_samples=50,\n",
    "        verbose=False,\n",
    "        max_val_samples=10,\n",
    "    )\n",
    "    search_train_cfg = replace(\n",
    "        train_cfg,\n",
    "        epochs=12,\n",
    "        early_stop_patience=3,\n",
    "        output_dir='artifacts/deeplab_resnet50_optuna',\n",
    "        exp_name='deeplabv3plus-resnet50-optuna',\n",
    "        arch='deeplab_resnet50',\n",
    "    )\n",
    "\n",
    "    _MODEL_PARAM_KEYS = {\n",
    "        'output_stride',\n",
    "        'aspp_dilations',\n",
    "        'decoder_filters',\n",
    "        'aspp_dropout',\n",
    "        'decoder_activation',\n",
    "        'aspp_activation',\n",
    "    }\n",
    "\n",
    "    best_deeplab_model_params = None\n",
    "    best_deeplab_train_params = None\n",
    "    best_deeplab_data_params = None\n",
    "    best_deeplab_val_miou = None\n",
    "\n",
    "\n",
    "    def _suggest_params(trial: optuna.Trial) -> dict:\n",
    "        return {\n",
    "            'output_stride': trial.suggest_categorical('output_stride', [8, 16]),\n",
    "            'aspp_dilations': trial.suggest_categorical('aspp_dilations', [(6, 12, 18), (12, 24, 36)]),\n",
    "            'decoder_filters': trial.suggest_categorical('decoder_filters', [128, 256, 512]),\n",
    "            'aspp_dropout': trial.suggest_float('aspp_dropout', 0.0, 0.3),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "            'momentum': trial.suggest_float('momentum', 0.8, 0.95),\n",
    "            'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),\n",
    "            'optimizer': trial.suggest_categorical('optimizer', ['SGD', 'AdamW']),\n",
    "            'poly_power': trial.suggest_float('poly_power', 0.8, 1.0),\n",
    "            'decoder_activation': trial.suggest_categorical('decoder_activation', ['relu', 'gelu']),\n",
    "            'aspp_activation': trial.suggest_categorical('aspp_activation', ['relu', 'gelu']),\n",
    "        }\n",
    "\n",
    "\n",
    "    def objective(trial: optuna.Trial) -> float:\n",
    "        params = _suggest_params(trial)\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "        trial_data_cfg = replace(\n",
    "            search_data_cfg,\n",
    "            batch_size=2,\n",
    "        )\n",
    "        momentum = params['momentum'] if params['optimizer'] == 'SGD' else None\n",
    "        trial_train_cfg = replace(\n",
    "            search_train_cfg,\n",
    "            lr=params['learning_rate'],\n",
    "            optimizer=params['optimizer'].lower(),\n",
    "            momentum=momentum,\n",
    "            weight_decay=params['weight_decay'],\n",
    "            poly_power=params['poly_power'],\n",
    "            output_dir=f\"artifacts/deeplab_resnet50_optuna/trial_{trial.number}\",\n",
    "        )\n",
    "        model_kwargs = {k: params[k] for k in _MODEL_PARAM_KEYS}\n",
    "\n",
    "        metrics = {}\n",
    "        try:\n",
    "            metrics = train(\n",
    "                'deeplab_resnet50',\n",
    "                trial_data_cfg,\n",
    "                trial_train_cfg,\n",
    "                aug_cfg,\n",
    "                model_kwargs=model_kwargs,\n",
    "                use_mlflow=False,\n",
    "                keep_artifacts=False,\n",
    "                cleanup_after=True,\n",
    "                probe_dataset=False,\n",
    "            )\n",
    "        except (tf.errors.ResourceExhaustedError, tf.errors.InternalError, MemoryError) as exc:\n",
    "            trial.set_user_attr('failure', f'{type(exc).__name__}: {exc}')\n",
    "            raise TrialPruned(f'Pruned because of resource exhaustion: {exc}') from exc\n",
    "        finally:\n",
    "            del trial_data_cfg, trial_train_cfg, model_kwargs\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "\n",
    "        val_miou = float(metrics.get('masked_mIoU', 0.0))\n",
    "        trial.set_user_attr('metrics', dict(metrics))\n",
    "        metrics.clear()\n",
    "        metrics = None\n",
    "        return val_miou\n",
    "\n",
    "    def _json_default(value):\n",
    "        if isinstance(value, tuple):\n",
    "            return list(value)\n",
    "        return value\n",
    "\n",
    "\n",
    "    if train_mode:\n",
    "        study = optuna.create_study(direction='maximize', study_name='deeplabv3plus_cityscapes')\n",
    "        study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "\n",
    "        best_deeplab_val_miou = float(study.best_value)\n",
    "        best_params = study.best_trial.params\n",
    "        best_deeplab_model_params = {k: best_params[k] for k in _MODEL_PARAM_KEYS}\n",
    "        best_deeplab_train_params = {\n",
    "            'learning_rate': best_params['learning_rate'],\n",
    "            'optimizer': best_params['optimizer'].lower(),\n",
    "            'momentum': best_params['momentum'] if best_params['optimizer'] == 'SGD' else None,\n",
    "            'weight_decay': best_params['weight_decay'],\n",
    "            'poly_power': best_params['poly_power'],\n",
    "        }\n",
    "        best_deeplab_data_params = {'batch_size': best_params['batch_size']}\n",
    "\n",
    "        print(f\"Best val_mIoU: {best_deeplab_val_miou:.4f}\")\n",
    "        print('Best hyperparameters:')\n",
    "        print(json.dumps(best_params, indent=2, default=_json_default))\n",
    "    else:\n",
    "        print('train_mode=False → Optuna search skipped.')\n",
    "        best_deeplab_model_params = {}\n",
    "        best_deeplab_train_params = {}\n",
    "        best_deeplab_data_params = {}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "75116b2e",
   "metadata": {},
   "source": [
    "## 14. Entraîner DeepLabV3+ sur 100% des données\n",
    "\n",
    "Je relance DeepLabV3+ avec **l'intégralité** du dataset et sansn puis avec toutes les augmentations activées. Grâce aux dataclasses, un simple `replace` suffit pour figer une nouvelle configuration traçable."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sans augmentation",
   "id": "baf48f05d43c8931"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if True:\n",
    "    # Baseline sans augmentation : métriques loggées dans MLflow uniquement\n",
    "    from dataclasses import replace\n",
    "\n",
    "    baseline_data_cfg = replace(\n",
    "        data_cfg,\n",
    "        max_train_samples=None,\n",
    "        max_val_samples=None,\n",
    "    )\n",
    "\n",
    "    baseline_train_cfg = replace(\n",
    "        train_cfg,\n",
    "        output_dir=\"artifacts/deeplab_resnet50_noaug\",\n",
    "        exp_name=\"cityscapes-seg-8cls-noaug\",\n",
    "    )\n",
    "\n",
    "    baseline_aug_cfg = replace(\n",
    "        aug_cfg,\n",
    "        enabled=False,\n",
    "    )\n",
    "\n",
    "    train(\n",
    "        \"deeplab_resnet50\",\n",
    "        baseline_data_cfg,\n",
    "        baseline_train_cfg,\n",
    "        baseline_aug_cfg,\n",
    "        model_kwargs=deeplab_kwargs,\n",
    "        keep_artifacts=True,\n",
    "        cleanup_after=True,\n",
    "    )\n"
   ],
   "id": "c7128a84d3fdf135",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Avec augmentation (final)",
   "id": "58081a698597de53"
  },
  {
   "cell_type": "code",
   "id": "1828657bb72691a6",
   "metadata": {},
   "source": [
    "from dataclasses import replace\n",
    "# Configuration finale : dataset complet + sorties dédiées (avec et sans augmentation)\n",
    "final_data_cfg = replace(\n",
    "    data_cfg,\n",
    "    max_train_samples=None,\n",
    "    max_val_samples=None,\n",
    ")\n",
    "final_train_cfg = replace(\n",
    "    train_cfg,\n",
    "    output_dir=\"artifacts/deeplab_resnet50_full\",\n",
    "    exp_name=\"cityscapes-seg-8cls-full\",\n",
    ")\n",
    "final_aug_cfg = replace(\n",
    "    aug_cfg,\n",
    "    enabled=True,\n",
    ")\n",
    "\n",
    "# Lancement de l'entraînement complet (sauvegarde locale + tracking MLflow)\n",
    "train(\"deeplab_resnet50\", final_data_cfg, final_train_cfg, final_aug_cfg, model_kwargs=deeplab_kwargs)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dbd9095f",
   "metadata": {},
   "source": [
    "## 15. Sauvegarder le meilleur modèle pour l'API\n",
    "\n",
    "Une fois l'apprentissage terminé, je copie le meilleur checkpoint vers un dossier standardisé (`artifacts/api`). Cette étape garantit que l'API et les futurs tests chargeront exactement le même modèle."
   ]
  },
  {
   "cell_type": "code",
   "id": "73c6a586d43eb52b",
   "metadata": {},
   "source": [
    "# Copie explicite du meilleur modèle pour l'API\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "best_model = Path(\"artifacts/deeplab_resnet50_full/deeplab_resnet50_best.keras\")\n",
    "api_export = Path(\"artifacts/api/deeplabv3plus_resnet50_full.keras\")\n",
    "api_export.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if best_model.exists():\n",
    "    shutil.copy2(best_model, api_export)\n",
    "    print(f\"✅ Modèle API (best) : {api_export}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Aucun modèle entraîné trouvé. Lance d'abord la cellule d'entraînement.\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
