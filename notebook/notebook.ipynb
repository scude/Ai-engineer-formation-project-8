{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5eeaf89ff30992",
   "metadata": {},
   "source": [
    "# Projet 8"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_mode = True",
   "id": "5d0adc820682eef2"
  },
  {
   "cell_type": "code",
   "id": "84b61c778fafe1a",
   "metadata": {},
   "source": [
    "# --- Simple dataset scanner (counts only, no filenames, no JSON) ---\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# >>> Change this to your dataset root (WSL path) <<<\n",
    "ROOT = Path(\"../data\")\n",
    "\n",
    "IGNORE_HIDDEN = True       # ignore .git, __pycache__, etc.\n",
    "MAX_DIRS_TO_SHOW = 80      # limit directory lines for readability\n",
    "\n",
    "total_files = 0\n",
    "total_dirs = 0\n",
    "by_ext = Counter()\n",
    "by_dir = {}\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(ROOT):\n",
    "    # optionally hide hidden/internal dirs\n",
    "    if IGNORE_HIDDEN:\n",
    "        dirnames[:] = [d for d in dirnames if not d.startswith(\".\") and not d.startswith(\"__\")]\n",
    "    total_dirs += 1\n",
    "    rel = Path(dirpath).relative_to(ROOT) if Path(dirpath) != ROOT else Path(\".\")\n",
    "    by_dir[str(rel)] = len(filenames)\n",
    "    for fn in filenames:\n",
    "        by_ext[Path(fn).suffix.lower()] += 1\n",
    "    total_files += len(filenames)\n",
    "\n",
    "print(f\"[ROOT] {ROOT}\")\n",
    "print(f\"dirs={total_dirs:,}  files={total_files:,}\\n\")\n",
    "\n",
    "print(\"By extension (top 10):\")\n",
    "for ext, n in by_ext.most_common(10):\n",
    "    print(f\"  {ext or '(no ext)'}: {n:,}\")\n",
    "print()\n",
    "\n",
    "print(f\"Directory counts (first {MAX_DIRS_TO_SHOW}):\")\n",
    "for i, (rel, n) in enumerate(sorted(by_dir.items())):\n",
    "    if i >= MAX_DIRS_TO_SHOW:\n",
    "        print(\"  ... (truncated)\")\n",
    "        break\n",
    "    print(f\"  {rel}: {n}\")\n",
    "\n",
    "# -------- Cityscapes mini-summary (counts only) --------\n",
    "def count_pattern(base: Path, split: str, suffix: str) -> int:\n",
    "    split_dir = base / split\n",
    "    total = 0\n",
    "    if split_dir.exists():\n",
    "        for city_dir in split_dir.iterdir():\n",
    "            if city_dir.is_dir():\n",
    "                total += sum(1 for p in city_dir.iterdir()\n",
    "                             if p.is_file() and p.name.endswith(suffix))\n",
    "    return total\n",
    "\n",
    "print(\"\\n[Cityscapes summary]\")\n",
    "for split in (\"train\", \"val\", \"test\"):\n",
    "    gt_base = ROOT / \"gtFine\"\n",
    "    left_base = ROOT / \"leftImg8bit\"\n",
    "    label = count_pattern(gt_base, split, \"_gtFine_labelIds.png\")\n",
    "    color = count_pattern(gt_base, split, \"_gtFine_color.png\")\n",
    "    inst  = count_pattern(gt_base, split, \"_gtFine_instanceIds.png\")\n",
    "    poly  = count_pattern(gt_base, split, \"_gtFine_polygons.json\")\n",
    "    left  = count_pattern(left_base, split, \"_leftImg8bit.png\")\n",
    "    print(f\"  {split:5s}: leftImg8bit={left:6d}  labelIds={label:6d}  color={color:6d}  instanceIds={inst:6d}  polygons.json={poly:6d}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b8ee8ae56f5c54f",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"../data\")  # adapte si besoin\n",
    "SUF_LEFT = \"_leftImg8bit.png\"\n",
    "SUF_LBL  = \"_gtFine_labelIds.png\"\n",
    "\n",
    "def base_id(name: str) -> str:\n",
    "    return name[:-len(SUF_LEFT)] if name.endswith(SUF_LEFT) else name[:-len(SUF_LBL)]\n",
    "\n",
    "def split_counts(split: str):\n",
    "    left_dir = ROOT / \"leftImg8bit\" / split\n",
    "    lbl_dir  = ROOT / \"gtFine\"      / split\n",
    "    left = sorted(left_dir.rglob(f\"*{SUF_LEFT}\")) if left_dir.exists() else []\n",
    "    lbl  = sorted(lbl_dir.rglob (f\"*{SUF_LBL}\" )) if lbl_dir.exists()  else []\n",
    "    left_ids = {base_id(p.name) for p in left}\n",
    "    lbl_ids  = {base_id(p.name) for p in lbl}\n",
    "    paired = left_ids & lbl_ids\n",
    "    print(f\"{split:<5} | left={len(left):4d}  labels={len(lbl):4d}  paired={len(paired):4d}\")\n",
    "\n",
    "for sp in (\"train\", \"val\", \"test\"):\n",
    "    split_counts(sp)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3b30f7c4d8786b68",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "ROOT = Path(\"../data\")\n",
    "\n",
    "PALETTE = {\n",
    "    7:(128,64,128), 8:(244,35,232), 11:(70,70,70), 12:(102,102,156), 13:(190,153,153),\n",
    "    17:(153,153,153), 19:(250,170,30), 20:(220,220,0), 21:(107,142,35), 22:(152,251,152),\n",
    "    23:(70,130,180), 24:(220,20,60), 25:(255,0,0), 26:(0,0,142), 27:(0,0,70),\n",
    "    28:(0,60,100), 31:(0,80,100), 32:(0,0,230), 33:(119,11,32),\n",
    "}\n",
    "\n",
    "def pairs(split=\"val\"):\n",
    "    lbls = sorted((ROOT/\"gtFine\"/split).rglob(\"*_gtFine_labelIds.png\"))\n",
    "    out = []\n",
    "    for lp in lbls:\n",
    "        stem = lp.name.replace(\"_gtFine_labelIds.png\", \"\")\n",
    "        city = lp.parent.name\n",
    "        left = ROOT/\"leftImg8bit\"/split/city/(stem+\"_leftImg8bit.png\")\n",
    "        if left.exists():\n",
    "            out.append((left, lp))\n",
    "    return out\n",
    "\n",
    "def colorize(ids: np.ndarray) -> Image.Image:\n",
    "    h, w = ids.shape\n",
    "    rgb = np.zeros((h, w, 3), np.uint8)\n",
    "    for k, c in PALETTE.items():\n",
    "        rgb[ids == k] = c\n",
    "    return Image.fromarray(rgb, \"RGB\")\n",
    "\n",
    "def overlay(img: Image.Image, mask_rgb: Image.Image, alpha=0.5) -> Image.Image:\n",
    "    a = np.asarray(img.convert(\"RGB\"), np.float32)\n",
    "    b = np.asarray(mask_rgb, np.float32)\n",
    "    return Image.fromarray(np.clip((1-alpha)*a + alpha*b, 0, 255).astype(np.uint8))\n",
    "\n",
    "samples = pairs(\"val\")\n",
    "assert samples, \"No pairs found — check your paths.\"\n",
    "random.shuffle(samples)\n",
    "k = 3\n",
    "\n",
    "plt.figure(figsize=(15, 5*k))\n",
    "for i, (left_p, lbl_p) in enumerate(samples[:k]):\n",
    "    left = Image.open(left_p).convert(\"RGB\")\n",
    "    ids  = np.array(Image.open(lbl_p))\n",
    "    mask = colorize(ids)\n",
    "    over = overlay(left, mask, alpha=0.45)\n",
    "    for j, (img, title) in enumerate([(left,\"leftImg8bit\"),(mask,\"labelIds (colored)\"),(over,\"overlay\")]):\n",
    "        ax = plt.subplot(k, 3, i*3 + j + 1)\n",
    "        ax.imshow(img); ax.set_title(f\"{left_p.parent.name} — {title}\", fontsize=10); ax.axis(\"off\")\n",
    "plt.tight_layout(); plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a81dea999fabce6b",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f540b1886f92c7b1",
   "metadata": {},
   "source": [
    "\n",
    "### Remapping Cityscapes 32→8 classes\n",
    "\n",
    "Vérifier la balance des classes !!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1b632fc00041fe27",
   "metadata": {},
   "source": [
    "# --- 32→8 mapping (Cityscapes labelIds -> 8-class IDs), ignore = 255 ---\n",
    "import numpy as np\n",
    "\n",
    "CS_LABELID_TO_8 = {\n",
    "    # 0..5 (voids) -> ignore by LUT fill (no need to list)\n",
    "    6: 0,\n",
    "    7: 0,  9: 0, 10: 0,           # road-like: road, parking, rail track\n",
    "    8: 1,                         # sidewalk\n",
    "    11: 2, 12: 2, 13: 2, 14: 2, 15: 2, 16: 2,   # building + barriers\n",
    "    17: 3, 18: 3, 19: 3, 20: 3,                 # traffic objs (pole/ts/tl)\n",
    "    21: 4, 22: 4,                                 # vegetation + terrain\n",
    "    23: 5,                                       # sky\n",
    "    24: 6, 25: 6,                                 # person + rider\n",
    "    26: 7, 27: 7, 28: 7, 29: 7, 30: 7, 31: 7, 32: 7, 33: 7,  # vehicles\n",
    "}\n",
    "\n",
    "def build_labelid_to8_lut(ignore_value: int = 255) -> np.ndarray:\n",
    "    \"\"\"Create a 256-entry LUT mapping Cityscapes labelIds -> {0..7} or 255(ignore).\"\"\"\n",
    "    lut = np.full(256, ignore_value, dtype=np.uint8)\n",
    "    for k, v in CS_LABELID_TO_8.items():\n",
    "        lut[k] = v\n",
    "    return lut\n",
    "\n",
    "LUT_32TO8 = build_labelid_to8_lut(ignore_value=255)\n",
    "\n",
    "def remap_labelids_to8(arr_uint16: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Vectorized remap of HxW labelIds (uint16/uint8) to 8-class IDs with 255 ignore.\"\"\"\n",
    "    arr = arr_uint16.astype(np.uint16)\n",
    "    arr = np.minimum(arr, 255).astype(np.uint8)\n",
    "    return LUT_32TO8[arr]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4090ae75134d47fe",
   "metadata": {},
   "source": [
    "PALETTE_8 = {\n",
    "    0:(128,64,128),   # road\n",
    "    1:(244,35,232),   # sidewalk\n",
    "    2:(70,70,70),     # building+barrier\n",
    "    3:(220,220,0),    # traffic objs\n",
    "    4:(107,142,35),   # vegetation/terrain\n",
    "    5:(70,130,180),   # sky\n",
    "    6:(220,20,60),    # person+rider\n",
    "    7:(0,0,142),      # vehicle\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "623c52d0928c016c",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def colorize_8(label8: np.ndarray, palette: dict) -> Image.Image:\n",
    "    h, w = label8.shape\n",
    "    rgb = np.zeros((h, w, 3), np.uint8)\n",
    "    for k, c in palette.items():\n",
    "        rgb[label8 == k] = c\n",
    "    return Image.fromarray(rgb, \"RGB\")\n",
    "\n",
    "sample_lbl = next(Path(\"../data/gtFine/val/frankfurt\").glob(\"*_gtFine_labelIds.png\"))\n",
    "arr = np.array(Image.open(sample_lbl))\n",
    "arr8 = remap_labelids_to8(arr)\n",
    "plt.figure(figsize=(8,4)); plt.imshow(colorize_8(arr8, PALETTE_8)); plt.axis(\"off\"); plt.title(\"8-class mask\"); plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "71f824d108f9b17f",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# ↓↓↓ Quieter TensorFlow logs (set BEFORE importing tf)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"     # 0=all, 1=INFO off, 2=INFO+WARNING off, 3=all off\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"  # avoid grabbing all GPU memory\n",
    "# Optional: disable oneDNN (removes the \"oneDNN custom ops are on\" line, and tiny numeric diffs)\n",
    "# os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from absl import logging as absl_logging\n",
    "absl_logging.set_verbosity(absl_logging.ERROR)  # reduce absl spam\n",
    "\n",
    "# (Optional) confirm GPU + set memory growth (extra safety)\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for g in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(g, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"TF:\", tf.__version__, \"| GPUs:\", gpus)\n",
    "\n",
    "# ==========================\n",
    "# Cityscapes 32→8 remapping\n",
    "# ==========================\n",
    "import numpy as np\n",
    "\n",
    "# 8 classes for embedded use (ignore=255):\n",
    "# 0=road (7,9,10) | 1=sidewalk(8) | 2=building+barriers(11–16) | 3=traffic objs(17–20)\n",
    "# 4=vegetation+terrain(21,22) | 5=sky(23) | 6=person+rider(24,25) | 7=vehicle(26–33)\n",
    "CS_LABELID_TO_8 = {\n",
    "    6:0,\n",
    "    7:0, 9:0, 10:0,\n",
    "    8:1,\n",
    "    11:2, 12:2, 13:2, 14:2, 15:2, 16:2,\n",
    "    17:3, 18:3, 19:3, 20:3,\n",
    "    21:4, 22:4,\n",
    "    23:5,\n",
    "    24:6, 25:6,\n",
    "    26:7, 27:7, 28:7, 29:7, 30:7, 31:7, 32:7, 33:7,\n",
    "}\n",
    "\n",
    "def build_labelid_to8_lut(ignore_value: int = 255) -> np.ndarray:\n",
    "    lut = np.full(256, ignore_value, dtype=np.uint8)\n",
    "    for k, v in CS_LABELID_TO_8.items():\n",
    "        lut[k] = v\n",
    "    return lut\n",
    "\n",
    "LUT_32TO8 = build_labelid_to8_lut(ignore_value=255)\n",
    "LUT_TF = tf.convert_to_tensor(LUT_32TO8, dtype=tf.uint8)  # shape [256]\n",
    "\n",
    "# ===================\n",
    "# Dataset (tf.data)\n",
    "# ===================\n",
    "from pathlib import Path\n",
    "ROOT = Path(\"../data\")               # <<< change if needed (WSL path)\n",
    "INPUT_SIZE = (512, 1024)             # (H, W)\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "SUF_LEFT = \"_leftImg8bit.png\"\n",
    "SUF_LBL  = \"_gtFine_labelIds.png\"\n",
    "\n",
    "def list_pairs(split: str):\n",
    "    \"\"\"Return two aligned lists: left paths and label paths for a given split.\"\"\"\n",
    "    lefts, labels = [], []\n",
    "    lbl_root = ROOT / \"gtFine\" / split\n",
    "    for lbl in sorted(lbl_root.rglob(f\"*{SUF_LBL}\")):\n",
    "        city = lbl.parent.name\n",
    "        stem = lbl.name.replace(SUF_LBL, \"\")\n",
    "        left = ROOT / \"leftImg8bit\" / split / city / f\"{stem}{SUF_LEFT}\"\n",
    "        if left.exists():\n",
    "            lefts.append(str(left))\n",
    "            labels.append(str(lbl))\n",
    "    if not lefts:\n",
    "        raise FileNotFoundError(f\"No pairs found for split='{split}'. Check your paths under {ROOT}.\")\n",
    "    return lefts, labels\n",
    "\n",
    "def decode_and_preprocess(left_path, lbl_path, training: bool):\n",
    "    # 1) Read bytes\n",
    "    left_bytes = tf.io.read_file(left_path)\n",
    "    lbl_bytes  = tf.io.read_file(lbl_path)\n",
    "\n",
    "    # 2) Decode\n",
    "    img = tf.io.decode_png(left_bytes, channels=3)     # uint8 [H,W,3]\n",
    "    lab = tf.io.decode_png(lbl_bytes,  channels=1)     # uint8/16 [H,W,1]\n",
    "\n",
    "    # 3) To workable dtypes\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)   # [0,1]\n",
    "    lab = tf.cast(lab, tf.int32)                          # index dtype for LUT\n",
    "\n",
    "    # 4) Remap 32→8 via LUT\n",
    "    lab_clipped = tf.minimum(lab, 255)\n",
    "    lab8 = tf.gather(LUT_TF, lab_clipped)                 # uint8 [H,W,1]\n",
    "    lab8 = tf.squeeze(lab8, axis=-1)                      # uint8 [H,W]\n",
    "\n",
    "    # 5) Simple augment (sync flip)\n",
    "    if training:\n",
    "        do_flip = tf.random.uniform(()) > 0.5\n",
    "        img  = tf.cond(do_flip, lambda: tf.image.flip_left_right(img), lambda: img)\n",
    "        lab8 = tf.cond(do_flip, lambda: tf.image.flip_left_right(lab8[..., None])[:, :, 0], lambda: lab8)\n",
    "\n",
    "    # 6) Resize (labels in nearest, keep uint8)\n",
    "    img  = tf.image.resize(img,  INPUT_SIZE, method=\"bilinear\")\n",
    "    lab8 = tf.cast(tf.image.resize(lab8[..., None], INPUT_SIZE, method=\"nearest\")[:, :, 0], tf.uint8)\n",
    "\n",
    "    # 7) Ignore handling → sample_weight (float32); labels safe (uint8→int32)\n",
    "    ignore_val = tf.constant(255, dtype=tf.uint8)\n",
    "    ignore = tf.equal(lab8, ignore_val)  # bool [H,W]\n",
    "\n",
    "    weights = tf.where(ignore,\n",
    "                       tf.zeros_like(lab8, dtype=tf.float32),\n",
    "                       tf.ones_like(lab8,  dtype=tf.float32))              # float32 [H,W]\n",
    "\n",
    "    lab8_safe = tf.where(ignore,\n",
    "                         tf.zeros_like(lab8),   # uint8 0 (will be masked by weights anyway)\n",
    "                         lab8)\n",
    "    labels = tf.cast(lab8_safe, tf.int32)                                    # int32 [H,W]\n",
    "\n",
    "    return img, labels, weights\n",
    "\n",
    "def make_dataset(split: str, batch_size: int = BATCH_SIZE, training: bool = True) -> tf.data.Dataset:\n",
    "    lefts, labels = list_pairs(split)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((lefts, labels))\n",
    "    if training:\n",
    "        ds = ds.shuffle(buffer_size=min(len(lefts), 2000), reshuffle_each_iteration=True)\n",
    "    ds = ds.map(lambda l, y: decode_and_preprocess(l, y, training),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size, drop_remainder=training)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# ==============\n",
    "# Smoke test\n",
    "# ==============\n",
    "train_ds = make_dataset(\"train\", batch_size=2, training=True)\n",
    "xb, yb, wb = next(iter(train_ds))\n",
    "print(\"x:\", xb.shape, xb.dtype, \"| y:\", yb.shape, yb.dtype, \"| w:\", wb.shape, wb.dtype)\n",
    "\n",
    "# Example compile/fit (model must output logits with 8 channels)\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# opt  = tf.keras.optimizers.Adam(1e-3)\n",
    "# model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
    "# model.fit(train_ds,\n",
    "#           validation_data=make_dataset(\"val\", batch_size=2, training=False),\n",
    "#           epochs=1)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fa8eafa784ea3287",
   "metadata": {},
   "source": [
    "# ==== Class balance for Cityscapes 8 classes (with ignore=255) ====\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# ---- Config (adapt if needed) ----\n",
    "ROOT = Path(\"../data\")          # dataset root (WSL path)\n",
    "SPLIT = \"train\"                 # \"train\" | \"val\" | \"test\"\n",
    "SUF_LBL = \"_gtFine_labelIds.png\"\n",
    "\n",
    "# 8-class names (your mapping)\n",
    "CLASS8_NAMES = [\n",
    "    \"road\", \"sidewalk\", \"building+barriers\", \"traffic-objs\",\n",
    "    \"vegetation+terrain\", \"sky\", \"person+rider\", \"vehicle\"\n",
    "]\n",
    "\n",
    "# If LUT_32TO8 not in scope, (re)build it quickly:\n",
    "try:\n",
    "    LUT_32TO8\n",
    "except NameError:\n",
    "    CS_LABELID_TO_8 = {\n",
    "        6:0, 7:0, 9:0, 10:0, 8:1, 11:2,12:2,13:2,14:2,15:2,16:2,\n",
    "        17:3,18:3,19:3,20:3, 21:4,22:4, 23:5, 24:6,25:6,\n",
    "        26:7,27:7,28:7,29:7,30:7,31:7,32:7,33:7,\n",
    "    }\n",
    "    LUT_32TO8 = np.full(256, 255, dtype=np.uint8)\n",
    "    for k, v in CS_LABELID_TO_8.items():\n",
    "        LUT_32TO8[k] = v\n",
    "\n",
    "def remap_to8_np(arr_label_ids: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"arr_label_ids: HxW uint16/uint8 -> HxW uint8 in {0..7,255}\"\"\"\n",
    "    arr = arr_label_ids.astype(np.uint16)\n",
    "    arr = np.minimum(arr, 255).astype(np.uint8)\n",
    "    return LUT_32TO8[arr]\n",
    "\n",
    "def class_balance(split: str = SPLIT):\n",
    "    lbl_paths = sorted((ROOT/\"gtFine\"/split).rglob(f\"*{SUF_LBL}\"))\n",
    "    assert lbl_paths, f\"No labels found under {ROOT}/gtFine/{split}\"\n",
    "    counts = np.zeros(8, dtype=np.int64)\n",
    "    ignore = 0\n",
    "    for i, p in enumerate(lbl_paths, 1):\n",
    "        lab = np.array(Image.open(p))         # (H,W) uint16/uint8\n",
    "        lab8 = remap_to8_np(lab)              # (H,W) uint8\n",
    "        m_ignore = (lab8 == 255)\n",
    "        ignore += int(m_ignore.sum())\n",
    "        # bincount only on valid pixels\n",
    "        c = np.bincount(lab8[~m_ignore].ravel(), minlength=8)\n",
    "        counts += c[:8]\n",
    "        if i % 500 == 0 or i == len(lbl_paths):\n",
    "            print(f\"[{split}] processed {i}/{len(lbl_paths)} images...\", end=\"\\r\")\n",
    "    print()\n",
    "    total_valid = int(counts.sum())\n",
    "    total_pixels = total_valid + ignore\n",
    "    freqs = counts / max(total_valid, 1)\n",
    "    return counts, ignore, total_valid, total_pixels, freqs\n",
    "\n",
    "counts, ignore, total_valid, total_pixels, freqs = class_balance(\"train\")\n",
    "\n",
    "print(\"\\n=== Class balance (train) ===\")\n",
    "for k, (name, n, f) in enumerate(zip(CLASS8_NAMES, counts, freqs)):\n",
    "    print(f\"{k}: {name:<20s}  pixels={n:,}   freq={f:.4%}\")\n",
    "print(f\"\\nignore pixels (==255): {ignore:,}\")\n",
    "print(f\"total valid pixels:     {total_valid:,}\")\n",
    "print(f\"total pixels (incl. ignore): {total_pixels:,}\")\n",
    "\n",
    "# ---- Optional: derive class weights ----\n",
    "# Inverse-frequency, normalized to mean=1 (good starting point)\n",
    "weights_inv = (1.0 / np.maximum(freqs, 1e-12))\n",
    "weights_inv = weights_inv / weights_inv.mean()\n",
    "print(\"\\nSuggested class weights (inverse-freq, mean≈1):\")\n",
    "for k, (name, w) in enumerate(zip(CLASS8_NAMES, weights_inv)):\n",
    "    print(f\"{k}: {name:<20s}  w={w:.3f}\")\n",
    "\n",
    "# Median-frequency balancing (alternative)\n",
    "median_f = np.median(freqs[freqs > 0])\n",
    "weights_med = median_f / np.maximum(freqs, 1e-12)\n",
    "weights_med = weights_med / weights_med.mean()\n",
    "print(\"\\nSuggested class weights (median-freq, mean≈1):\")\n",
    "for k, (name, w) in enumerate(zip(CLASS8_NAMES, weights_med)):\n",
    "    print(f\"{k}: {name:<20s}  w={w:.3f}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8433da3712483ac",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# expects: counts (np.array shape [8]), freqs (shape [8]), ignore (int),\n",
    "#          total_valid (int), total_pixels (int), CLASS8_NAMES (list of 8 str)\n",
    "\n",
    "# ---- 1) Bar chart des 8 classes (trié décroissant) ----\n",
    "order = np.argsort(freqs)[::-1]\n",
    "names_sorted = [CLASS8_NAMES[i] for i in order]\n",
    "freqs_sorted = freqs[order]\n",
    "counts_sorted = counts[order]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.bar(range(len(names_sorted)), freqs_sorted)  # no explicit colors\n",
    "plt.xticks(range(len(names_sorted)), names_sorted, rotation=20, ha=\"right\")\n",
    "plt.ylabel(\"Frequency (share of valid pixels)\")\n",
    "plt.title(\"Cityscapes (train) — Class balance (8 classes)\")\n",
    "\n",
    "# annotations: % + millions de pixels\n",
    "for i, (b, f, c) in enumerate(zip(bars, freqs_sorted, counts_sorted)):\n",
    "    plt.text(b.get_x() + b.get_width()/2,\n",
    "             b.get_height() + 0.002,\n",
    "             f\"{f*100:.1f}%\\n{c/1e6:.1f}M\",\n",
    "             ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.ylim(0, max(freqs_sorted)*1.15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 2) Valid vs Ignore (pour info) ----\n",
    "valid_share = total_valid / total_pixels\n",
    "ignore_share = 1.0 - valid_share\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "bars2 = plt.bar([0,1], [valid_share, ignore_share])\n",
    "plt.xticks([0,1], [\"valid\", \"ignore (==255)\"])\n",
    "plt.ylabel(\"Share of total pixels\")\n",
    "plt.title(\"Valid vs Ignore pixels (train)\")\n",
    "\n",
    "for x, v in zip([0,1], [valid_share, ignore_share]):\n",
    "    plt.text(x, v + 0.005, f\"{v*100:.1f}%\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.ylim(0, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# In notebook (Python)\n",
    "from scripts.config import DataConfig, TrainConfig, AugmentConfig\n",
    "from scripts.train import train\n",
    "\n",
    "data_cfg = DataConfig(\n",
    "    data_root=\"../data\",\n",
    "    height=512, width=1024,\n",
    "    batch_size=2,\n",
    "    max_train_samples=100,\n",
    "    max_val_samples=100,\n",
    ")\n",
    "\n",
    "# Essai 1 : DeepLab ResNet50, augmentation légère\n",
    "aug_cfg = AugmentConfig(\n",
    "    enabled=False, hflip=True, vflip=False,\n",
    "    random_rotate_deg=3.0,\n",
    "    random_scale_min=0.85, random_scale_max=1.20,\n",
    "    random_crop=True,\n",
    "    brightness_delta=0.10, contrast_delta=0.10, saturation_delta=0.05, hue_delta=0.02,\n",
    "    gaussian_noise_std=0.00\n",
    ")\n",
    "train_cfg = TrainConfig(lr=3e-4, epochs=60, optimizer=\"adam\", exp_name=\"cityscapes-seg-8cls\")\n"
   ],
   "id": "a76257cdcbb6ea60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Contrôle visuel de la data augmentation\n",
    "\n",
    "La cellule suivante pioche quelques paires image/masque, applique le pipeline Albumentations configuré (\\`aug_cfg\\`) et affiche les versions redimensionnées vs augmentées pour vérifier que les masques restent alignés.\n"
   ],
   "id": "b3b9819cc887027b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not train_mode :\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "\n",
    "    from scripts.config import AugmentConfig\n",
    "    from scripts.augment import build_augment_fn\n",
    "    from scripts.remap import build_cityscapes_8cls_lut, remap_labels\n",
    "\n",
    "    lut = build_cityscapes_8cls_lut(data_cfg.ignore_index)\n",
    "    no_aug_fn = build_augment_fn(AugmentConfig(enabled=False), data_cfg.height, data_cfg.width, data_cfg.ignore_index)\n",
    "    augmented_fn = build_augment_fn(aug_cfg, data_cfg.height, data_cfg.width, data_cfg.ignore_index)\n",
    "\n",
    "    def remap_to_training_ids(mask_np):\n",
    "        mask_tf = tf.convert_to_tensor(mask_np, dtype=tf.int32)\n",
    "        return remap_labels(mask_tf, lut).numpy()\n",
    "\n",
    "    def colorize_mask(mask_np, palette=PALETTE_8, ignore_value=data_cfg.ignore_index):\n",
    "        rgb = np.zeros((mask_np.shape[0], mask_np.shape[1], 3), dtype=np.uint8)\n",
    "        for cls_id, color in palette.items():\n",
    "            rgb[mask_np == cls_id] = color\n",
    "        if ignore_value is not None:\n",
    "            rgb[mask_np == ignore_value] = (0, 0, 0)\n",
    "        return rgb\n",
    "\n",
    "    def overlay_mask(image_uint8, mask_uint8, alpha=0.45):\n",
    "        colored = colorize_mask(mask_uint8)\n",
    "        return np.clip((1.0 - alpha) * image_uint8 + alpha * colored, 0, 255).astype(np.uint8)\n",
    "\n",
    "    samples = pairs(\"train\")\n",
    "    assert samples, \"Aucun couple image/masque trouvé — vérifie le dossier data.\"\n",
    "\n",
    "    random.shuffle(samples)\n",
    "    num_rows = min(3, len(samples))\n",
    "    fig, axes = plt.subplots(num_rows, 6, figsize=(22, 5 * num_rows))\n",
    "    if num_rows == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "    for row, (left_path, lbl_path) in enumerate(samples[:num_rows]):\n",
    "        raw_img = np.array(Image.open(left_path).convert(\"RGB\"), dtype=np.float32) / 255.0\n",
    "        raw_mask = np.array(Image.open(lbl_path), dtype=np.int32)\n",
    "\n",
    "        mask8 = remap_to_training_ids(raw_mask)\n",
    "\n",
    "        img_tf = tf.convert_to_tensor(raw_img, dtype=tf.float32)\n",
    "        mask_tf = tf.convert_to_tensor(mask8, dtype=tf.int32)\n",
    "\n",
    "        base_img, base_mask = no_aug_fn(img_tf, mask_tf)\n",
    "        aug_img, aug_mask = augmented_fn(img_tf, mask_tf)\n",
    "\n",
    "        base_img_u8 = np.clip(base_img.numpy() * 255.0, 0, 255).astype(np.uint8)\n",
    "        aug_img_u8 = np.clip(aug_img.numpy() * 255.0, 0, 255).astype(np.uint8)\n",
    "        base_mask_u8 = base_mask.numpy().astype(np.uint8)\n",
    "        aug_mask_u8 = aug_mask.numpy().astype(np.uint8)\n",
    "\n",
    "        base_mask_rgb = colorize_mask(base_mask_u8)\n",
    "        aug_mask_rgb = colorize_mask(aug_mask_u8)\n",
    "\n",
    "        axes[row, 0].imshow(base_img_u8)\n",
    "        axes[row, 0].set_title(\"Image (resize)\")\n",
    "        axes[row, 1].imshow(base_mask_rgb)\n",
    "        axes[row, 1].set_title(\"Masque (resize)\")\n",
    "        axes[row, 2].imshow(overlay_mask(base_img_u8, base_mask_u8))\n",
    "        axes[row, 2].set_title(\"Overlay resize\")\n",
    "        axes[row, 3].imshow(aug_img_u8)\n",
    "        axes[row, 3].set_title(\"Image augmentée\")\n",
    "        axes[row, 4].imshow(aug_mask_rgb)\n",
    "        axes[row, 4].set_title(\"Masque augmenté\")\n",
    "        axes[row, 5].imshow(overlay_mask(aug_img_u8, aug_mask_u8))\n",
    "        axes[row, 5].set_title(\"Overlay augmentée\")\n",
    "\n",
    "        for ax in axes[row]:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "fae32854cfcd9ef9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "    from notebook.scripts.data import build_dataset\n",
    "\n",
    "    val_ds = build_dataset(\n",
    "        data_cfg,\n",
    "        AugmentConfig(enabled=False),\n",
    "        split=\"val\",\n",
    "        training=False,\n",
    "    )\n",
    "\n",
    "    images, masks, _ = next(iter(val_ds))\n",
    "    images_np = images.numpy()\n",
    "    masks_np = masks.numpy()\n",
    "\n",
    "    num_samples = min(3, images_np.shape[0])\n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image = images_np[i]\n",
    "        mask = masks_np[i]\n",
    "\n",
    "        if image.dtype != np.uint8:\n",
    "            image_u8 = np.clip(image * 255.0, 0, 255).astype(np.uint8)\n",
    "        else:\n",
    "            image_u8 = image\n",
    "\n",
    "        mask_u8 = mask.astype(np.uint8)\n",
    "        mask_rgb = colorize_mask(mask_u8)\n",
    "        overlay_rgb = overlay_mask(image_u8, mask_u8)\n",
    "\n",
    "        overlay_on_black = overlay_mask(np.zeros_like(image_u8), mask_u8, alpha=1.0)\n",
    "        assert np.array_equal(overlay_on_black, mask_rgb), \"Overlay misaligned with mask (check dataset pipeline).\"\n",
    "\n",
    "        for j, (img, title) in enumerate([\n",
    "            (image_u8, \"Image (val)\"),\n",
    "            (mask_rgb, \"Masque colorisé\"),\n",
    "            (overlay_rgb, \"Overlay\"),\n",
    "        ]):\n",
    "            ax = plt.subplot(num_samples, 3, i * 3 + j + 1)\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(f\"Échantillon {i + 1} — {title}\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "44c2b5ff61d6ffcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### UNet Mini\n",
    "Modèle U-Net compact utilisé comme référence rapide pour valider le pipeline d'entraînement.\n",
    "Sa légèreté le rend adapté aux tests itératifs sur Cityscapes réduit.\n"
   ],
   "id": "b54c83afdf449ad0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    from dataclasses import replace\n",
    "    unet_mini_cfg = replace(\n",
    "        train_cfg,\n",
    "        output_dir=\"artifacts/unet_mini\",\n",
    "    )\n",
    "    train(\"unet_mini\", data_cfg, unet_mini_cfg, aug_cfg)\n"
   ],
   "id": "f08d845ee14a58e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### UNet VGG16\n",
    "Architecture U-Net à encodeur VGG16 offrant une capacité accrue pour capturer des détails fins.\n",
    "Elle reste compatible avec notre pipeline et sert de baseline plus profonde.\n",
    "\n",
    "/!\\ Le décodeur à été réduit sur plusieurs couches par soucis de consommation de vram\n"
   ],
   "id": "625be39c1c74f8c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    unet_vgg16_cfg = replace(\n",
    "        train_cfg,\n",
    "        output_dir=\"artifacts/unet_vgg16\",\n",
    "    )\n",
    "    train(\"unet_vgg16\", data_cfg, unet_vgg16_cfg, aug_cfg)\n"
   ],
   "id": "968a91d468deb367",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### MobileDet Seg\n",
    "Variante segmentation de MobileDet pensée pour l'inférence embarquée tout en conservant une précision correcte.\n",
    "Ce modèle illustre un compromis agressif entre vitesse et qualité.\n"
   ],
   "id": "97eac04f2c1a0511"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    mobiledet_seg_cfg = replace(\n",
    "        train_cfg,\n",
    "        output_dir=\"artifacts/mobiledet_seg\",\n",
    "    )\n",
    "    train(\"mobiledet_seg\", data_cfg, mobiledet_seg_cfg, aug_cfg)\n"
   ],
   "id": "17cc0800dcd2447b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### YOLOv9 Seg\n",
    "Déclinaison segmentation de YOLOv9 visant une extraction simultanée des instances et des masques.\n",
    "Nous l'évaluons pour mesurer le gain potentiel des architectures one-stage.\n"
   ],
   "id": "dda9ed5cbc57b167"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    yolov9_seg_cfg = replace(\n",
    "        train_cfg,\n",
    "        output_dir=\"artifacts/yolov9_seg\",\n",
    "    )\n",
    "    train(\"yolov9_seg\", data_cfg, yolov9_seg_cfg, aug_cfg)\n"
   ],
   "id": "2bfc844613848b5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## deeplab resnet50",
   "id": "e73d2a8040472e9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # Entraînement\n",
    "    train(\"deeplab_resnet50\", data_cfg, train_cfg, aug_cfg)"
   ],
   "id": "c919827d0f66cddd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 🧩 Vue d’ensemble des résultats\n",
    "\n",
    "| Modèle                     |   Durée  | `masked_mIoU` (train) | `val_masked_mIoU` | `pix_acc` | `val_pix_acc` | `dice_coef` | `val_dice_coef` |\n",
    "| :------------------------- | :------: | :-------------------: | :---------------: | :-------: | :-----------: | :---------: | :-------------: |\n",
    "| **DeepLabV3+ (ResNet50)**  | 13.4 min |       **0.947**       |     **0.639**     | **0.989** |   **0.872**   |  **0.965**  |    **0.716**    |\n",
    "| **YOLOv9_seg (simplifié)** | 10.5 min |         0.689         |       0.400       |   0.913   |     0.714     |    0.753    |      0.494      |\n",
    "| **MobileDet_seg**          | 16.3 min |         0.938         |       0.502       |   0.987   |     0.779     |    0.953    |      0.600      |\n",
    "| **U-Net VGG16**            | 29.7 min |         0.903         |       0.542       |   0.977   |     0.805     |    0.923    |      0.633      |\n",
    "| **U-Net mini**             |  6.1 min |         0.563         |       0.319       |   0.851   |     0.634     |    0.650    |      0.407      |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Interprétation métrique par métrique\n",
    "\n",
    "### 🟦 `masked_mIoU` (train)\n",
    "\n",
    "* Mesure principale de segmentation (intersection sur union moyenne).\n",
    "* Tous sauf U-Net mini > 0.9 en entraînement → bon apprentissage.\n",
    "* U-Net mini (0.56) : trop léger, manque de capacité.\n",
    "\n",
    "### 🟧 `val_masked_mIoU`\n",
    "\n",
    "* Évalue la **généralisation**.\n",
    "* DeepLab (0.639) est **nettement supérieur** aux autres.\n",
    "* U-Net VGG16 (0.54) et MobileDet (0.50) suivent derrière.\n",
    "* YOLOv9 seg (0.40) et U-Net mini (0.32) décrochent clairement.\n",
    "\n",
    "### 🟩 `val_pix_acc`\n",
    "\n",
    "* Corrélation assez bonne avec `val_mIoU`.\n",
    "* DeepLab atteint 0.87 → très bonne segmentation globale.\n",
    "* U-Net VGG16 ≈ 0.80 → correct.\n",
    "* Les autres chutent < 0.78.\n",
    "\n",
    "### 🟪 `val_dice_coef`\n",
    "\n",
    "* Très proche du mIoU mais plus sensible aux petits objets.\n",
    "* DeepLab ≈ 0.72 → cohérent avec sa bonne mIoU.\n",
    "* U-Net VGG16 ≈ 0.63 et MobileDet ≈ 0.60 → acceptables.\n",
    "* YOLOv9 ≈ 0.49, U-Net mini ≈ 0.40 → faibles.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚖️ Analyse comparative\n",
    "\n",
    "| Critère                               | Meilleur modèle                            |\n",
    "| :------------------------------------ | :----------------------------------------- |\n",
    "| **Précision globale (mIoU/Dice)**     | 🟢 **DeepLabV3+ ResNet50**                 |\n",
    "| **Généralisation / stabilité val**    | 🟢 **DeepLabV3+ ResNet50**                 |\n",
    "| **Compromis vitesse/qualité**         | 🟢 **MobileDet_seg** (plus léger, correct) |\n",
    "| **Performance brute (haute qualité)** | 🟢 **U-Net VGG16** si VRAM suffisante      |\n",
    "| **Légereté / prototypage rapide**     | 🟢 **U-Net mini**, mais précision faible   |\n",
    "\n",
    "---\n",
    "\n",
    "## Interprétation détaillée\n",
    "\n",
    "### 🥇 **DeepLabV3+ (ResNet50)**\n",
    "\n",
    "* **Meilleur équilibre** entre précision et stabilité.\n",
    "* mIoU = 0.64 (val) et Dice = 0.72 (val) : excellents scores sur 8 classes.\n",
    "* Surapprentissage modéré (train-val gap raisonnable).\n",
    "* Très bonne capacité à capter les contours fins et la hiérarchie spatiale.\n",
    "  ✅ **→ Modèle à garder comme référence.**\n",
    "\n",
    "### 🥈 **U-Net VGG16**\n",
    "\n",
    "* Très bon entraînement, mais écart train-val > 0.35 : léger overfit.\n",
    "* Lourdeur mémoire (VGG16) mais résultats solides.\n",
    "  ✅ Alternative si tu veux plus de stabilité visuelle (textures fines).\n",
    "\n",
    "### 🥉 **MobileDet_seg**\n",
    "\n",
    "* Performances correctes pour un modèle “mobile-like”.\n",
    "* Bonne efficacité (seulement 16 min d’entraînement, résultats décents).\n",
    "  🟡 Bon compromis si tu cibles l’inférence embarquée.\n",
    "\n",
    "### ⚙️ **YOLOv9_seg**\n",
    "\n",
    "* Correct mais sous-optimal : architecture pas parfaitement adaptée à la segmentation dense.\n",
    "* Val mIoU = 0.40, Dice = 0.49 : pas suffisant pour une segmentation de qualité.\n",
    "  🔴 À éviter pour cette tâche spécifique.\n",
    "\n",
    "### ⚪ **U-Net mini**\n",
    "\n",
    "* Très rapide mais sous-entraîné / sous-dimensionné.\n",
    "* Mauvais scores val (mIoU = 0.32, Dice = 0.40).\n",
    "  🔴 Bon pour tests rapides, pas pour production.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧾 Conclusion\n",
    "\n",
    "| Rang | Modèle                    | Pourquoi                                                  |\n",
    "| :--: | :------------------------ | :-------------------------------------------------------- |\n",
    "|  🥇  | **DeepLabV3+ (ResNet50)** | Meilleur équilibre précision / généralisation / stabilité |\n",
    "|  🥈  | **U-Net VGG16**           | Très bon mais plus lourd, tendance à overfitter           |\n",
    "|  🥉  | **MobileDet_seg**         | Légèreté et vitesse, mais précision un cran en dessous    |\n",
    "|   4  | **YOLOv9_seg**            | Pas adapté à la segmentation dense                        |\n",
    "|   5  | **U-Net mini**            | Trop limité, résultats faibles                            |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 En résumé\n",
    "\n",
    "> **DeepLabV3+ ResNet50** est le **meilleur modèle global** :\n",
    ">\n",
    "> * meilleures métriques de validation,\n",
    "> * bon Dice et mIoU,\n",
    "> * rapport vitesse/qualité très favorable,\n",
    "> * faible overfit comparé à VGG16.\n"
   ],
   "id": "154c65411f171045"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Entraînement final DeepLabV3+ (ResNet50) sur l'intégralité du dataset\n\nNous relançons DeepLabV3+ avec **toutes** les images `train/val` de Cityscapes (plus de limite `max_*_samples`).\nLes artefacts (best/final) seront exportés dans `artifacts/deeplab_resnet50_full` et suivis dans **MLflow** (`artifacts/mlruns`)."
   ],
   "id": "f224bfb380ca5bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration finale : dataset complet + sortie dédiée\nfinal_data_cfg = replace(\n    data_cfg,\n    max_train_samples=None,\n    max_val_samples=None,\n)\nfinal_train_cfg = replace(\n    train_cfg,\n    output_dir=\"artifacts/deeplab_resnet50_full\",\n    exp_name=\"cityscapes-seg-8cls-full\",\n)\nfinal_aug_cfg = replace(\n    aug_cfg,\n    enabled=True,\n)\n\n# Lancement de l'entraînement complet (sauvegarde locale + tracking MLflow)\ntrain(\"deeplab_resnet50\", final_data_cfg, final_train_cfg, final_aug_cfg)"
   ],
   "id": "1828657bb72691a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copie explicite du meilleur modèle pour l'API\nfrom pathlib import Path\nimport shutil\n\nbest_model = Path(\"artifacts/deeplab_resnet50_full/deeplab_resnet50_best.keras\")\nfinal_model = Path(\"artifacts/deeplab_resnet50_full/deeplab_resnet50_final.keras\")\napi_export = Path(\"artifacts/api/deeplabv3plus_resnet50_full.keras\")\napi_export.parent.mkdir(parents=True, exist_ok=True)\n\nif best_model.exists():\n    shutil.copy2(best_model, api_export)\n    print(f\"✅ Modèle API (best) : {api_export}\")\nelif final_model.exists():\n    shutil.copy2(final_model, api_export)\n    print(f\"⚠️ Best absent, export du modèle final : {api_export}\")\nelse:\n    raise FileNotFoundError(\"Aucun modèle entraîné trouvé. Lance d'abord la cellule d'entraînement.\")"
   ],
   "id": "73c6a586d43eb52b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
