# Documentation technique du projet de segmentation urbaine

## PrÃ©ambule

Ce document prÃ©sente une vue d'ensemble technique complÃ¨te du projet de segmentation sÃ©mantique dÃ©veloppÃ© dans le cadre de la formation AI Engineer ProjectÂ 8. Il synthÃ©tise les choix d'architecture, les stratÃ©gies d'entraÃ®nement, les rÃ©sultats expÃ©rimentaux et la mise Ã  disposition d'une API Flask pour l'infÃ©rence et l'aperÃ§u des augmentations. La structure suit les axes demandÃ©sÂ :

1. **Expliquer les modÃ¨les Ã©tudiÃ©s**Â : description des architectures, du pipeline de donnÃ©es et des paramÃ¨tres d'entraÃ®nement utilisÃ©s lors de l'exploration.
2. **PrÃ©senter le modÃ¨le retenu**Â : justification dÃ©taillÃ©e de la sÃ©lection de DeepLabV3+ avec backbone ResNet50, en lien avec les contraintes fonctionnelles et les mÃ©triques.
3. **Afficher le benchmarking des rÃ©sultats**Â : comparaison quantitative et qualitative des candidats Ã  partir des expÃ©riences menÃ©es dans le notebook de recherche.
4. **DerniÃ¨re partie sur l'API avec conclusion ouverte**Â : fonctionnement du service d'infÃ©rence, exposition des endpoints REST et perspectives d'Ã©volution.

L'ensemble du texte est rÃ©digÃ© en franÃ§ais et vise un niveau de dÃ©tail Ã©quivalent Ã  environ sept pages de documentation technique.


## 1. Dataset Cityscapes et stratÃ©gie d'augmentation

### 1.1. PrÃ©sentation du dataset

Le dataset **Cityscapes** est une rÃ©fÃ©rence pour la segmentation sÃ©mantique urbaine. Il contient 5Â 000 scÃ¨nes finement annotÃ©es (2Â 975 pour l'entraÃ®nement, 500 pour la validation, 1Â 525 pour le test) capturÃ©es depuis une camÃ©ra embarquÃ©e dans 50Â villes europÃ©ennes. Chaque image RGB, au format 1024Ã—2048, est accompagnÃ©e d'un masque pixel Ã  pixel couvrant 30Â classes originales. Pour ce projet, nous utilisons la version remappÃ©e officielle Â«Â eight classesÂ Â» qui regroupe les catÃ©gories en huit super-classes pertinentes pour la conduite autonome lÃ©gÃ¨reÂ : route, trottoir, bÃ¢timent, feu de circulation, vÃ©gÃ©tation, ciel, personne et vÃ©hicule. Ce remapping limite la confusion entre classes proches, rÃ©duit la complexitÃ© du modÃ¨le et accÃ©lÃ¨re l'entraÃ®nement.

Les annotations Ã©tant rÃ©alisÃ©es Ã  la main, elles intÃ¨grent une valeur spÃ©ciale 255 (`ignore_index`) pour les pixels ambigus (occlusions, reflets). Ces zones ne sont ni prises en compte dans la loss ni dans les mÃ©triques, ce qui Ã©vite de pÃ©naliser artificiellement les prÃ©dictions.

### 1.2. PrÃ©paration et split des donnÃ©es

Les images sont d'abord converties en PNG 16 bits sans perte puis harmonisÃ©es en 512Ã—1024 pour Ã©quilibrer la consommation mÃ©moire et la fidÃ©litÃ© spatiale. La pipeline `build_dataset` se charge deÂ :

- lire les paires image/masque depuis le disque en utilisant `tf.data.Dataset.list_files` avec un shuffle dÃ©terministe pour garantir la reproductibilitÃ©Â ;
- appliquer la table de remapping (`REMAPPING_DICT`) sur les masques bruts afin de condenser les 30 classes en huit valeurs entiÃ¨resÂ ;
- gÃ©nÃ©rer un masque de poids binaire (`valid_mask`) marquant les pixels valides pour la lossÂ ;
- rÃ©partir les Ã©chantillons selon un split **80Â % / 10Â % / 10Â %** (train/val/test) afin de conserver un lot d'images pour l'Ã©valuation finale hors entraÃ®nement.

### 1.3. Motivation et dÃ©tails des augmentations

Pour reproduire fidÃ¨lement le protocole de **Bhuiya et al. (2022)**, nous avons remplacÃ© la chaÃ®ne d'effets cumulÃ©s par un sÃ©lecteur Ã  choix unique. Ã€ chaque passage, Albumentations applique exactement **une** transformation tirÃ©e uniformÃ©ment parmi les quinze corruptions dÃ©crites dans l'articleÂ : `Blur`, `GaussianBlur`, `GlassBlur`, `MotionBlur`, `CLAHE`, `Equalize`, `ColorJitter`, `HueSaturationValue`, `Posterize`, `ISONoise`, `OpticalDistortion`, `RandomRain`, `RandomFog`, `RandomSnow` et `RandomSunflare`. La probabilitÃ© globale est fixÃ©e Ã  `p = 1.0`, garantissant qu'une corruption est toujours utilisÃ©e.

Chaque effet est paramÃ©trÃ© avec des intensitÃ©s Â«Â modÃ©rÃ©esÂ Â» adaptÃ©es Ã  une camÃ©ra embarquÃ©e (sigma de flou limitÃ© Ã  1,5, bruit ISO plafonnÃ© Ã  0,2, coefficients de brume et d'Ã©blouissement restreints). Les transformations respectent strictement la gÃ©omÃ©trie d'origineÂ : aucun recadrage, zoom ni flip n'est appliquÃ© et Albumentations travaille en mode `mask` pour propager exactement la mÃªme opÃ©ration au masque de segmentation via un `A.OneOf` encapsulÃ© dans `A.Compose`.

Ce fonctionnement rend l'Ã©valuation beaucoup plus lisibleÂ : on sait quelle corruption a Ã©tÃ© utilisÃ©e, on peut relancer un lot pour Ã©chantillonner un autre effet et l'on limite les interactions entre transformations. L'API Python expose l'usine Ã  corruption (`NamedTransformSpec`) de sorte que la pipeline d'entraÃ®nement, le service Flask et le notebook de recherche partagent la mÃªme dÃ©finition.

Albumentations reste la bibliothÃ¨que privilÃ©giÃ©e face Ã  **imgaug** ou `tf.image`, toujours pour les mÃªmes raisonsÂ : API dÃ©clarative, support natif des masques et performances. Le fait de n'activer qu'un seul opÃ©rateur par appel renforce encore la reproductibilitÃ© et facilite la comparaison directe avec les rÃ©sultats rapportÃ©s par Bhuiya et al.

### 1.4. Impact mesurÃ© des augmentations

En reproduisant les corruptions isolÃ©es de Bhuiya et al., nous observons un gain moyen de +2,5 points de `val_masked_mIoU` par rapport au jeu de donnÃ©es brut, et surtout une variance rÃ©duite entre runs. Chaque transformation peut Ãªtre Ã©valuÃ©e individuellement dans le notebook comme dans l'interface Flask, ce qui a permis d'identifier les effets bÃ©nÃ©fiques (brumes lÃ©gÃ¨res, ajustements de couleur) et ceux Ã  surveiller (distorsions optiques trop fortes sur les masques fins). L'absence d'opÃ©rations gÃ©omÃ©triques agressives garantit que les contours critiques (trottoirs, vÃ©hicules) restent alignÃ©s, tandis que les perturbations photomÃ©triques couvrent les scÃ©narios mÃ©tÃ©o rÃ©ellement rencontrÃ©s par la flotte de test.

---

## 2. ModÃ¨les Ã©tudiÃ©s

### 2.1. Contexte expÃ©rimental et pipeline commun

Les expÃ©rimentations ont Ã©tÃ© menÃ©es sur une version remappÃ©e du dataset Cityscapes, rÃ©duite Ã  huit classes principales pour simplifier l'entraÃ®nement et l'infÃ©rence temps rÃ©el. La configuration des donnÃ©es (`DataConfig`) impose des images RGB redimensionnÃ©es Ã  **512Ã—1024 pixels**, avec un lot (`batch_size`) de deux Ã©chantillons pour Ã©quilibrer l'utilisation mÃ©moire et la stabilitÃ© des gradients. Les labels bruts Cityscapes (`_gtFine_labelIds.png`) sont remappÃ©s sur l'intervalle \[0Â ;Â 7\], avec la valeur 255 conservÃ©e comme *ignore index* pour exclure les pixels indÃ©cidables durant l'entraÃ®nement.

Le pipeline de donnÃ©es (`build_dataset`) s'appuie sur `tf.data` pour la lecture, le prÃ©traitement et la prÃ©servation d'un masque de poids par pixel. Ce masque est ensuite utilisÃ© pour pondÃ©rer les pertes lorsque des pixels ignorÃ©s traversent la chaÃ®ne de calcul. Les Ã©tapes principales sontÂ :

- DÃ©codage PNG et conversion en float32 normalisÃ© \[0Â ;Â 1\].
- Remapping des classes via une table de correspondance dÃ©diÃ©e.
- Application d'un pipeline d'augmentations Albumentations harmonisÃ© pour les images et les masques.
- Redimensionnement final vers la rÃ©solution cible avec interpolation bilinÃ©aire (images) et *nearest neighbor* (masques).
- Batching et prÃ©chargement asynchrone (`prefetch`) pour alimenter efficacement le GPU.

La chaÃ®ne d'augmentation applique une seule corruption photomÃ©trique/mÃ©tÃ©o Ã  la fois via `A.OneOf`. Les quinze effets issus de Bhuiya et al. reproduisent des perturbations rÃ©alistes (flous, bruit de capteur, intempÃ©ries synthÃ©tiques, distorsion optique lÃ©gÃ¨re) sans toucher Ã  la gÃ©omÃ©trie. Chaque transformation est configurÃ©e pour laisser les masques intacts hormis l'option `OpticalDistortion`, qui met Ã  jour la segmentation avec une interpolation nearest-neighbor cohÃ©rente.

Le script d'entraÃ®nement (`train.py`) compile chaque modÃ¨le avec une *loss* principale en entropie croisÃ©e (`SparseCategoricalCrossentropy` Ã  rÃ©duction `NONE`) pondÃ©rÃ©e par le masque de validitÃ©, et ajoute au besoin une composante de Dice loss. Trois mÃ©triques sont suivies sur train et validationÂ : **masked pixel accuracy**, **masked mean IoU** et **Dice coefficient**.

### 2.2. Catalogue des architectures Ã©valuÃ©es

Les architectures suivantes ont Ã©tÃ© implÃ©mentÃ©es dans `notebook/scripts/models.py` et Ã©valuÃ©es sur le mÃªme pipelineÂ :

#### U-Net mini

- **Objectif**Â : servir de base lÃ©gÃ¨re pour valider le pipeline de donnÃ©es et la configuration d'entraÃ®nement avant de lancer des expÃ©riences coÃ»teuses.
- **Architecture**Â : encodeur Ã  deux niveaux (strides Ã—2 successifs) aboutissant Ã  un goulot d'Ã©tranglement 1/4 de la rÃ©solution d'entrÃ©e. Chaque Ã©tage applique une double convolution 3Ã—3 suivie de `BatchNormalization` et ReLU, puis transmet ses cartes au dÃ©codeur via des *skip connections* directes.
- **Fonctionnement**Â : le dÃ©codeur reconstruit progressivement la rÃ©solution grÃ¢ce Ã  des upsamplings bilinÃ©aires couplÃ©s Ã  des convolutions 3Ã—3, ce qui permet de fusionner les caractÃ©ristiques basse et haute frÃ©quence.
- **CapacitÃ©**Â : 24 filtres de base, soit moins de 1Â million de paramÃ¨tres. Cette compacitÃ© facilite l'entraÃ®nement sur CPU, mais limite la richesse des reprÃ©sentations.
- **Analyse des rÃ©sultats**Â : l'architecture capture correctement les surfaces Ã©tendues (route, ciel) mais Ã©choue sur les objets fins (piÃ©tons, feux). La faible profondeur rÃ©duit la capacitÃ© Ã  agrÃ©ger du contexte global, expliquant le `val_masked_mIoU` plafonnÃ© Ã  â‰ˆÂ 0,32. Ce modÃ¨le reste utile pour dÃ©boguer rapidement des changements dans le pipeline.

#### U-Net small

- **Objectif**Â : explorer un compromis entre la version mini et les architectures plus lourdes, avec davantage de niveaux hiÃ©rarchiques.
- **SpÃ©cificitÃ©s**Â : trois niveaux d'encodage/dÃ©codage et un goulot d'Ã©tranglement Ã  1/8 de la rÃ©solution d'entrÃ©e. Chaque bloc suit le motif `Convâ†’BNâ†’ReLU` rÃ©pÃ©tÃ© deux fois et intÃ¨gre un *dropout* lÃ©ger (0,2) pour limiter le surapprentissage.
- **Fonctionnement**Â : les *skip connections* permettent de fusionner les textures fines (bordures trottoir/route) avec le contexte global obtenu au fond du rÃ©seau.
- **Analyse des rÃ©sultats**Â : le modÃ¨le atteint un `val_masked_mIoU` d'environ 0,46 lors des runs exploratoires. Il dÃ©montre que l'ajout d'un niveau hiÃ©rarchique amÃ©liore la segmentation des trottoirs et bÃ¢timents, mais reste en retrait face Ã  U-Net VGG16 qui profite d'un backbone prÃ©-entraÃ®nÃ©. Faute de temps de calcul, cette variante n'a pas Ã©tÃ© re-testÃ©e dans la campagne finale.

#### U-Net VGG16

- **Backbone**Â : rÃ©utilise les couches convolutionnelles de VGG16 prÃ©-entraÃ®nÃ© sur ImageNet, ce qui fournit d'emblÃ©e des dÃ©tecteurs de motifs universels (angles, textures, motifs rÃ©pÃ©titifs).
- **DÃ©coder**Â : quatre blocs `decoder_block` assurent la remontÃ©e en rÃ©solution. Chaque bloc combine un upsampling bilinÃ©aire, une concatÃ©nation avec l'activation correspondante du backbone et deux convolutions 3Ã—3 pour raffiner le signal. Le `double_conv_block` final ajuste la carte Ã  huit canaux (classes) avant la projection softmax.
- **Fonctionnement**Â : le prÃ©-entraÃ®nement permet au modÃ¨le de converger plus vite et de mieux segmenter les structures complexes (faÃ§ades, fenÃªtres, mobilier urbain). La profondeur (23 couches convolutionnelles) capture un large contexte spatial, bÃ©nÃ©fique pour diffÃ©rencier les bÃ¢timents du ciel mÃªme par temps couvert.
- **Analyse des rÃ©sultats**Â : malgrÃ© une bonne qualitÃ© visuelle, l'Ã©cart entre `train_masked_mIoU` (â‰ˆÂ 0,90) et validation (â‰ˆÂ 0,54) montre une tendance Ã  l'overfit lorsque les augmentations sont rÃ©duites. Les 14,7Â millions de paramÃ¨tres impliquent aussi un temps d'entraÃ®nement long (~30Â min/run) et un besoin VRAM plus Ã©levÃ©. Ces contraintes expliquent sa non-sÃ©lection pour la production, mÃªme si le modÃ¨le constitue une rÃ©fÃ©rence qualitative.

#### MobileDet_seg

- **Inspiration**Â : dÃ©clinaison segmentation d'un backbone MobileNetV2, pensÃ©e pour des environnements embarquÃ©s (vÃ©hicule, robot).
- **Design**Â : reprend la structure Ã  blocs inversÃ©s (`inverted residual`) avec convolutions depthwise separables. Trois *skip connections* (`block_1`, `block_3`, `block_6`) alimentent un dÃ©codeur lÃ©ger composÃ© de `separable_conv_block` et d'upsampling bilinÃ©aire.
- **Fonctionnement**Â : la factorisation depthwise + pointwise rÃ©duit drastiquement le nombre de multiplications, ce qui rend l'infÃ©rence fluide sur CPU tout en capturant les structures principales. Le dÃ©codeur privilÃ©gie la conservation des contours via des convolutions 3Ã—3 suivies de `BatchNormalization`.
- **Analyse des rÃ©sultats**Â : le modÃ¨le atteint ~0,50 de `val_masked_mIoU`. Il segmente efficacement les surfaces continues (route, bÃ¢timents) mais perd en prÃ©cision sur les objets fins (personnes, feux) car la profondeur rÃ©duite limite l'agrÃ©gation de contexte. Toutefois, son temps d'entraÃ®nement (â‰ˆ16Â min) et sa rapiditÃ© d'infÃ©rence en font un candidat sÃ©rieux pour une version mobile ou temps rÃ©el.

#### YOLOv9_seg (simplifiÃ©)

- **Origine**Â : dÃ©rivÃ© d'une branche YOLO orientÃ©e dÃ©tection, modifiÃ© pour produire des cartes de segmentation multi-Ã©chelles.
- **Structure**Â : empilement de blocs rÃ©siduels CSP suivis d'une tÃªte PANet allÃ©gÃ©e. Le modÃ¨le fusionne trois rÃ©solutions diffÃ©rentes avant une projection 1Ã—1 vers les classes.
- **Fonctionnement**Â : la philosophie YOLO privilÃ©gie la dÃ©tection d'objets discrets avec ancrages. La conversion en segmentation dense nÃ©cessite une Ã©tape d'upsampling et de fusion qui reste moins expressive qu'un dÃ©codeur dÃ©diÃ©.
- **Analyse des rÃ©sultats**Â : bien que le temps d'entraÃ®nement soit court (~10,5Â min), le `val_masked_mIoU` plafonne Ã  0,40. Les masques produits sont corrects pour les vÃ©hicules mais bruyants sur les trottoirs et bÃ¢timents. Cette contre-performance provient de la perte d'information spatiale due aux strides importants en dÃ©but de rÃ©seau et Ã  l'absence de *skip connections* riches.

#### DeepLabV3+ (ResNet50)

- **Backbone**Â : ResNet50 prÃ©-entraÃ®nÃ© ImageNet, tronquÃ© aprÃ¨s `conv4_block6_out` (stride effectif 16). Les blocs rÃ©siduels profonds permettent de capter des patrons contextuels Ã  large champ rÃ©ceptif tout en restant optimisÃ©s pour les GPU.
- **TÃªte ASPP**Â : l'*Atrous Spatial Pyramid Pooling* combine quatre branches (dilatations 1, 6, 12, 18) et une branche de pooling global. Cette structure capture simultanÃ©ment des motifs fins et des structures larges (carrefours, intersections) sans rÃ©duire la rÃ©solution spatiale.
- **DÃ©codeur**Â : une projection 1Ã—1 ramÃ¨ne les caractÃ©ristiques basses (`conv2_block3_out`) Ã  48 canaux, concatÃ©nÃ©es avec la sortie ASPP puis filtrÃ©es par deux convolutions sÃ©parables 3Ã—3. L'upsampling final Ã—4 reconstitue une carte quasi pleine rÃ©solution.
- **Fonctionnement**Â : l'association ASPP + *skip* basse rÃ©solution permet de segmenter finement les contours (personnes, poteaux) tout en conservant une vision globale de la scÃ¨ne. Le modÃ¨le reste stable grÃ¢ce aux rÃ©sidus et Ã  la normalisation Batch.
- **Analyse des rÃ©sultats**Â : DeepLabV3+ domine les mÃ©triques avec 0,639 de `val_masked_mIoU`. Les logs montrent une convergence rÃ©guliÃ¨re, un Ã©cart train/val maÃ®trisÃ© et des masques cohÃ©rents visuellement. Les erreurs restantes concernent surtout les classes rares oÃ¹ les donnÃ©es restent limitÃ©es, mais elles sont moins prononcÃ©es que sur les autres architectures.

#### Fast-SCNN (prototype)

- **Principe**Â : architecture temps rÃ©el reposant sur une branche de *learning global* (convolutions dilatÃ©es) et une branche dÃ©taillÃ©e (convolutions 3Ã—3 classiques), fusionnÃ©es par addition.
- **Fonctionnement**Â : les convolutions depthwise separables et le *feature fusion module* limitent drastiquement le nombre d'opÃ©rations. Un mini-ASPP (dilatations 2 et 4) capture le contexte global.
- **Statut**Â : faute de budget GPU, l'entraÃ®nement complet (300Â Ã©poques) n'a pas Ã©tÃ© conduit. Les tests prÃ©liminaires sur 50Â Ã©poques donnaient un `val_masked_mIoU` autour de 0,45, suggÃ©rant un potentiel intÃ©ressant pour une version embarquÃ©e.

Dans la suite, les benchmarks se concentrent sur les cinq architectures Ã©valuÃ©es quantitativement dans le notebookÂ : DeepLabV3+Â (ResNet50), U-NetÂ VGG16, MobileDet_seg, YOLOv9_seg simplifiÃ© et U-Net mini.

---

## 3. ModÃ¨le retenuÂ : DeepLabV3+ avec backbone ResNet50

### 3.1. CritÃ¨res de sÃ©lection

Le choix final s'est portÃ© sur **DeepLabV3+** car il offre le meilleur Ã©quilibre entre prÃ©cision, robustesse et temps de calcul sur la cible matÃ©rielle visÃ©e (GPU moyen de gamme pour l'entraÃ®nement, CPU/GPU modeste pour l'infÃ©rence). Les critÃ¨res retenusÂ :

1. **QualitÃ© de segmentation**Â : scores de `val_masked_mIoU` et `val_dice_coef` nettement supÃ©rieurs aux autres architectures.
2. **StabilitÃ© du training**Â : convergence rÃ©guliÃ¨re sans oscillations, Ã©cart contrÃ´lÃ© entre train et validation.
3. **CapacitÃ© Ã  gÃ©nÃ©raliser**Â : peu sensible au surapprentissage malgrÃ© une forte capacitÃ©, grÃ¢ce Ã  l'ASPP et au *skip* basse rÃ©solution.
4. **CompatibilitÃ© production**Â : modÃ¨le supportÃ© nativement par TensorFlow/Keras, facilement sÃ©rialisable en `.keras` et optimisable via TensorRT/TF-Lite si nÃ©cessaire.

### 3.2. HyperparamÃ¨tres et pipeline d'entraÃ®nement

Les expÃ©riences finales sur DeepLabV3+ utilisent les paramÃ¨tres par dÃ©faut du `TrainConfig`, ajustÃ©s ponctuellementÂ :

- **Optimiseur**Â : SGD avec momentum 0,9 et Nesterov, scheduler polynomial (`poly_power`Â =Â 0,9) pour dÃ©croÃ®tre le taux d'apprentissage sur toute la durÃ©e de l'entraÃ®nement.
- **Taux d'apprentissage initial**Â : 1e-2, adaptÃ© automatiquement par la dÃ©croissance polynomiale en fonction du nombre total d'itÃ©rations (`decay_steps = epochs Ã— steps_per_epoch`).
- **Nombre d'Ã©poques**Â : 200 avec *early stopping* (patienceÂ 10) sur la mÃ©trique `val_masked_mIoU` afin d'Ã©viter d'entraÃ®ner au-delÃ  du plateau de validation.
- **Politique de prÃ©cision**Â : `float32` par dÃ©faut, mais la configuration supporte `mixed_float16` en production pour accÃ©lÃ©rer l'infÃ©rence.
- **Perte**Â : entropie croisÃ©e catÃ©gorique pondÃ©rÃ©e par le masque, assurant que les pixels marquÃ©s `ignore_index` n'influencent ni la loss ni les gradients.
- **Suivi**Â : intÃ©gration MLflow (`KerasMlflowLogger`) pour historiser hyperparamÃ¨tres, mÃ©triques et artefacts (checkpoints, CSV des logs d'entraÃ®nement).

Le pipeline de donnÃ©es assure un mÃ©lange (`shuffle`) Ã  chaque Ã©poque avec une graine fixe pour la reproductibilitÃ©. Les augmentations ont Ã©tÃ© laissÃ©es actives sur l'entraÃ®nement finalÂ : chaque lot subit exactement une corruption Bhuiya (flou, bruit, mÃ©tÃ©o synthÃ©tique, distorsion) choisie alÃ©atoirement pour couvrir les variations photomÃ©triques rÃ©alistes rencontrÃ©es sur route.

### 3.3. Artefacts gÃ©nÃ©rÃ©s et livraison du modÃ¨le

L'entraÃ®nement final exporte deux artefacts principauxÂ :

- `deeplab_resnet50_final.keras`Â : modÃ¨le complet (poids + architecture) stockÃ© dans `app/models/` pour l'infÃ©rence.
- Checkpoints nommÃ©s selon le motif `{arch}.{monitor}.{epoch}-{score}.keras` dans `artifacts/checkpoints/`, permettant de restaurer la meilleure Ã©poque selon `val_masked_mIoU`.

En production, l'application Flask charge `deeplab_resnet50_final.keras` lors du dÃ©marrage, ce qui garantit que l'API exploite la meilleure itÃ©ration identifiÃ©e pendant l'entraÃ®nement.

### 3.4. Analyse qualitative

Au-delÃ  des mÃ©triques, les observations qualitatives montrent que DeepLabV3+Â :

- Conserve les bordures nettes entre route et trottoirs.
- Identifie correctement les vÃ©hicules et piÃ©tons malgrÃ© des occlusions partielles.
- GÃ¨re mieux le ciel et la vÃ©gÃ©tation que les modÃ¨les plus lÃ©gers, en limitant les confusions entre classes adjacentes.

Cette supÃ©rioritÃ© visuelle dÃ©coule directement de l'ASPP, qui capture un contexte multi-Ã©chelle, et du dÃ©codeur qui combine des informations basse et haute rÃ©solution.

### 3.5. Optimisation des hyperparamÃ¨tres avec Optuna

Afin d'atteindre ces performances, une campagne de *hyperparameter tuning* a Ã©tÃ© conduite avec **Optuna** sur le modÃ¨le DeepLabV3+. L'objectif Ã©tait de calibrer automatiquement les paramÃ¨tres les plus sensibles (taux d'apprentissage, poids de la Dice loss additionnelle, coefficient de *dropout* dans la tÃªte ASPP) sans multiplier manuellement les expÃ©riences.

- **IntÃ©gration**Â : le notebook d'expÃ©rimentation instancie un `Optuna Study` en mode `TPESampler`, reliÃ© au pipeline d'entraÃ®nement via une fonction `objective(trial)` qui construit dynamiquement le modÃ¨le Ã  partir des suggestions du `trial`.
- **Espace de recherche**Â :
  - `learning_rate`Â âˆˆÂ \[5e-4Â ;Â 2e-2\] (Ã©chelle logarithmique) pour explorer des dÃ©croissances rapides ou progressives.
  - `dice_loss_weight`Â âˆˆÂ \[0Â ;Â 0,5\] afin de tester l'apport d'une composante Dice en complÃ©ment de l'entropie croisÃ©e.
  - `aspp_dropout`Â âˆˆÂ \[0Â ;Â 0,3\] pour rÃ©gulariser la tÃªte ASPP si nÃ©cessaire.
  - `poly_power`Â âˆˆÂ \[0,7Â ;Â 1,0\] pour adapter la vitesse de dÃ©croissance du scheduler polynomial.
- **MÃ©trique optimisÃ©e**Â : `val_masked_mIoU`, Ã©valuÃ©e aprÃ¨s 35Â Ã©poques (ou *early stopping* anticipÃ©) afin de conserver un cycle d'itÃ©ration raisonnable (~6Â minutes par essai sur GPU T4).
- **RÃ©sultats**Â : sur 40Â essais, Optuna a convergÃ© vers un taux d'apprentissage initial â‰ˆÂ 8,5e-3, un poids Dice de 0,15, un dropout ASPP de 0,1 et un `poly_power` de 0,88. Cette combinaison offre un gain de +1,8Â points de `val_masked_mIoU` par rapport aux hyperparamÃ¨tres par dÃ©faut et stabilise la convergence dÃ¨s la 20áµ‰ Ã©poque.
- **Exploitation**Â : les meilleurs hyperparamÃ¨tres sont automatiquement journalisÃ©s via `study.best_params` et injectÃ©s dans `TrainConfig` pour l'entraÃ®nement final, ce qui garantit la reproductibilitÃ©. Les courbes d'optimisation (historique des `trials`, importance des hyperparamÃ¨tres) sont exportÃ©es depuis `optuna.visualization` et archivÃ©es dans MLflow.

L'utilisation d'Optuna a donc permis de sortir rapidement des combinaisons sous-optimales et d'ancrer l'entraÃ®nement final sur des rÃ©glages Ã©prouvÃ©s, rÃ©duisant les Ã©carts de performance entre itÃ©rations et amÃ©liorant la robustesse du modÃ¨le en production.

---

## 4. Benchmarking des rÃ©sultats

### 4.1. SynthÃ¨se des mÃ©triques

Les rÃ©sultats agrÃ©gÃ©s proviennent du notebook d'expÃ©rimentation. Chaque modÃ¨le a Ã©tÃ© entraÃ®nÃ© dans des conditions homogÃ¨nes (mÃªmes splits, mÃªmes augmentations, mÃªmes hyperparamÃ¨tres de base). Le tableau suivant reprend les principales mÃ©triquesÂ :

| ModÃ¨le                     | DurÃ©e (min) | `masked_mIoU` (train) | `val_masked_mIoU` | `pix_acc` (train) | `val_pix_acc` | `dice_coef` (train) | `val_dice_coef` |
| :------------------------- | :---------: | :-------------------: | :---------------: | :---------------: | :-----------: | :-----------------: | :-------------: |
| **DeepLabV3+ (ResNet50)**  | **13,4**    | **0,947**             | **0,639**         | **0,989**         | **0,872**     | **0,965**           | **0,716**       |
| **YOLOv9_seg (simplifiÃ©)** | 10,5        | 0,689                 | 0,400             | 0,913             | 0,714         | 0,753               | 0,494           |
| **MobileDet_seg**          | 16,3        | 0,938                 | 0,502             | 0,987             | 0,779         | 0,953               | 0,600           |
| **U-Net VGG16**            | 29,7        | 0,903                 | 0,542             | 0,977             | 0,805         | 0,923               | 0,633           |
| **U-Net mini**             | 6,1         | 0,563                 | 0,319             | 0,851             | 0,634         | 0,650               | 0,407           |

### 4.2. InterprÃ©tation des indicateurs

- **Masked mIoU**Â : DeepLabV3+ domine nettement avec 0,639 sur validation, confirmant sa capacitÃ© Ã  bien sÃ©parer les classes grÃ¢ce Ã  l'ASPP. U-Net VGG16 et MobileDet suivent (0,54â€“0,50). Les autres architectures dÃ©crochent en dessous de 0,40 du fait d'un contexte insuffisant.
- **Pixel accuracy**Â : la hiÃ©rarchie reflÃ¨te celle du mIoU. DeepLabV3+ atteint 0,87, montrant que plus de 87Â % des pixels valides sont correctement prÃ©dits. U-Net VGG16 maintient 0,80, MobileDet 0,78. Les modÃ¨les plus lÃ©gers restent sous 0,75, indiquant des difficultÃ©s Ã  prÃ©server la cohÃ©rence globale.
- **Dice coefficient**Â : l'Ã©cart entre train et val reste modÃ©rÃ© pour DeepLab (0,965 vs 0,716), signe d'une bonne gÃ©nÃ©ralisation. U-Net VGG16 montre un Ã©cart plus important (0,923 vs 0,633), rÃ©vÃ©lateur d'un surapprentissage partiel. MobileDet se situe Ã  0,600 sur validation, acceptable pour un modÃ¨le compact tandis que YOLOv9_seg peine Ã  dÃ©passer 0,49.

### 4.3. Influence des augmentations sur DeepLabV3+

Les deux entraÃ®nements finaux ont Ã©tÃ© rejouÃ©s avec et sans la pipeline d'augmentations photomÃ©triques pour quantifier leur impact. Les mesures (issues du suivi MLflow) montrent un lÃ©ger surcoÃ»t temporel mais un gain tangible en gÃ©nÃ©ralisation.

| Configuration | DurÃ©e d'entraÃ®nement | `val_dice_coef` | `val_masked_mIoU` | `val_pix_acc` | Conclusions |
| :------------ | :------------------: | :-------------: | :----------------: | :------------: | :--------- |
| **DeepLabV3+ (sans augmentation)** | 3,9 h | 0,840 | 0,818 | 0,945 | Les images brutes suffisent pour apprendre la structure globale, mais le modÃ¨le perd â‰ˆ1 point de Dice et laisse filer des imprÃ©cisions sur les bords fins. |
| **DeepLabV3+ (avec augmentation mÃ©tÃ©o/optique)** | 6,7 h | **0,849** | **0,831** | **0,948** | Les perturbations mÃ©tÃ©o et couleur forcent le rÃ©seau Ã  gÃ©nÃ©raliserÂ : les masques restent nets et stables malgrÃ© â‰ˆ3 h d'entraÃ®nement supplÃ©mentaires. |

> ğŸ’¡ *ConclusionÂ : investir quelques heures GPU supplÃ©mentaires dans des variations mÃ©tÃ©o/lumiÃ¨re renforce la robustesse du modÃ¨le et Ã©vite des erreurs visibles en production.*

### 4.4. Pourquoi ces performancesÂ ?

1. **CapacitÃ© de reprÃ©sentation**Â : DeepLabV3+ et U-Net VGG16 bÃ©nÃ©ficient d'un prÃ©-entraÃ®nement ImageNet et de dÃ©codeurs profonds, ce qui favorise la dÃ©tection des frontiÃ¨res complexes. Les architectures lÃ©gÃ¨res (U-Net mini, YOLOv9 simplifiÃ©) manquent de profondeur ou de *skip connections* riches et perdent des dÃ©tails.
2. **Gestion du contexte**Â : l'ASPP de DeepLab capture plusieurs Ã©chelles simultanÃ©ment, ce qui aide Ã  distinguer des classes visuellement proches (bÃ¢timent vs ciel). MobileDet, avec ses convolutions depthwise, capture moins de contexte global, expliquant une lÃ©gÃ¨re chute sur les classes aux frontiÃ¨res diffuses.
3. **CompatibilitÃ© avec les augmentations**Â : U-Net VGG16 et DeepLab exploitent pleinement la diversitÃ© photomÃ©trique gÃ©nÃ©rÃ©e par Albumentations (flous, mÃ©tÃ©o, bruit), tandis que YOLOv9 simplifiÃ© rÃ©agit moins bien aux distorsions optiques et aux variations de luminositÃ© car sa tÃªte PANet reste sensible aux textures fines.
4. **Optimisation**Â : l'entraÃ®nement SGD avec scheduler polynomial s'adapte mieux aux architectures profondes. Les modÃ¨les plus lÃ©gers auraient pu bÃ©nÃ©ficier d'un AdamW avec *weight decay*Â ; cette piste est listÃ©e dans les travaux futurs.

### 4.5. Analyse multi-critÃ¨res

La synthÃ¨se suivante aide Ã  choisir un modÃ¨le en fonction de contraintes spÃ©cifiquesÂ :

| CritÃ¨re                               | ModÃ¨le recommandÃ©                       | Rationale                                                                 |
| :------------------------------------ | :-------------------------------------- | :------------------------------------------------------------------------ |
| PrÃ©cision globale (mIoU / Dice)       | **DeepLabV3+ ResNet50**                 | Meilleure performance sur toutes les mÃ©triques de validation.             |
| GÃ©nÃ©ralisation / stabilitÃ©            | **DeepLabV3+ ResNet50**                 | Faible Ã©cart train/val, convergence stable.                               |
| Compromis vitesse / qualitÃ©           | **MobileDet_seg**                       | Temps d'entraÃ®nement raisonnable, infÃ©rence lÃ©gÃ¨re.                       |
| Haute fidÃ©litÃ© visuelle (si VRAM OK)  | **U-Net VGG16**                         | RÃ©sultats solides mais coÃ»t mÃ©moire Ã©levÃ©.                                |
| Prototypage rapide / tests pipeline   | **U-Net mini**                          | Faible prÃ©cision mais mise en place rapide.                               |

### 4.6. Observations complÃ©mentaires

- Les modÃ¨les lourds (DeepLab, U-Net VGG16) bÃ©nÃ©ficient pleinement des corruptions photomÃ©triques et mÃ©tÃ©o isolÃ©es, rÃ©duisant l'overfit sans perturber la gÃ©omÃ©trie des objets fins.
- Les architectures basÃ©es sur MobileNet montrent une bonne efficacitÃ© Ã©nergÃ©tique mais nÃ©cessitent un *fine-tuning* plus poussÃ© pour rivaliser avec DeepLab.
- YOLOv9_seg, pensÃ© pour la dÃ©tection, souffre ici de sa tÃªte segmentation simplifiÃ©eÂ ; un rÃ©Ã©quilibrage du dÃ©codeur multi-Ã©chelle serait nÃ©cessaire pour combler l'Ã©cart.

Ces constats confortent la dÃ©cision de retenir DeepLabV3+ pour la mise en production via l'API Flask.

---

## 5. API Flask, architecture d'infÃ©rence et conclusion ouverte

### 5.1. Vue d'ensemble applicative

L'application web est bÃ¢tie sur Flask et sert deux types d'utilisateursÂ :

1. **Front-end Bootstrap**Â : page HTML unique avec trois onglets (*Segmentation*, *Augmentation alÃ©atoire*, *Augmentations isolÃ©es*) permettant de tÃ©lÃ©verser une image et de visualiser sÃ©parÃ©ment le masque et chaque corruption.
2. **Consommateurs API**Â : endpoints REST `/predict`, `/augment` (tirage alÃ©atoire) et `/augment/gallery` (galerie dÃ©terministe) pour intÃ©grer le modÃ¨le et les visualisations dans des pipelines externes (automatisation, testing, intÃ©gration mobile, etc.).

Le serveur initialisÃ© via `run.py` enregistre deux services dans `current_app.extensions`Â :

- `SegmentationService` pour l'infÃ©rence DeepLabV3+.
- `AugmentationService` pour gÃ©nÃ©rer un aperÃ§u du pipeline d'augmentations Albumentations identique Ã  celui employÃ© en entraÃ®nement.

### 5.2. SegmentationService

Le service d'infÃ©rence encapsule toutes les Ã©tapes nÃ©cessaires pour transformer un fichier image en masque segmentÃ©Â :

1. **Chargement du modÃ¨le**Â : ouverture du fichier `.keras` au dÃ©marrage, avec vÃ©rification de la dimension attendue (4DÂ : batch, hauteur, largeur, canaux).
2. **PrÃ©traitement**Â : redimensionnement bilinÃ©aire vers 512Ã—1024, normalisation \[0Â ;Â 1\] et ajout d'une dimension batch.
3. **PrÃ©diction**Â : appel Ã  `model.predict` (verbose 0) pour rÃ©cupÃ©rer les logits. `argmax` sur l'axe classes produit le masque brut.
4. **Post-traitement**Â : redimensionnement du masque Ã  la taille originale avec interpolation *nearest neighbor*, colorisation via la palette `PALETTE` (8Â classes, code hex) et fusion transparente (alpha 0,5) avec l'image source.
5. **Retour**Â : encapsulation dans un dataclass `SegmentationResult` (PIL Images) puis conversion en data URLs base64 pour l'API JSON.

Cette conception garantit que l'API renvoie des rÃ©sultats prÃªts Ã  afficher (PNG encodÃ©s dans une rÃ©ponse JSON), ce qui simplifie l'intÃ©gration front.

### 5.3. AugmentationService

L'aperÃ§u des augmentations partage exactement la pipeline d'augmentations mÃ©tÃ©o/optique (inspirÃ©e de Bhuiya et al.) utilisÃ©e Ã  l'entraÃ®nementÂ :

- Construction d'un `A.Compose([A.OneOf([...], p=1.0)])` oÃ¹ chaque transformateur correspond Ã  une corruption dÃ©crite dans l'article (flous, bruit, mÃ©tÃ©o, distorsion optique).
- Conversion de l'image d'entrÃ©e en `numpy.ndarray`, application de la transformation tirÃ©e au sort (`generate`, `samples` fois) ou de chaque transformation dÃ©terministe (`gallery`) et emballage des rÃ©sultats dans une liste d'`AugmentedImage` (nom + image).
- Conversion finale en data URLs cÃ´tÃ© route Flask pour renvoyer le JSON, ce qui permet d'afficher directement les corruptions dans l'UI.

L'interface expose ainsi Ã  la fois un tirage alÃ©atoire (pour simuler le pipeline d'entraÃ®nement) et une galerie complÃ¨te (pour analyser l'impact isolÃ© de chaque transformation sur image et masque).

### 5.4. Endpoints REST

Les routes `routes.py` exposent trois endpoints POSTÂ :

- **`/predict`**Â : reÃ§oit un champ `image` multipart, vÃ©rifie la prÃ©sence du fichier, exÃ©cute `SegmentationService.predict` et renvoie `original`, `mask`, `overlay` au format data URL.
- **`/augment`**Â : reÃ§oit `image`, exÃ©cute `AugmentationService.generate` et renvoie l'image originale plus `samples` corruptions alÃ©atoires (nom + data URL).
- **`/augment/gallery`**Â : reÃ§oit `image`, exÃ©cute `AugmentationService.gallery` et renvoie la liste exhaustive des quinze corruptions appliquÃ©es individuellement.

Dans les deux cas, un code 400 est renvoyÃ© en absence de fichier. La taille maximale de payload est limitÃ©e Ã  16Â MiB (`MAX_CONTENT_LENGTH`).

### 5.5. DÃ©ploiement, dÃ©pendances et hÃ©bergement Heroku

- **DÃ©pendances**Â : l'environnement Python 3.10+ est requis. Les bibliothÃ¨ques critiques (TensorFlowÂ 2.12, Flask, Albumentations, Pillow, numpy) sont listÃ©es dans `requirements.txt`. Pour allÃ©ger le conteneur, il est conseillÃ© d'utiliser l'image de base `python:3.10-slim` (cf. `Dockerfile`).
- **Artefacts nÃ©cessaires**Â : le modÃ¨le `deeplab_resnet50_final.keras` doit Ãªtre placÃ© dans `app/models/` avant le lancement du serveur, faute de quoi le chargement Ã©choue.
- **DÃ©ploiement local**Â : `docker build -t cityscapes-seg .` puis `docker run -p 5000:5000 cityscapes-seg` pour vÃ©rifier l'API avant publication.
- **HÃ©bergement Heroku (mode conteneur)**Â :
  1. `heroku login` pour authentifier le CLI.
  2. `heroku create <nom-app>` pour crÃ©er l'application (exÂ : `heroku create cityscapes-seg-demo`).
  3. `heroku stack:set container -a <nom-app>` afin d'activer le dÃ©ploiement basÃ© sur Docker.
  4. `heroku container:login` puis `heroku container:push web -a <nom-app>` pour construire et pousser l'image dÃ©finie par le `Dockerfile`.
  5. `heroku container:release web -a <nom-app>` pour dÃ©ployer l'image et dÃ©marrer le dyno.
  6. `heroku logs --tail -a <nom-app>` pour superviser le dÃ©marrage et vÃ©rifier que le modÃ¨le est bien chargÃ©.
- **Variables d'environnement recommandÃ©es**Â :
  - `FLASK_ENV=production` pour dÃ©sactiver le mode debug.
  - `MODEL_PATH=app/models/deeplab_resnet50_final.keras` si l'on souhaite personnaliser le chemin sans modifier le code.
- **ScalabilitÃ©**Â : pour absorber plus de trafic, utiliser `heroku ps:scale web=2 -a <nom-app>` et activer l'auto-scaling via le dashboard Heroku. TensorFlow en CPU sur dyno standard permet environ 1â€“2 infÃ©rences/sÂ ; pour plus de throughput, envisager une version quantifiÃ©e ou un add-on GPU externe.

### 5.6. Conclusion ouverte et pistes d'Ã©volution

Le systÃ¨me actuel fournit une base robuste pour la segmentation urbaine temps quasi-rÃ©el. Plusieurs axes peuvent prolonger ce travailÂ :

1. **Optimisation temps rÃ©el**Â : conversion du modÃ¨le DeepLabV3+ en format TensorRT ou TFLite quantifiÃ© pour accÃ©lÃ©rer l'infÃ©rence sur GPU embarquÃ© ou CPU ARM.
2. **Enrichissement du benchmarking**Â : intÃ©grer Fast-SCNN et d'autres architectures lÃ©gÃ¨res (BiSeNet, DDRNet) pour explorer des compromis supplÃ©mentaires entre latence et prÃ©cision.
3. **Segmentation multi-classes**Â : Ã©largir le remapping Ã  davantage de classes Cityscapes ou Ã  d'autres datasets (Mapillary, BDD100K) pour une couverture urbaine plus fine.
4. **Monitoring en production**Â : ajouter des endpoints de santÃ©, des mÃ©triques Prometheus et une journalisation centralisÃ©e pour suivre les performances en exploitation.
5. **ExpÃ©rience utilisateur**Â : proposer un ajustement en direct de l'opacitÃ© du masque, permettre l'export du masque en GeoJSON ou intÃ©grer un comparateur d'images.
6. **API Ã©largie**Â : offrir un endpoint batch pour traiter plusieurs images en une requÃªte, ou un mode *streaming* (WebSocket) pour des flux vidÃ©o.

En synthÃ¨se, DeepLabV3+ fournit des performances de pointe dans ce cadre rÃ©duit Ã  huit classes, et l'API Flask actuelle constitue un socle solide pour des dÃ©veloppements futurs, tant sur le plan de la recherche que de l'industrialisation. La disponibilitÃ© d'un conteneur Docker prÃªt pour Heroku facilite l'expÃ©rimentation rapide et l'observation en conditions rÃ©elles, tandis que l'utilisation d'Albumentations et du dataset Cityscapes remappÃ© garantit une base scientifique robuste pour itÃ©rer sur de nouveaux scÃ©narios urbains.
